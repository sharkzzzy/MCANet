  def forward(
/home/linux/anaconda3/envs/dp/lib/python3.10/site-packages/mamba_ssm/ops/triton/layernorm.py:568: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, dout, *args):
layer:  1  dstate:  16
layer:  2  dstate:  16
layer:  3  dstate:  16
Output shape: torch.Size([2, 6, 256, 256])
Training mode - Main output shape: torch.Size([2, 6, 256, 256])
Training mode - Number of auxiliary outputs: 3
  Auxiliary output 0 shape: torch.Size([2, 6, 64, 64])
  Auxiliary output 1 shape: torch.Size([2, 6, 64, 64])
  Auxiliary output 2 shape: torch.Size([2, 6, 32, 32])
Total parameters: 55.57M
