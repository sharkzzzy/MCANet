摘 要

遥感图像语义分割是遥感领域的核心技术之一，在城市规划、环境监测、灾害评估等领域具有重要的应用价值。随着遥感传感器分辨率的不断提高，高分辨率遥感图像中包含的地物类型日益多样、尺度差异显著、背景环境愈加复杂，对语义分割算法的特征提取能力和计算效率提出了更高的要求。近年来，状态空间模型（State Space Model, SSM）凭借其线性计算复杂度和全局建模能力，为遥感图像分割提供了新的技术路径。然而，现有基于SSM的分割方法仍然存在全局建模与局部细节增强相互耦合、注意力机制冗余堆叠导致计算开销增大等问题。同时，单一模态数据往往难以全面表征复杂地物的特征信息，光学图像与合成孔径雷达（SAR）图像在成像机理上的本质差异也给跨模态特征融合带来了显著挑战。
为此，本文围绕遥感图像语义分割中的上述问题开展了以下研究：（1）针对现有基于SSM的分割方法中全局上下文建模与局部细节增强在单一路径中耦合处理、注意力机制冗余堆叠等问题，本文提出了基于双路径解耦的语义分割网络DP-UNet。该网络在解码器中设计了双路径解耦VSS模块（DVSS），采用"共享基座、分别增强"的策略：以二维选择性扫描模块（SS2D）作为共享基座生成基础特征，全局路径直接保留该基础特征以维持完整的上下文语义信息，局部路径则在此基础上通过高效通道注意力（ECA）进行通道重标定，并利用参数域可调卷积（PMC）在参数空间引入可学习的中心抑制先验以增强对边缘与纹理细节的感知能力，最终通过自适应路径融合门控（APFG）实现两条路径特征的自适应整合。在多尺度特征融合阶段，设计了轻量化多尺度空间核模块（MSK），在压缩通道空间内通过多尺度卷积提取空间模式并以纯空间注意力进行特征增强，有效避免了通道注意力在网络中的冗余施加。实验结果表明，DP-UNet在ISPRS Potsdam数据集上的平均交并比达到86.71%、总体精度达到91.57%，在ISPRS Vaihingen数据集上的平均交并比达到83.84%、总体精度达到91.43%，在LoveDA数据集上的平均交并比达到53.21%，模型参数量为11.30M，计算量为44.26G FLOPs，能够有效解决全局与局部特征耦合及注意力冗余问题，在分割精度与计算效率之间取得了良好的平衡。（2）针对单一模态遥感数据特征表征能力有限、光学图像与SAR图像模态差异显著导致特征融合不充分等问题，本文在DP-UNet的基础上提出了基于跨模态融合的多模态语义分割网络。该网络设计了双分支编码器分别提取光学图像和SAR图像的模态专属特征，并提出跨模态多尺度融合模块（CrossModalMSK），通过模态内多尺度空间特征提取捕获各模态不同尺度的空间信息，通过模态间交叉注意力机制建模两种模态特征之间的关联性并提取互补信息，实现不同模态、不同尺度特征的有效对齐与协同融合。解码器端复用DVSS模块对融合后的多模态特征进行渐进式上采样与精细化重建。实验结果表明，该网络在WHU-OPT-SAR数据集上的[占位：平均交并比达到XX%、总体精度达到XX%]，能够有效融合光学与SAR图像的互补信息，显著提升了复杂场景下的地物分割性能。

关键词：遥感图像语义分割；状态空间模型；双路径解耦；参数域可调卷积；多模态融合；跨模态注意力
 
Abstract

Keywords: 
  
绪论
研究背景与意义
随着遥感技术的飞速发展，尤其是卫星遥感和无人机遥感平台的日益普及，遥感图像的空间分辨率不断提高，所获取的图像数据量也在急剧增长[1]。遥感图像能够为地理环境研究提供广阔的观测视角，已被广泛应用于土地覆盖分类[2-3]、环境监测[4]、灾害评估[5]、城市规划[6]和精准农业[7]等领域，为国民经济建设和社会发展提供了重要的数据支撑。因此，如何从海量遥感图像中高效、准确地提取地物信息，已成为遥感领域亟待解决的关键问题。
语义分割是计算机视觉领域中的一项核心技术，旨在对图像中的每一个像素进行类别标注，在遥感图像分析中扮演着至关重要的角色[8]。该技术能够实现对建筑物、道路、植被、水体等不同地物类型的自动识别与精细划分，为上述应用场景提供高效的决策支持。近年来，基于深度学习的语义分割方法取得了显著进展，从早期的全卷积网络（Fully Convolutional Network, FCN）[9]到编码器-解码器架构的U-Net[10]，再到融合多尺度上下文信息的DeepLabV3+[11]，分割精度和效率均得到了大幅提升。
然而，高分辨率遥感图像的语义分割仍然面临诸多挑战。首先，遥感图像中地物类型多样、尺度差异显著，同一场景中既包含大面积的农田、水体等宏观地物，也存在车辆、小型建筑等细小目标，要求模型同时具备全局上下文建模能力和局部细节捕获能力。其次，遥感图像背景复杂，地物边界模糊，不同类别之间存在"同物异谱"和"异物同谱"现象，进一步加大了精确分割的难度。此外，高分辨率图像的数据量庞大，对算法的计算效率提出了更高的要求，如何在精度与效率之间取得平衡是一个重要的研究课题。
为应对上述挑战，研究者们不断探索更强大的模型架构。基于Transformer[12]的方法通过自注意力机制有效捕获了长距离依赖关系，在语义分割任务中展现出优越的性能。然而，自注意力机制的计算复杂度与输入序列长度呈二次方关系，当应用于高分辨率遥感图像时，巨大的计算开销严重制约了其实际部署。近年来，状态空间模型（State Space Model, SSM）[13]凭借其线性计算复杂度和全局序列建模能力，为解决这一矛盾提供了新的技术路径。以Mamba[14]为代表的选择性状态空间模型通过输入依赖的选择机制，在保持线性复杂度的同时实现了对长序列的高效建模，已在自然语言处理和计算机视觉等领域展现出巨大潜力。在遥感图像分割中，基于Mamba的方法[15]已初步验证了其在处理大规模场景时兼顾精度与效率的可行性。然而，现有方法仍存在全局上下文建模与局部细节增强在单一处理路径中相互耦合、注意力机制在网络不同阶段冗余堆叠等问题，限制了模型性能的进一步提升。
此外，在实际遥感应用中，单一模态的数据往往难以全面表征复杂地物的特征信息。光学图像虽然具有丰富的光谱和纹理信息，能够直观反映地表覆盖物的属性和类型，但易受云层遮挡、光照变化等环境因素的影响，在恶劣天气条件下成像质量显著下降。合成孔径雷达（Synthetic Aperture Radar, SAR）作为一种主动微波遥感技术，具备全天候、全天时的成像能力，能够穿透云层和部分地表覆盖物，提供稳定可靠的地表观测信息[16]。然而，SAR图像受相干斑噪声干扰严重，且缺乏光谱信息，对地物类型的区分能力有限。通过融合光学图像与SAR图像，可以充分发挥两种模态的互补优势，弥补单一数据源的不足，从而全面、准确地获取地表特征信息。然而，光学图像与SAR图像在成像机理、数据特性和信息表达形式上存在本质差异，如何有效弥合这种模态鸿沟、实现跨模态特征的充分融合，仍然是一个具有挑战性的研究问题。
综上所述，针对遥感图像语义分割中存在的全局与局部特征建模耦合、计算效率受限以及多模态特征融合困难等问题，开展基于状态空间模型的高效分割方法研究，并探索光学与SAR图像的跨模态融合策略，对于推动遥感图像智能解译技术的发展具有重要的理论意义和应用价值。
国内外研究现状
遥感图像语义分割是遥感与计算机视觉交叉领域的重要研究方向。近年来，随着深度学习技术的快速发展，该领域的研究方法经历了从传统方法到深度学习方法、从卷积神经网络到Transformer和状态空间模型、从单模态到多模态的演进过程。这一演进的核心驱动力在于对两个基本矛盾的持续求解：一是全局上下文建模能力与计算效率之间的矛盾，二是单一模态信息不足与多模态融合困难之间的矛盾。本节将从基于卷积神经网络的方法、基于Transformer的方法、基于状态空间模型的方法以及多模态融合方法四个方面，对国内外研究现状进行综述。
基于卷积神经网络的语义分割方法
卷积神经网络（Convolutional Neural Network, CNN）是深度学习在语义分割领域最早也是最广泛应用的基础架构。语义分割任务本质上要求模型同时完成两个目标：对图像内容进行高层语义理解以确定"是什么"，以及保留精确的空间位置信息以确定"在哪里"。这两个目标之间的内在张力构成了语义分割方法演进的核心线索。
2015年，Shelhamer等人[9]提出的全卷积网络（FCN）首次将分类网络中的全连接层替换为卷积层，实现了端到端的像素级预测，奠定了深度学习语义分割的基础范式。FCN的核心贡献在于证明了分类网络中学习到的高层语义特征可以通过全卷积化直接转化为密集预测能力。然而，FCN的根本局限也随之暴露：分类网络中连续的池化操作虽然有效地扩大了感受野并提取了抽象语义特征，但同时导致特征图分辨率大幅降低，分割结果的边界较为粗糙，空间细节信息损失严重。简单的双线性插值上采样难以恢复已丢失的空间精度，这本质上反映了语义抽象与空间精度之间的矛盾。
为系统性地解决这一矛盾，Ronneberger等人[10]提出了U-Net架构。U-Net的核心创新在于其对称的编码器-解码器结构以及跳跃连接机制。编码器通过逐层下采样提取从低级纹理到高级语义的层次化特征，解码器则通过逐层上采样逐步恢复空间分辨率。关键的跳跃连接将编码器各层的高分辨率特征直接传递至对应的解码器层，使得解码器在恢复空间细节时能够利用编码器保留的精确位置信息，从而有效融合了深层语义信息与浅层空间细节。这种设计巧妙地将"语义理解"和"空间定位"分别交由编码器和解码器处理，再通过跳跃连接实现两者的协同，在医学图像分割中取得了突破性成果，并迅速被广泛应用于遥感图像分割领域，成为后续众多分割网络的基础架构。在此基础上，Zhou等人[17]提出了UNet++，通过在编码器与解码器之间引入密集的嵌套跳跃连接，构建了多条不同深度的特征传递路径。这种设计的深层动机在于：U-Net中单一层级的跳跃连接存在语义鸿沟问题——编码器浅层特征富含空间细节但语义信息薄弱，直接与解码器深层的高语义特征拼接可能导致融合效果不佳。UNet++通过密集连接使得不同语义层级的特征能够逐步过渡和融合，缓解了这一问题。
在特征提取骨干网络方面，He等人[18]提出了深度残差网络（ResNet），通过引入残差学习框架有效解决了深层网络训练中的梯度消失问题。ResNet的核心思想是在卷积层之间建立恒等映射的捷径连接，使网络仅需学习输入与输出之间的残差信息，极大地简化了优化过程，使得网络深度可以大幅增加而不会出现性能退化。由于其优异的特征提取能力和训练稳定性，ResNet系列（尤其是ResNet-18和ResNet-50）被广泛用作语义分割模型的编码器骨干，为后续的特征解码和像素级预测提供了高质量的多层级特征表示。
尽管U-Net及其变体有效缓解了空间信息丢失的问题，但编码器中卷积操作的感受野仍然受到卷积核大小的固有限制。即使通过多层堆叠，每个卷积层仅能感知其局部邻域内的信息，对于遥感图像中大面积均质区域（如水体、农田）的一致性分割以及远距离空间关系（如道路的连通性）的捕获，仍然力有不逮。为在不增加参数量的前提下有效扩大感受野，Chen等人[19]提出了DeepLab系列方法。其中，DeepLabV2引入了空洞空间金字塔池化（Atrous Spatial Pyramid Pooling, ASPP）模块，其核心思想是在标准卷积核的采样位置之间插入空洞（即固定间隔），使得卷积核在不增加参数数量的情况下覆盖更大的空间范围。通过并行应用多个不同膨胀率的空洞卷积，ASPP能够同时捕获不同尺度的上下文信息。DeepLabV3+[11]在此基础上进一步结合了编码器-解码器结构，利用解码器逐步恢复空间细节，提升了分割边界的精细度。然而，空洞卷积虽然扩大了理论感受野，但其采样模式仍然是规则的网格状分布，对于不规则形状的地物目标和复杂的空间关系，建模能力仍然有限。
与ASPP的并行多尺度策略不同，Zhao等人[20]提出的金字塔场景解析网络（PSPNet）采用了全局池化的思路来获取多尺度上下文信息。PSPNet通过金字塔池化模块将特征图划分为不同大小的区域并分别进行池化操作，从1×1到6×6的多种尺度，从而聚合从全局到局部的上下文信息。与ASPP相比，PSPNet的全局池化操作能够直接获取整幅图像的全局信息，有效缓解了对大尺度物体和复杂背景的误判问题。然而，池化操作本身会丢弃空间位置信息，且多尺度特征的简单拼接方式可能引入冗余信息。
在模型轻量化方面，随着遥感图像实时处理和边缘部署需求的增长，如何在保持分割精度的同时降低模型的计算开销成为另一重要研究方向。Badrinarayanan等人[21]提出了SegNet，通过利用编码阶段的最大池化索引进行非参数化上采样，避免了转置卷积中大量可学习参数的引入，显著减少了模型参数量。Yu等人[22]提出了双边分割网络（BiSeNet），其设计思想尤为值得关注：它明确地将语义理解和空间细节保持这两个目标分配给两个独立的分支——空间路径通过少量下采样保留高分辨率的空间细节，上下文路径通过快速下采样获取全局语义信息——最后通过特征融合模块将两者结合。这种"将不同任务分配给专用路径"的设计理念，为后续的多路径分割架构提供了重要启发。Howard等人[23]提出的MobileNetV3通过引入深度可分离卷积和反转残差结构，大幅降低了骨干网络的计算开销，为轻量化语义分割模型提供了高效的特征提取基础。
在遥感图像分割领域，基于CNN的方法同样取得了丰富成果，并针对遥感图像的特殊性进行了针对性优化。Li等人[24]提出的ABCNet通过注意力增强的双边网络结构，在空间细节分支和上下文信息分支中分别引入注意力机制，增强了对遥感图像中复杂场景的特征选择能力，实现了精细分割。Wang等人[25]提出了UNetFormer，以ResNet作为编码器骨干，并在解码器中融合了Transformer模块的全局-局部注意力机制。UNetFormer的设计反映了一个重要趋势：研究者开始认识到CNN在局部特征提取方面的高效性和Transformer在全局建模方面的优越性可以互补，而非互相替代。这一思想为后续CNN与其他全局建模技术（如状态空间模型）的融合提供了重要参考。
综上所述，基于CNN的语义分割方法经历了从FCN的全卷积化开创、U-Net的编码器-解码器结构确立、DeepLab系列的多尺度感受野扩展到轻量化模型的效率优化等发展阶段。然而，无论是空洞卷积还是池化操作，本质上都是通过各种策略来弥补卷积操作局部感受野的固有限制，难以真正实现全局范围内任意位置之间的直接信息交互。这一根本性限制促使研究者将目光转向具备全局建模能力的Transformer架构。
基于Transformer的语义分割方法
卷积神经网络通过局部感受野的层层堆叠来间接捕获全局信息的方式，在效率和效果上均存在瓶颈。Transformer[12]的出现提供了一种全新的思路：通过自注意力机制，序列中任意两个位置之间可以建立直接的依赖关系，从根本上克服了CNN局部感受野的限制。
Dosovitskiy等人[26]首次将Transformer应用于计算机视觉领域，提出了Vision Transformer（ViT）。ViT将图像划分为固定大小的图像块（patch），将每个图像块展平为一维向量后作为序列输入Transformer编码器，通过多头自注意力机制建模图像块之间的全局关系。ViT在大规模图像分类任务中取得了优异的性能，证明了Transformer架构在视觉任务中的可行性。然而，ViT的设计也暴露了Transformer应用于视觉任务的核心挑战：自注意力的计算复杂度与序列长度呈二次方关系为 O\left(N^2\right)，其中 N 为图像块数量。对于遥感图像分割而言，高分辨率图像需要密集的像素级预测，图像块数量远大于分类任务，导致计算开销急剧增大。此外，ViT的固定单尺度图像块划分方式无法生成类似CNN的多尺度层次化特征，而多尺度特征对于处理遥感图像中尺度差异显著的地物目标至关重要。
针对上述问题，研究者从两个方向进行了改进。第一个方向是构建层次化的Transformer结构以生成多尺度特征。Wang等人[27]提出了PVT（Pyramid Vision Transformer），设计了具有渐进缩减空间分辨率的金字塔结构，在每个阶段逐步降低特征图分辨率的同时增加通道维度，生成了类似CNN的多尺度特征金字塔。PVT还引入了空间缩减注意力机制，通过在计算注意力前对键（Key）和值（Value）进行空间降采样来降低计算复杂度，为后续密集预测任务中的Transformer骨干设计提供了重要参考。第二个方向是通过局部化策略降低自注意力的计算复杂度。Liu等人[28]提出了Swin Transformer，通过滑动窗口机制将自注意力的计算限制在固定大小的局部窗口内，将计算复杂度从O\left(N^2\right)降低至O\left(N\right)。同时，通过相邻层之间窗口的规律性移位实现跨窗口信息交互，在不显著增加计算量的前提下维持了全局建模能力。Swin Transformer的层次化设计与窗口注意力机制使其成为密集预测任务中最具影响力的Transformer骨干之一。
在语义分割领域，Zheng等人[29]提出了SETR（Segmentation Transformer），将ViT作为编码器，首次验证了纯Transformer架构在语义分割任务中的可行性。然而，SETR直接使用ViT的单尺度特征进行解码，缺乏多尺度信息的融合，在处理多尺度地物时效果受限。Xie等人[30]提出了SegFormer，设计了层次化的Mix-Transformer编码器和轻量级的全多层感知机（All-MLP）解码器。SegFormer的编码器通过重叠的图像块嵌入保留了相邻块之间的局部连续性，同时利用高效的自注意力机制进行全局建模。其轻量级的MLP解码器摒弃了复杂的注意力模块，仅通过多层感知机融合不同层级的特征，在多个分割基准上取得了精度与效率的良好平衡，证明了编码器中充分的特征提取可以简化解码器的设计。Cao等人[31]提出了Swin-Unet，首次构建了基于纯Swin Transformer的U形编码器-解码器架构，将U-Net中的CNN模块完全替换为Swin Transformer块，通过块合并和块扩展操作实现特征的下采样和上采样，在医学图像分割任务中验证了Transformer架构可以端到端地替代CNN完成密集预测任务。
在遥感图像分割中，基于Transformer的方法也得到了广泛应用。Wang等人[32]提出了BANet，采用Transformer作为骨干网络提取多层级特征，并设计了双边感知模块将高层语义信息与低层空间细节进行融合，其核心思想是通过变换注意力机制动态地建模不同层级特征之间的关联关系，从而实现更精细的语义-空间信息交互，有效提升了对遥感图像中复杂场景的理解能力。Strudel等人[33]提出了Segmenter，采用纯Transformer架构进行语义分割，在编码器和解码器中均使用Transformer块，通过全局注意力机制增强了对大范围地物的识别能力。然而，纯Transformer架构的计算开销较大，且缺乏CNN在局部特征提取方面的归纳偏置优势，在处理遥感图像中精细的地物边界和小目标时，其性能相比CNN-Transformer混合架构并不总是占优。
然而，尽管Transformer在建模长距离依赖方面具有显著优势，其固有的计算效率问题始终是制约实际应用的核心瓶颈。即使采用Swin Transformer的局部窗口策略，当应用于高分辨率遥感图像（如6000×6000像素）时，窗口数量仍然巨大，计算和内存开销依然可观。此外，局部窗口策略虽然降低了计算复杂度，但也在一定程度上牺牲了Transformer最核心的全局直接交互优势——窗口内的注意力计算实质上退化为一种特殊的局部操作，跨窗口信息仅通过窗口移位间接传递。这一效率与全局建模能力之间的根本矛盾，促使研究者开始探索Transformer之外的全局建模范式，其中状态空间模型凭借其线性复杂度的序列建模能力成为最具前景的替代方案。
基于状态空间模型的语义分割方法
状态空间模型（State Space Model, SSM）源于控制理论和信号处理领域，其基本思想是通过一组隐状态的线性递推来描述输入序列到输出序列的映射关系[13]。具体而言，SSM将输入信号通过状态方程映射为隐状态序列，再通过观测方程将隐状态转化为输出。与Transformer的自注意力机制通过全局配对计算来建模序列依赖关系不同，SSM通过隐状态的逐步递推积累全局信息，其计算复杂度与序列长度呈线性关系O\left(N\right)，在处理长序列时具有显著的效率优势。
然而，早期的SSM存在一个关键限制：其状态转移矩阵是固定的，无法根据输入内容自适应地调整信息的保留和遗忘策略，导致模型对不同输入的区分能力不足。Gu等人[34]提出的结构化状态空间序列模型（S4）通过对状态转移矩阵进行HiPPO初始化和对角化参数设计，首次在长距离序列建模任务中取得了突破性成果，证明了SSM在捕获长距离依赖关系方面的潜力。但S4的参数仍然是输入无关的，即对所有输入使用相同的状态转移规则，这限制了其对输入内容的选择性处理能力。
在此基础上，Gu和Dao[14]提出了Mamba模型，实现了SSM从线性时不变系统到输入依赖系统的关键跨越。Mamba的核心创新在于将状态转移矩阵和输入矩阵参数化为输入的函数，使得模型能够根据当前输入内容自适应地决定哪些信息应当被保留到隐状态中、哪些信息应当被遗忘。这种选择性状态空间机制（Selective State Space）赋予了模型类似于注意力机制的内容感知能力，但其计算方式仍然是线性递推，保持了O\left(N\right)的计算复杂度。同时，Mamba设计了硬件感知的并行扫描算法，通过将递推计算重构为并行前缀和运算，充分利用GPU的并行计算能力，在保持理论线性复杂度的同时实现了实际运行速度的大幅提升。
Mamba在自然语言处理任务中展现出了与Transformer相当甚至超越的性能后，迅速被引入计算机视觉领域。然而，将SSM从一维序列扩展到二维图像面临一个根本性挑战：图像数据具有天然的二维空间结构，而SSM本质上是一维序列模型，如何将二维空间信息有效地编码为一维序列并保持空间关系是关键问题。针对这一挑战，Liu等人[35]提出了VMamba，设计了二维选择性扫描模块（SS2D）。SS2D的核心思想是沿四个方向（从左上到右下、从右下到左上、从左下到右上、从右上到左下）分别将二维特征图展开为一维序列，对每个方向独立进行选择性SSM扫描，最后将四个方向的输出进行融合。这种交叉扫描策略确保了特征图上任意两个位置之间至少存在一条扫描路径使其信息可达，从而在保持线性复杂度的同时实现了对二维空间全局依赖关系的有效建模。Zhu等人[36]提出了Vision Mamba（Vim），采用双向状态空间模型处理图像序列，通过正向和反向两条扫描路径捕获全局视觉上下文，在图像分类任务中实现了与ViT相当的性能但具有更低的计算开销。
在语义分割领域，基于Mamba的方法针对密集预测任务的特殊需求进行了针对性设计。Xing等人[37]提出了SegMamba，将Mamba引入三维医学图像分割任务。三维体数据的序列长度远大于二维图像，传统Transformer方法在此场景下面临更为严峻的计算瓶颈，而Mamba的线性复杂度使其能够高效处理长达数百万体素的三维序列，有效建模了体数据中的长距离空间依赖关系。Ruan和Xiang[38]提出了VM-UNet，将Vision Mamba模块嵌入U-Net的编码器-解码器架构中，构建了纯SSM的语义分割网络。VM-UNet的实验结果表明，SSM不仅可以作为Transformer的替代方案用于特征编码，还能够在解码器中有效地进行特征恢复和精细化，验证了SSM作为编码器-解码器骨干的全面可行性。Ma等人[39]提出了U-Mamba，将Mamba模块与CNN模块在编码器中进行混合设计，利用CNN提取局部特征、Mamba捕获全局依赖，两者的协同使模型在局部细节和全局语义两方面均具有良好表现，为CNN与SSM的融合提供了有效范式。
在遥感图像分割领域，基于Mamba的方法同样受到了广泛关注，并针对遥感图像的特殊需求进行了适配。Chen等人[40]提出了RS3Mamba，将Mamba与CNN相结合用于遥感图像语义分割。RS3Mamba设计了辅助的选择性状态空间模块，通过与CNN分支的协同工作增强了多尺度特征的提取能力，在多个遥感分割基准上展现了SSM在遥感领域的应用潜力。Shi等人[15]提出了CM-UNet，构建了一个将基于CNN的ResNet编码器与基于Mamba的解码器相结合的混合架构。CM-UNet在解码器中设计了CSMamba模块，该模块在SS2D模块的基础上引入了通道注意力和空间注意力门控机制，旨在增强SS2D输出特征的判别性。同时，CM-UNet提出了多尺度注意力聚合（MSAA）模块用于跳跃连接处的多尺度特征融合，该模块同样采用了通道-空间双重注意力来增强融合特征的表征能力。CM-UNet在ISPRS Potsdam、Vaihingen和LoveDA等多个遥感分割数据集上取得了优异的性能，验证了CNN编码器+Mamba解码器这一混合架构的有效性。
尽管上述方法取得了显著进展，但深入分析现有基于Mamba的遥感分割方法，仍可发现两方面值得关注的问题。
第一，全局与局部的耦合处理问题。现有方法的解码器通常采用单一路径的序列化设计，在同一处理流程中先后完成全局上下文建模和局部细节增强两种任务。然而，这两种任务具有本质不同的优化目标：全局上下文建模需要在较大的空间范围内整合语义信息以确保类别预测的一致性，而局部细节增强则需要在精细的空间尺度上保留边缘、纹理等高频信息。将它们耦合在同一路径中意味着两种任务共享相同的参数空间和计算流程，在优化过程中可能相互制约——加强全局建模的参数更新可能损害局部细节的保持，反之亦然。这种耦合限制了模型在两个方面同时达到最优的潜力。
第二，注意力机制的冗余施加问题。部分方法在网络的多个阶段重复使用功能相似的注意力操作。例如，在特征变换阶段已经应用了通道和空间注意力来增强特征选择能力，而在跳跃连接的特征融合阶段又再次施加了类似的通道-空间双重注意力。这种同质注意力的重复堆叠虽然在一定程度上增强了特征聚焦能力，但也带来了显著的参数和计算冗余，且重复的注意力操作在梯度传播过程中可能产生优化干扰，并不能保证性能的等比例提升。
如何将全局建模与局部增强解耦为独立的专用路径使各自获得充分的优化空间，以及如何在网络的不同阶段合理分配注意力机制的功能以避免冗余，是当前基于SSM的遥感分割方法需要进一步解决的关键问题。
多模态遥感图像融合分割方法
上述单模态语义分割方法的进展主要集中在模型架构的改进上，但无论采用何种架构，其输入数据均为单一模态的遥感图像。在实际遥感应用中，单一模态数据的信息表征能力往往受限于其自身的成像机理。光学图像能够提供丰富的光谱和纹理信息，但在云层遮挡、光照不足等条件下成像质量严重退化；SAR图像具备全天候、全天时成像能力，但受相干斑噪声干扰且缺乏光谱信息。多模态数据融合通过整合不同传感器的互补信息，可以从根本上弥补单一模态的不足，已成为提升语义分割性能的重要手段[41]。
根据融合阶段的不同，多模态融合方法通常可分为早期融合、中期融合和晚期融合三种策略[42]。早期融合在输入层直接拼接不同模态的数据，方法最为简单直接，但由于不同模态数据在数值范围、统计分布和语义表达上存在显著差异，直接拼接可能导致模态间的特征相互干扰，反而降低分割性能。晚期融合在决策层对各模态独立预测的结果进行集成（如投票或加权平均），能够避免模态间特征的直接干扰，但由于各模态的特征提取过程完全独立，缺乏特征层面的交互，无法充分利用模态间的互补信息。中期融合在特征提取的中间阶段对不同模态的特征进行交互与整合，能够在保持模态特异性特征提取的同时实现深层次的特征互补，是目前研究最为广泛且效果最为显著的融合策略。
在中期融合的具体实现方式上，早期工作主要采用简单的特征拼接或逐元素操作。Hazirbas等人[43]提出了FuseNet，采用双分支编码器分别处理RGB图像和深度图像，并将深度分支各层的特征通过逐元素相加的方式融合至RGB分支的编码器中。FuseNet的设计思想在于让深度信息作为辅助信息逐级增强RGB特征的表达能力。然而，这种单向融合方式存在两个局限：一是深度分支向RGB分支的单向信息传递未能实现双向的模态交互，RGB分支的信息无法反过来指导深度特征的提取；二是简单的逐元素相加难以建模两种模态特征之间的复杂非线性关系，融合深度有限。Audebert等人[44]同样采用双分支CNN网络分别处理光学图像和数字表面模型（DSM）图像，并通过逐元素相加进行特征融合，面临类似的局限。
为解决简单融合策略的不足，研究者们引入了注意力机制来实现更精细的模态特征交互。Ma等人[45]提出了AMM-FuseNet，采用通道注意力机制动态评估不同模态各通道特征的重要性，并结合密集连接的空间金字塔池化增强多尺度特征的表征能力，在一定程度上缓解了简单融合策略中模态信息利用不充分的问题。然而，通道注意力仅在通道维度上对特征进行重标定，无法在空间维度上精细地对齐不同模态的特征响应，对于存在空间错位的多模态数据融合效果有限。
在光学与SAR图像的融合分割方面，由于两种模态在成像机理上的本质差异，跨模态特征融合面临更大的挑战。光学图像反映地物的光谱反射特性，其特征表达主要体现在颜色、纹理和光谱梯度等方面；而SAR图像通过后向散射系数表征地表结构特征，其特征表达主要体现在散射强度、极化特性和空间纹理等方面。这两种截然不同的特征表达方式使得简单的特征级融合难以有效地建模模态间的语义对应关系。此外，SAR图像固有的相干斑噪声会在融合过程中传播至光学特征空间，可能干扰光学特征的判别性。
针对光学与SAR图像融合的特殊挑战，Li等人[46]提出了MCANet并构建了WHU-OPT-SAR数据集。MCANet设计了多模态交叉注意力模块，其核心思想是以一种模态的特征作为查询（Query），以另一种模态的特征作为键（Key）和值（Value），通过交叉注意力计算来提取两种模态之间的互补信息。这种交叉注意力机制相比简单的特征拼接或相加，能够更精细地建模模态间的语义关联，动态地从对方模态中选取最相关的信息进行融合。同时，MCANet还设计了低高层特征融合模块，将浅层的空间细节与深层的语义信息相结合以优化分割结果。实验结果验证了交叉注意力机制在光学-SAR融合任务中的显著优势。Feng等人[47]提出了CMGFNet，进一步引入了门控融合机制来解决模态间信息质量不均衡的问题。门控机制通过学习一组空间自适应的融合权重，使模型能够根据不同空间位置的特征质量动态调节光学和SAR特征的贡献比例：在光学特征质量较高的区域（如无云覆盖区域）赋予光学特征更大的权重，在SAR特征更可靠的区域（如云层覆盖区域）则加大SAR特征的权重，有效应对了模态间语义不一致和信息质量不均衡的问题。Zhang等人[48]提出了CMX，设计了交叉模态特征校正模块，通过双向特征交互实现不同模态之间的有效融合。CMX的设计思想具有较好的通用性，其跨模态融合范式不局限于特定的模态组合，对光学与SAR图像的融合也具有重要的借鉴意义。
综上所述，多模态遥感图像融合分割方法从早期的简单特征拼接发展到基于注意力机制的精细交互融合，在融合深度和效果上取得了显著进步。然而，现有方法仍存在以下需要进一步解决的问题：首先，多数方法在融合过程中仅关注单一尺度的特征交互，而光学与SAR图像在不同尺度上的特征互补性存在差异——在低层级，两种模态的纹理和边缘信息差异显著，需要精细的空间对齐；在高层级，两种模态的语义信息趋于一致，可以进行更直接的语义融合——缺乏对多尺度跨模态交互的系统性考虑。其次，现有方法的解码器设计大多沿用标准的CNN或Transformer架构，未能充分利用SSM等新型全局建模技术在多模态融合特征重建中的潜力。如何在多模态融合框架中有效整合状态空间模型的高效全局建模能力，并针对多模态场景设计专用的多尺度跨模态融合策略，是一个值得深入探索的研究方向。
主要研究内容
针对上述研究现状中存在的问题，本文围绕基于状态空间模型的遥感图像语义分割方法展开研究，致力于解决现有方法中全局与局部特征建模耦合、注意力机制冗余以及多模态特征融合不充分等问题。具体研究内容如下：
（1）针对现有基于SSM的遥感分割方法中全局上下文建模与局部细节增强在单一路径中耦合处理、注意力机制在不同网络阶段冗余堆叠等问题，本文提出了基于双路径解耦的语义分割网络DP-UNet。全局上下文建模与局部细节增强是语义分割中两个本质不同的子任务：前者需要在较大空间范围内整合语义信息以保证类别预测的区域一致性，后者则需要在精细空间尺度上保留高频细节以确保分割边界的准确性。现有方法将两者置于同一处理路径中顺序执行，导致两种任务的参数优化相互制约。同时，功能相似的注意力机制在特征变换和特征融合等多个阶段被重复施加，带来了不必要的计算冗余。为此，本文在解码器中设计了双路径解耦VSS模块（DVSS），采用"共享基座、分别增强"的策略。该设计的核心思想是：全局建模和局部增强虽然是不同的任务，但它们共享相同的底层特征基础。因此，DVSS首先以SS2D模块作为共享基座生成统一的基础特征表示，随后将其分发至两条专用路径——全局路径直接保留该基础特征以维持完整的上下文语义信息，局部路径则在此基础上通过高效通道注意力（ECA）进行通道重标定，聚焦于信息量丰富的特征通道，并利用所设计的参数域可调卷积（PMC）在卷积权重的参数空间中引入可学习的中心抑制先验，打破标准卷积的中心主导倾向，增强对边缘与纹理等局部细节的感知能力。最终通过自适应路径融合门控（APFG）根据各路径特征的信息量动态分配融合权重，实现两条路径特征的自适应整合。在多尺度特征融合阶段，考虑到DVSS的局部路径已承担了通道维度的特征重标定任务，本文设计了轻量化多尺度空间核模块（MSK），在压缩通道空间内以纯空间注意力完成多尺度特征融合，避免通道注意力在网络中的冗余施加，从而在提升分割精度的同时有效降低计算开销。
（2）针对单一模态遥感数据特征表征能力有限、光学与SAR图像模态差异显著导致特征融合不充分等问题，本文在DP-UNet的基础上进一步扩展至多模态融合场景，提出了基于跨模态融合的语义分割网络。单一模态遥感数据在面对复杂地表环境时存在信息不足的固有局限：光学图像易受云层遮挡和光照变化影响，在恶劣天气条件下地物信息严重缺失；SAR图像虽能全天候成像，但受相干斑噪声干扰且缺乏光谱信息，对地物类型的区分能力有限。两种模态在信息维度上具有天然的互补性——光学图像擅长表征地物的光谱属性和视觉纹理，SAR图像擅长反映地表的几何结构和散射特性。然而，两种模态在成像机理、特征分布和噪声特性上的巨大差异，使得简单的特征拼接或相加难以有效挖掘模态间的互补信息。为此，本文设计了双分支编码器分别提取光学图像和SAR图像的模态专属特征，避免异质模态特征在早期阶段的相互干扰。在此基础上，提出了跨模态多尺度融合模块（CrossModalMSK），该模块通过模态内多尺度空间特征提取分别捕获各模态在不同尺度上的空间模式，并通过模态间交叉注意力机制建模两种模态特征之间的语义关联性，从对方模态中动态提取最具互补价值的信息，实现不同模态、不同尺度特征之间的有效对齐与协同融合。解码器端复用第一个工作中已验证有效的DVSS模块，对融合后的多模态特征进行全局-局部解耦增强与渐进式精细化重建，将单模态工作中的核心设计思想自然地延伸至多模态场景。
（3）为验证所提方法的有效性，本文在ISPRS Potsdam、ISPRS Vaihingen、LoveDA和WHU-OPT-SAR四个公开遥感数据集上开展了系统性的实验评估。实验设计涵盖三个层面：首先，与多种具有代表性的现有方法进行全面的对比实验，从平均交并比、总体精度、各类别F1分数等多个指标维度验证所提方法的分割性能优势；其次，通过逐步添加和移除关键模块的消融实验，系统性地验证DVSS、PMC、MSK、CrossModalMSK等核心模块各自的贡献及其协同效应；最后，从浮点运算量、参数量和推理速度等方面进行模型复杂度分析，全面评估所提方法在精度与效率之间的平衡能力。
本文结构
本文聚焦于基于状态空间模型的遥感图像语义分割方法研究，全文共分为五章，各章主要内容安排如下：
第一章为绪论。首先介绍了遥感图像语义分割的研究背景与意义；其次分别从基于卷积神经网络的方法、基于Transformer的方法、基于状态空间模型的方法以及多模态融合方法四个方面综述了国内外研究现状，分析了各类方法的技术演进脉络、核心优势及存在的局限性；接着根据当前研究中存在的问题，确定了本文的主要研究内容；最后介绍了论文的组织结构。
第二章为相关理论与技术基础。首先介绍了语义分割任务的基本概念和编码器-解码器框架；其次阐述了状态空间模型的基本原理，包括从S4到Mamba的演进过程以及二维选择性扫描机制；然后介绍了本文涉及的注意力机制和多模态遥感数据的基本特性；接着介绍了本文实验所用的四个公开数据集和语义分割任务的评价指标；最后对本章内容进行小结。
第三章为基于双路径解耦的单模态遥感图像语义分割方法。首先分析了现有基于SSM的分割方法中存在的全局-局部耦合和注意力冗余问题；然后详细介绍了DP-UNet的整体架构以及DVSS模块、PMC模块和MSK模块的设计原理与实现细节；接着在ISPRS Potsdam、ISPRS Vaihingen和LoveDA三个数据集上进行对比实验、消融实验和模型复杂度分析；最后对本章内容进行小结。
第四章为基于跨模态融合的多模态遥感图像语义分割方法。首先分析了单模态分割方法的局限性以及光学与SAR图像跨模态融合的挑战；然后详细介绍了多模态融合网络的整体架构以及跨模态多尺度融合模块的设计原理；接着在WHU-OPT-SAR数据集上进行实验分析；最后对本章内容进行小结。
第五章为总结与展望。总结了本文的主要研究工作和创新点，分析了当前工作的局限性，并对未来的研究方向进行了展望。
	相关理论与技术基础
引言
本文的研究工作主要基于状态空间模型和注意力机制等深度学习方法，在单模态和多模态遥感图像语义分割领域展开。为建立后续研究的理论基础，本章将系统介绍相关的核心理论与关键技术。首先，阐述语义分割任务的基本概念和编码器-解码器架构的设计原理；其次，详细介绍状态空间模型的数学基础及其从S4到Mamba的技术演进；然后，介绍本文涉及的通道注意力、空间注意力和交叉注意力等机制的原理与实现；接着，分析光学遥感图像与SAR图像的数据特性及其互补关系；随后，介绍本文实验所采用的四个公开数据集及语义分割任务的评价指标；最后，对本章内容进行小结。
语义分割基础
语义分割任务概述
语义分割是计算机视觉领域中像素级理解任务的核心代表，其目标是对输入图像中的每一个像素赋予一个语义类别标签。与图像分类任务仅需输出整幅图像的单一类别标签不同，语义分割要求模型在理解"图像中有什么"的同时，精确地确定"每个目标在哪里"，即同时完成语义理解和空间定位两个目标。
形式化地，给定一幅输入图像 $I \in \mathbb{R}^{H \times W \times C}$，其中 $H$、$W$、$C$ 分别表示图像的高度、宽度和通道数，语义分割的目标是学习一个映射函数 $f$，将输入图像映射为一幅与原图尺寸相同的标签图 $Y \in \{0, 1, \ldots, K-1\}^{H \times W}$，其中 $K$ 为预定义的类别总数。标签图中每个位置 $(i, j)$ 的值 $y_{ij}$ 即为该像素所属的语义类别。
在遥感图像语义分割中，常见的语义类别包括建筑物、道路、植被、水体、裸地、车辆等地物类型。与自然场景图像相比，遥感图像的语义分割具有以下特殊性：第一，遥感图像分辨率高、覆盖面积大，单幅图像可能包含数千万甚至上亿个像素，对算法的计算效率提出了极高要求；第二，遥感图像中地物尺度差异显著，同一场景中既有数百米范围的大面积农田，也有仅数米大小的单辆车辆，要求模型具备强大的多尺度建模能力；第三，遥感图像采用俯视角度拍摄，地物呈现的形态与日常视角存在较大差异，且不同类别之间的边界往往模糊不清，增加了精确分割的难度。这些特殊性构成了遥感图像语义分割研究的核心挑战，也是本文方法设计的重要出发点。
编码器-解码器架构
编码器-解码器架构是当前语义分割领域最主流的网络设计范式。该架构的设计动机源于语义分割任务的内在矛盾：一方面，为了准确判断每个像素的语义类别，模型需要通过逐层抽象提取高层语义特征，而这一过程不可避免地会降低特征图的空间分辨率；另一方面，分割任务要求输出与原图尺寸相同的逐像素预测，需要精确的空间位置信息。编码器-解码器架构通过将这两个相互矛盾的需求分别交由编码器和解码器两个阶段处理，巧妙地实现了语义理解与空间定位的协同。
编码器承担特征提取的任务，其核心功能是从输入图像中逐层提取从低级到高级的层次化特征表示。以本文采用的ResNet-18为例，编码器由四个残差阶段（Stage）组成，每个阶段通过卷积操作和下采样操作将特征图的空间分辨率降低一半，同时将通道数增加一倍。具体而言，假设输入图像的尺寸为 $H \times W \times 3$，四个阶段分别输出尺寸为 $\frac{H}{4} \times \frac{W}{4} \times 64$、$\frac{H}{8} \times \frac{W}{8} \times 128$、$\frac{H}{16} \times \frac{W}{16} \times 256$ 和 $\frac{H}{32} \times \frac{W}{32} \times 512$ 的特征图。随着层级的加深，特征图的空间分辨率逐步降低，但其中包含的语义信息逐步增强：浅层特征图保留了丰富的边缘、纹理等局部空间细节，但语义信息较为薄弱；深层特征图具有较强的语义抽象能力，能够区分不同类别的地物，但空间细节已经大量丢失。
编码器中的下采样操作（如步幅为2的卷积或池化）在提升语义抽象能力的同时，不可逆地丢弃了空间位置信息。以一个 $2 \times 2$ 的最大池化操作为例，它将每 $2 \times 2$ 个相邻像素合并为一个像素，特征图面积缩小为原来的四分之一。这意味着原始图像中四个相邻像素的精确位置区别被消除，仅保留了最显著的特征响应。经过四次下采样后，最终特征图的每个像素对应原图 $32 \times 32$ 个像素的区域，空间精度已远远不足以支撑像素级的精确分割。
[此处插入图：编码器特征层次化示意图。展示ResNet-18四个阶段的特征图，从左到右尺寸逐渐减小、通道数逐渐增加。浅层标注"边缘/纹理信息丰富，语义信息弱"，深层标注"语义信息强，空间细节丢失"。]
解码器的功能是将编码器输出的低分辨率高语义特征逐步恢复至原始分辨率，生成像素级的分割预测图。解码器的工作过程与编码器相反：通过逐层上采样操作（如双线性插值或转置卷积）逐步提高特征图的空间分辨率，同时通过卷积操作对上采样后的特征进行精细化处理。然而，仅靠上采样操作无法真正"恢复"编码器中已丢失的空间细节。上采样本质上是一种插值操作，只能使特征图在空间维度上变得更大，但无法凭空创造出编码过程中被丢弃的精确位置和边缘信息。这就引出了编码器-解码器架构中最关键的设计要素——跳跃连接。
跳跃连接（Skip Connection）在编码器的各层与解码器的对应层之间建立直接的信息传递通道。其核心思想是：编码器浅层的特征图虽然语义信息较弱，但保留了精确的空间位置和边缘细节；解码器在恢复空间分辨率时，可以直接利用这些来自编码器的高分辨率空间信息，弥补上采样过程中无法恢复的细节。具体实现方式通常是将编码器某层的特征图与解码器对应层上采样后的特征图进行拼接（concatenation）或逐元素相加（element-wise addition），然后通过卷积操作融合两者的信息。
[此处插入图：完整的编码器-解码器架构示意图（U形结构）。左侧编码器逐层下采样（标注各层尺寸和通道数），右侧解码器逐层上采样，中间用水平箭头标注跳跃连接。标注编码器各层特征的特点（浅层→空间细节，深层→语义信息）以及解码器如何通过跳跃连接融合两者。]
通过编码器-解码器架构和跳跃连接的协同工作，模型能够同时获取深层的语义理解能力和浅层的空间定位精度，有效缓解了语义抽象与空间精度之间的矛盾。这一架构范式被广泛应用于各种语义分割网络中，本文提出的DP-UNet同样采用编码器-解码器架构作为整体框架，以ResNet-18作为编码器骨干提取多层级特征，以DVSS模块构建解码器实现特征的渐进式恢复，并通过MSK模块增强跳跃连接中的多尺度特征融合。
损失函数设计
损失函数用于量化模型预测结果与真实标签之间的差异，是驱动模型参数优化的核心要素。损失函数的设计直接影响模型的训练方向和最终性能。在语义分割任务中，由于不同类别的像素数量往往严重不平衡（例如遥感图像中背景类像素可能占总像素的50%以上，而车辆类像素可能不足1%），单一的损失函数往往难以全面指导模型优化。因此，本文采用交叉熵损失与Dice损失的联合损失来训练模型，综合利用两者各自的优势。
交叉熵损失（Cross-Entropy Loss） 是语义分割中最基础也最广泛使用的损失函数，其核心思想是衡量模型预测的概率分布与真实标签分布之间的差异。在语义分割任务中，模型对每个像素输出一个 $K$ 维的概率向量（通过Softmax函数将网络输出转化为各类别的概率），真实标签则可以表示为独热编码的 $K$ 维向量（仅在真实类别位置为1，其余位置为0）。交叉熵损失的计算公式为：
$$\mathcal{L}_{CE} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{k=0}^{K-1} y_{i,k} \log(\hat{y}_{i,k})$$
其中，$N$ 为图像中的像素总数，$K$ 为类别总数，$y_{i,k}$ 为第 $i$ 个像素的真实标签的独热编码，$\hat{y}_{i,k}$ 为模型预测第 $i$ 个像素属于类别 $k$ 的概率。由于真实标签是独热编码，上式可以简化为：
$$\mathcal{L}_{CE} = -\frac{1}{N}\sum_{i=1}^{N} \log(\hat{y}_{i, c_i})$$
其中，$c_i$ 为第 $i$ 个像素的真实类别。这意味着交叉熵损失本质上是在惩罚模型对真实类别预测概率的不足：当模型对真实类别给出接近1的概率时，$-\log(\hat{y}_{i, c_i})$ 趋近于0，损失很小；当模型对真实类别给出接近0的概率时，损失值急剧增大。
交叉熵损失对每个像素独立计算，能够提供稳定的逐像素梯度信号，有利于模型在训练初期快速收敛。然而，它的一个显著局限是对类别不平衡问题敏感。由于损失值是所有像素损失的平均，当某一类别的像素数量远多于其他类别时，该类别对总损失的贡献占据主导地位，导致模型倾向于将更多像素预测为多数类以降低整体损失，从而忽视了少数类的分割质量。
Dice损失（Dice Loss）从区域重叠的角度评估分割质量，能够有效缓解类别不平衡问题。Dice损失基于Dice系数设计，Dice系数最初用于衡量两个集合之间的相似度，在语义分割中用于度量预测区域与真实区域的重叠程度。对于某一类别，Dice系数可以直观理解为"预测正确的区域面积占预测区域与真实区域总面积的比例"。Dice损失定义为：
$$\mathcal{L}_{Dice} = 1 - \frac{2\sum_{i=1}^{N} y_i \hat{y}_i + \epsilon}{\sum_{i=1}^{N} y_i + \sum_{i=1}^{N} \hat{y}_i + \epsilon}$$
其中，$y_i$ 为第 $i$ 个像素的真实标签（0或1），$\hat{y}_i$ 为模型对该像素的预测概率，$\epsilon$ 为平滑因子（通常取 $10^{-5}$），用于避免分母为零的数值问题。分子中的 $\sum y_i \hat{y}_i$ 表示预测区域与真实区域的"交集"（以概率形式近似），分母中的 $\sum y_i + \sum \hat{y}_i$ 表示两个区域的面积之和。
Dice损失之所以对类别不平衡具有较好的鲁棒性，关键在于其计算方式：它关注的是预测区域与真实区域的相对重叠比例，而非绝对的像素数量。对于一个仅占图像1%面积的少数类别，即使其像素数量很少，只要模型对该区域的预测不准确，Dice系数的分子（交集）就会明显偏小，导致损失值增大。这使得少数类别在损失函数中获得了与其面积占比不成正比的、更高的优化权重。
然而，Dice损失也有其不足之处。由于它是基于整个区域计算的全局指标，每个像素的梯度信号不仅取决于自身的预测值，还受到同类别所有其他像素预测值的影响，这导致梯度信号的稳定性不如交叉熵损失，在训练初期可能出现较大的波动。
基于以上分析，本文采用交叉熵损失与Dice损失的联合损失来训练模型：
$$\mathcal{L} = \mathcal{L}_{CE} + \mathcal{L}_{Dice}$$
两者的结合取长补短：交叉熵损失提供稳定的逐像素梯度信号，确保训练过程的平稳收敛；Dice损失从区域重叠的角度进行优化，增强模型对少数类别的关注，提升整体分割质量。
状态空间模型
状态空间模型是本文网络架构的核心理论基础，DP-UNet解码器中的DVSS模块即以状态空间模型作为全局上下文建模的核心引擎。本节将从连续状态空间模型的基本定义出发，阐述其在深度学习中的离散化实现方式，进而分析从S4到Mamba的关键技术突破，最后介绍将一维SSM扩展到二维图像处理的SS2D机制。
连续状态空间模型
状态空间模型起源于控制理论和信号处理领域，是一类通过隐状态的线性动力学方程来描述系统输入输出关系的数学模型。其基本思想是：系统在任意时刻的行为可以通过一组"隐状态"来完整描述，这些隐状态在外部输入的驱动下随时间演化，而系统的输出则由当前隐状态决定。
连续时间的状态空间模型由以下两个方程定义：
$$h'(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$$
$$y(t) = \mathbf{C}h(t) + \mathbf{D}x(t)$$
第一个方程称为状态方程，描述了隐状态 $h(t) \in \mathbb{R}^{N}$ 如何在时间维度上演化。方程右侧包含两项：$\mathbf{A}h(t)$ 表示隐状态的自演化，即系统在没有外部输入时隐状态自身的动态变化规律，状态转移矩阵 $\mathbf{A} \in \mathbb{R}^{N \times N}$ 决定了这种自演化的方向和速率；$\mathbf{B}x(t)$ 表示外部输入 $x(t)$ 对隐状态的影响，输入矩阵 $\mathbf{B} \in \mathbb{R}^{N \times 1}$ 决定了输入信号以何种方式被写入隐状态。直观理解，隐状态就像一个"记忆容器"，它持续地从输入中吸收新信息（通过 $\mathbf{B}$），同时按照固定的规律更新和遗忘旧信息（通过 $\mathbf{A}$）。
第二个方程称为观测方程，描述了如何从隐状态中提取输出信息。输出矩阵 $\mathbf{C} \in \mathbb{R}^{1 \times N}$ 决定了从 $N$ 维隐状态中选取哪些维度的信息用于生成输出 $y(t)$。直连矩阵 $\mathbf{D} \in \mathbb{R}$ 提供了输入到输出的直接通路，在实际应用中通常设置为零，因为这种直连效果可以通过网络中的残差连接来实现。
状态空间模型与循环神经网络（RNN）在形式上具有相似性——两者都通过隐状态的递推来积累序列信息。然而，SSM的关键优势在于其线性动力学特性：状态方程中不包含非线性激活函数，隐状态的演化完全由线性运算驱动。这一特性使得SSM可以被转化为全局卷积的形式进行高效并行计算（详见2.3.2节），克服了RNN只能逐步递推、无法并行训练的根本缺陷。
离散化与计算实现
深度学习中的数据通常以离散序列的形式存在（如经过分块处理的图像特征序列），因此需要将上述连续时间的SSM离散化为离散时间系统。离散化过程的核心是引入一个步长参数 $\Delta > 0$，它控制连续模型在时间轴上的采样间隔——较大的 $\Delta$ 意味着采样更稀疏，系统对输入的响应更慢但覆盖的时间范围更广；较小的 $\Delta$ 意味着采样更密集，系统对输入的响应更灵敏但关注的时间范围较窄。
采用零阶保持（Zero-Order Hold, ZOH）方法进行离散化。ZOH方法假设在每个采样间隔 $\Delta$ 内，输入信号保持恒定。在此假设下，连续系统的精确离散化结果为：
$$\overline{\mathbf{A}} = \exp(\Delta \mathbf{A})$$
$$\overline{\mathbf{B}} = (\Delta \mathbf{A})^{-1}(\exp(\Delta \mathbf{A}) - \mathbf{I}) \cdot \Delta \mathbf{B}$$
其中，$\overline{\mathbf{A}}$ 和 $\overline{\mathbf{B}}$ 分别为离散化后的状态转移矩阵和输入矩阵，$\exp(\cdot)$ 为矩阵指数运算，$\mathbf{I}$ 为单位矩阵。离散化后的系统方程变为：
$$h_t = \overline{\mathbf{A}} h_{t-1} + \overline{\mathbf{B}} x_t$$
$$y_t = \mathbf{C} h_t$$
上述递推公式的物理含义非常直观：在第 $t$ 个时间步，新的隐状态 $h_t$ 由两部分组成——前一时刻隐状态 $h_{t-1}$ 经过矩阵 $\overline{\mathbf{A}}$ 变换后保留的历史信息，加上当前输入 $x_t$ 经过矩阵 $\overline{\mathbf{B}}$ 编码后注入的新信息。矩阵 $\overline{\mathbf{A}}$ 可以理解为"记忆衰减系数"，它决定了历史信息在传递过程中被保留多少、遗忘多少；矩阵 $\overline{\mathbf{B}}$ 可以理解为"输入门控"，它决定了新输入以何种方式、多大强度被写入记忆。

离散SSM的一个重要特性是它同时具备**递推**和**卷积**两种等价的计算模式：
递推模式：按照上述递推公式逐步计算 $h_1, h_2, \ldots, h_L$，每一步的计算仅依赖前一步的结果和当前输入。这种模式的计算复杂度为 $O(L)$（$L$ 为序列长度），且每一步仅需常数级的存储空间，非常适合推理阶段的自回归生成。但其缺点是无法并行——第 $t$ 步的计算必须等待第 $t-1$ 步完成后才能进行。
卷积模式：将递推公式展开后，可以发现输出序列 $y$ 可以表示为输入序列 $x$ 与一个固定卷积核 $\overline{\mathbf{K}}$ 的卷积：
$$\overline{\mathbf{K}} = (\mathbf{C}\overline{\mathbf{B}}, \mathbf{C}\overline{\mathbf{A}}\overline{\mathbf{B}}, \mathbf{C}\overline{\mathbf{A}}^2\overline{\mathbf{B}}, \ldots, \mathbf{C}\overline{\mathbf{A}}^{L-1}\overline{\mathbf{B}})$$
$$y = x * \overline{\mathbf{K}}$$
该卷积核的第 $t$ 个元素 $\mathbf{C}\overline{\mathbf{A}}^t\overline{\mathbf{B}}$ 表示的是：距离当前位置 $t$ 步之前的输入对当前输出的影响强度。随着 $t$ 的增大，如果 $\overline{\mathbf{A}}$ 的特征值模小于1，则影响逐渐衰减，体现了"近期信息影响大、远期信息影响小"的记忆特性。这种卷积形式可以利用快速傅里叶变换（FFT）在 $O(L \log L)$ 的时间内完成计算，且所有位置的输出可以并行得到，非常适合训练阶段的高效并行计算。
SSM同时支持递推和卷积两种计算模式，使其在训练效率和推理效率上均具有优势，这是其相比Transformer和RNN的一个重要特点。
从S4到Mamba的演进
理解了SSM的基本原理后，接下来分析其在深度学习序列建模中的关键技术演进。这一演进过程围绕一个核心问题展开：如何让SSM在保持线性计算效率的同时，具备对输入内容的选择性处理能力。
S4（Structured State Space for Sequence Modeling）是2022年Gu等人提出的结构化状态空间模型，是将SSM成功应用于深度学习序列建模的里程碑工作。S4面临的首要挑战是：在实际训练中，状态转移矩阵 $\mathbf{A}$ 的矩阵指数运算 $\exp(\Delta \mathbf{A})$ 在 $\mathbf{A}$ 为一般矩阵时计算成本极高，且容易出现数值不稳定的问题（特征值过大导致梯度爆炸，特征值过小导致梯度消失）。
S4通过两项关键技术解决了这些问题。第一，S4对状态转移矩阵 $\mathbf{A}$ 采用了HiPPO（High-order Polynomial Projection Operators）初始化策略。HiPPO理论指出，特定结构的 $\mathbf{A}$ 矩阵能够使隐状态最优地压缩和记忆连续信号的历史信息——具体而言，隐状态的每个维度对应一个正交多项式的系数，整个隐状态向量就构成了对历史输入信号的最优多项式逼近。这种初始化使得SSM从一开始就具备了良好的长距离记忆能力。第二，S4将 $\mathbf{A}$ 矩阵参数化为对角加低秩（DPLR）的特殊结构，利用这种结构的数学性质将矩阵指数运算和卷积核计算转化为高效的频域操作，使得S4可以在 $O(L \log L)$ 的时间复杂度内处理长度为 $L$ 的序列。
S4在Long Range Arena等长序列建模基准测试中首次取得了突破性成果，证明了SSM在捕获长距离依赖关系方面的巨大潜力。然而，S4存在一个根本性的局限：其参数 $\mathbf{A}$、$\mathbf{B}$、$\mathbf{C}$ 和 $\Delta$ 在训练完成后即固定不变，对所有输入使用完全相同的状态转移规则。这种**线性时不变**（Linear Time-Invariant, LTI）特性意味着模型无法根据输入内容自适应地调整信息处理策略。例如，当处理一段包含重要地物特征和无关背景的像素序列时，S4对两者使用完全相同的"记忆-遗忘"规则，无法做到"选择性地保留重要信息、过滤无关噪声"。
**Mamba** 是2023年Gu和Dao提出的选择性状态空间模型，其核心创新在于将S4的固定参数升级为**输入依赖的动态参数**，从根本上赋予了SSM对输入内容的选择性处理能力。具体而言，Mamba将 $\mathbf{B}$、$\mathbf{C}$ 矩阵和步长参数 $\Delta$ 参数化为输入 $x$ 的函数：
$$\mathbf{B}_t = s_B(x_t), \quad \mathbf{C}_t = s_C(x_t), \quad \Delta_t = \text{softplus}(s_\Delta(x_t))$$
其中，$s_B$、$s_C$ 和 $s_\Delta$ 为可学习的线性投影。这种输入依赖的参数化赋予了模型内容感知的选择性：
步长 $\Delta_t$ 的选择性：$\Delta_t$ 控制着离散化的精度和隐状态的更新幅度。当模型遇到重要的输入时，可以学习产生较大的 $\Delta_t$，使得 $\overline{\mathbf{B}}_t$ 增大、$\overline{\mathbf{A}}_t$ 趋向于遗忘旧信息，从而让当前输入被强力写入隐状态并覆盖先前的记忆；当遇到不相关的输入时，模型产生较小的 $\Delta_t$，使得隐状态几乎不受当前输入的影响而保持先前的记忆。这种机制类似于LSTM中的"输入门"和"遗忘门"，但以更简洁高效的方式实现。
输入矩阵 $\mathbf{B}_t$ 的选择性：$\mathbf{B}_t$ 决定了当前输入如何被编码到隐状态中。输入依赖的 $\mathbf{B}_t$ 使模型能够根据输入内容选择性地提取不同维度的信息写入隐状态。
输出矩阵 $\mathbf{C}_t$ 的选择性：$\mathbf{C}_t$ 决定了从隐状态中读取哪些信息作为输出。输入依赖的 $\mathbf{C}_t$ 使模型能够根据当前上下文动态调整输出的信息组合。
然而，参数的输入依赖性也带来了计算上的挑战：由于 $\overline{\mathbf{A}}_t$、$\overline{\mathbf{B}}_t$、$\mathbf{C}_t$ 在每个时间步都不同，S4中基于固定卷积核和FFT的高效并行计算方式不再适用——固定卷积核的前提是系统参数在所有时间步上保持一致，而Mamba的参数是时变的。为此，Mamba设计了**硬件感知的并行扫描算法**，将递推计算重构为并行前缀和运算，充分利用GPU的并行计算架构，同时采用核融合和重计算等技术优化GPU显存访问模式，在保持 $O(L)$ 理论复杂度的同时实现了接近卷积模式的实际运行速度。
[此处插入图：S4与Mamba的核心区别对比图。左侧为S4，A/B/C/Δ标注为"固定参数"（训练后不变），每个时间步使用相同的规则处理输入。右侧为Mamba，B/C/Δ标注为"由输入动态生成"，每个时间步根据输入内容使用不同的规则。用不同粗细的箭头示意不同时间步参数的变化。]
二维选择性扫描机制（SS2D）
Mamba最初为一维序列数据（如文本、语音）设计，将其应用于二维图像数据面临一个根本性挑战：图像具有天然的二维空间结构，像素之间在水平、垂直和对角线等多个方向上存在空间依赖关系，而SSM本质上是一维序列模型，只能沿单一方向进行递推扫描。
如果简单地采用逐行扫描（raster scan）将二维图像展开为一维序列，会产生一个严重问题：同一行内相邻像素之间的依赖关系可以被SSM直接建模（因为它们在序列中相邻），但不同行之间在垂直方向上相邻的像素在展开后的序列中相隔了整整一行的长度（即 $W$ 个位置）。虽然SSM理论上可以通过隐状态的逐步传递来捕获这种跨行依赖，但信息需要经过 $W$ 步才能从一个像素传递到其正下方的像素，传递效率低下且容易出现信息衰减。更严重的是，单一方向的扫描引入了方向性偏差——沿扫描方向上游的像素可以影响下游像素，但下游像素的信息无法反向传递给上游像素。
为解决这些问题，VMamba提出了二维选择性扫描模块（2D Selective Scan, SS2D）。SS2D的核心设计思想是：沿多个互补方向分别展开二维特征图为一维序列，对每个方向独立进行SSM扫描，最后将各方向的结果进行融合。通过多方向的协同扫描，确保特征图上任意两个位置之间都存在高效的信息传递路径。

具体而言，SS2D采用四个扫描方向：
方向1（左上→右下）：逐行从左到右扫描，对应标准的行优先展开顺序。该方向能够有效捕获从左上区域向右下区域的空间依赖关系。
方向2（右下→左上）：方向1的逆序扫描，使得右下区域的信息能够向左上区域传递，弥补方向1中信息只能单向流动的缺陷。
方向3（右上→左下）：逐行从右到左扫描。该方向与方向1构成水平方向的互补，能够捕获从右上区域向左下区域的依赖关系。
方向4（左下→右上）：方向3的逆序扫描，与方向3构成完整的双向覆盖。

[此处插入图：SS2D四方向扫描示意图。中间展示一个4×4的二维特征图网格，四周用四种不同颜色的箭头路径分别标注四个扫描方向。在图下方，展示每个方向将2D特征图展开为1D序列后通过SSM处理的过程。最后，展示四个方向的输出如何通过逐元素求和合并为最终输出。]

通过这种四方向交叉扫描策略，特征图上任意两个位置 $(i_1, j_1)$ 和 $(i_2, j_2)$ 之间至少存在两条扫描路径使信息可达（一条正向路径和一条反向路径）。以特征图左上角和右下角两个位置为例：方向1中，左上角的信息可以沿着扫描顺序传递到右下角；方向2中，右下角的信息可以沿逆序传递到左上角。这样，无论两个位置的空间关系如何，都能通过至少一个扫描方向实现高效的信息交互。
SS2D模块的完整处理流程如下：输入特征 $X \in \mathbb{R}^{B \times H \times W \times C}$ 首先通过线性投影扩展通道维度，然后分为两个并行分支。第一个分支经过深度可分离卷积提取局部特征后，执行四方向扫描——将特征图分别按四个方向展开为一维序列，对每个方向独立进行选择性SSM递推扫描，再将四个方向的输出合并（逐元素求和）。第二个分支经过SiLU激活函数，作为门控信号对扫描结果进行调制。两个分支的输出逐元素相乘后，通过线性投影恢复至原始通道维度，得到最终输出特征。整个过程的计算复杂度为 $O(H \times W)$，与特征图的像素数呈线性关系，远低于Transformer自注意力的 $O(H^2 \times W^2)$ 复杂度。
本文在DP-UNet的DVSS模块中采用SS2D作为共享基座，利用其高效的全局建模能力为后续的全局路径和局部路径提供共同的基础特征表示。


注意力机制
注意力机制是深度学习中一种重要的特征增强技术，其设计灵感源于人类视觉系统的选择性注意机制：在面对复杂场景时，人类视觉系统并非均匀地处理视野中的所有信息，而是将有限的注意资源集中在与当前任务最相关的区域或特征上。在深度学习中，注意力机制通过动态计算特征的重要性权重，使模型能够自适应地聚焦于最有价值的信息，抑制不相关或冗余的信息。
根据作用维度的不同，注意力机制可以分为通道注意力（关注"哪些特征通道更重要"）、空间注意力（关注"哪些空间位置更重要"）以及自注意力/交叉注意力（关注"哪些位置之间存在强关联"）等类型。本文在不同模块中分别采用了这三种注意力机制，下面逐一介绍其原理和实现。
通道注意力机制
在卷积神经网络中，每个卷积层的输出特征图包含多个通道，不同通道对应不同类型的特征响应——有的通道可能捕获了边缘信息，有的通道可能响应了颜色变化，有的通道可能检测了特定的纹理模式。然而，标准卷积操作对所有通道赋予相同的处理权重，并未区分不同通道的重要性差异。在实际任务中，并非所有通道的特征都同等重要：对于建筑物的分割，边缘和几何特征通道可能更为关键；对于植被的分割，颜色和纹理特征通道可能更为重要。通道注意力机制的目标正是让模型自动学习各通道的相对重要性，增强有用通道的权重、抑制不相关通道的干扰。
SE（Squeeze-and-Excitation）注意力是通道注意力的经典方法，通过"压缩"和"激励"两个阶段实现通道重标定。
压缩（Squeeze）阶段的目的是获取每个通道的全局信息摘要。对于一个 $H \times W \times C$ 的特征图，每个通道是一个 $H \times W$ 的二维特征响应图，包含了丰富的空间信息。为了评估该通道的整体重要性，需要将其二维空间信息压缩为一个标量。SE模块通过全局平均池化实现这一目标：
$$z_c = \frac{1}{H \times W}\sum_{i=1}^{H}\sum_{j=1}^{W} X_{i,j,c}$$
其中，$z_c$ 为第 $c$ 个通道的全局描述符。这一操作将每个通道的空间信息浓缩为其平均激活强度，得到一个 $C$ 维的通道描述向量 $z \in \mathbb{R}^{C}$。
激励（Excitation）阶段的目的是基于通道描述向量学习通道间的依赖关系，生成每个通道的重要性权重。SE模块通过一个瓶颈结构的全连接网络来实现：
$$s = \sigma(\mathbf{W}_2 \cdot \delta(\mathbf{W}_1 \cdot z))$$

其中，$\mathbf{W}_1 \in \mathbb{R}^{\frac{C}{r} \times C}$ 为降维投影矩阵，将 $C$ 维描述向量压缩为 $\frac{C}{r}$ 维（$r$ 为压缩比，通常取16），$\delta$ 为ReLU激活函数引入非线性，$\mathbf{W}_2 \in \mathbb{R}^{C \times \frac{C}{r}}$ 为升维投影矩阵，将特征恢复至 $C$ 维，$\sigma$ 为Sigmoid激活函数，将输出值约束在 $[0, 1]$ 范围内。最终，权重向量 $s$ 与输入特征图逐通道相乘：$\hat{X}_c = s_c \cdot X_c$，实现通道级别的特征重标定。
SE模块的降维操作虽然减少了参数量，但也带来了信息损失——$C$ 维的通道交互关系被压缩至 $\frac{C}{r}$ 维的瓶颈空间中，可能丢失部分通道间的细微依赖关系。此外，两层全连接网络引入的参数量为 $\frac{2C^2}{r}$，在通道数较大时仍有可观的计算开销。
ECA（Efficient Channel Attention）注意力针对SE模块的上述局限进行了轻量化改进，本文在DVSS模块的局部路径中采用ECA进行通道重标定。ECA的核心改进思想是：通道间的依赖关系实际上主要存在于相邻通道之间，而非所有通道之间都需要建模全局依赖。基于这一观察，ECA用一个核大小为 $k$ 的一维卷积替代了SE模块中的两层全连接网络：
$$s = \sigma(\text{Conv1D}_k(z))$$
其中，一维卷积核 $\text{Conv1D}_k$ 在通道维度上滑动，每个通道的权重仅由其邻域内 $k$ 个通道的描述符共同决定。卷积核大小 $k$ 可以根据通道数 $C$ 自适应确定：
$$k = \psi(C) = \left|\frac{\log_2 C}{\gamma} + \frac{b}{\gamma}\right|_{odd}$$
其中，$\gamma$ 和 $b$ 为超参数（通常取 $\gamma=2, b=1$），$|\cdot|_{odd}$ 表示取最近的奇数。这一公式的设计思想是：通道数越多，通道间的依赖范围可能越广，因此需要更大的卷积核。
[此处插入图：SE模块与ECA模块的结构对比图。左侧SE模块：输入特征图→全局平均池化→FC(C→C/r)→ReLU→FC(C/r→C)→Sigmoid→通道加权。右侧ECA模块：输入特征图→全局平均池化→1D Conv(kernel=k)→Sigmoid→通道加权。用红色标注ECA省略了全连接层的降维升维过程。]
与SE模块相比，ECA具有两个显著优势。第一，ECA避免了通道维度的降维操作，所有通道的信息在注意力计算过程中得以完整保留，不会因瓶颈压缩而丢失通道间的细微依赖关系。第二，一维卷积的参数量仅为 $k$（通常为3或5），远小于SE模块中两层全连接层的参数量 $\frac{2C^2}{r}$，计算开销大幅降低。本文选择ECA作为DVSS局部路径中的通道注意力模块，正是出于其在轻量化和信息保留之间的良好平衡。
空间注意力机制
通道注意力解决了"哪些特征通道更重要"的问题，但它对每个通道施加统一的权重标量，无法区分同一通道内不同空间位置的重要性差异。然而，在语义分割任务中，不同空间位置的重要性往往存在显著差异：目标区域（如建筑物、道路）的像素位置对分割结果至关重要，而背景区域的像素位置则相对不重要。空间注意力机制的目标是生成一个与输入特征图具有相同空间尺寸的权重图，使模型能够自适应地增强目标区域的特征响应、抑制背景区域的干扰。
空间注意力的核心问题是：如何从多通道特征图中推断出各空间位置的相对重要性。一种有效的方法是利用通道维度的统计信息来反映空间位置的显著程度——如果某个空间位置在多数通道上都具有较强的激活响应，则该位置很可能对应重要的目标区域。基于这一思想，空间注意力模块首先沿通道维度分别进行平均池化和最大池化：
$$X_{avg} = \frac{1}{C}\sum_{c=1}^{C} X_{:,:,c} \in \mathbb{R}^{H \times W \times 1}$$
$$X_{max} = \max_{c} X_{:,:,c} \in \mathbb{R}^{H \times W \times 1}$$
平均池化反映了每个空间位置上所有通道的平均响应强度——平均值较高的位置通常具有较为一致的多通道激活，表明该位置可能是重要的目标区域。最大池化则捕获了每个空间位置上最显著的单通道响应——最大值较高的位置通常包含至少一种强烈的特征响应，可能对应突出的边缘、纹理或目标区域。两种池化方式从不同角度提取空间位置的显著性信息，相互补充。
将两个描述符沿通道维度拼接为一个2通道的特征图，然后通过一个 $7 \times 7$ 的卷积层（较大的卷积核用于捕获空间位置之间的局部关系）和Sigmoid激活函数生成空间注意力图：
$$M_s = \sigma(f^{7 \times 7}([X_{avg}; X_{max}]))$$
最终，空间注意力图 $M_s \in \mathbb{R}^{H \times W \times 1}$ 通过广播机制与输入特征图逐元素相乘，实现对不同空间位置的差异化增强：
$$\hat{X} = M_s \odot X$$
本文在MSK模块中采用了上述空间注意力机制进行多尺度融合特征的空间增强。选择纯空间注意力而非通道-空间混合注意力的设计考量将在第三章方法部分详细阐述。
自注意力与交叉注意力机制
通道注意力和空间注意力都属于轻量级的特征重标定机制，它们通过为通道或空间位置分配权重来增强特征，但并未建模特征图中不同位置之间的显式关联关系。在许多场景中，理解位置间的长距离关联对于语义分割至关重要——例如，遥感图像中一条道路的不同部分虽然在空间上可能相距较远，但它们之间存在强烈的语义关联，模型需要将它们识别为同一类别。自注意力机制正是为建模这种位置间的全局关联关系而设计。
自注意力机制的核心思想是：对于序列中的每个位置，计算它与所有其他位置之间的相关性，然后根据相关性从全局范围内聚合最相关的信息。这一过程通过查询（Query）、键（Key）和值（Value）三组向量的交互来实现。
给定输入特征 $X \in \mathbb{R}^{N \times d}$（$N$ 为序列长度或图像中的像素数，$d$ 为特征维度），首先通过三个独立的线性投影将输入映射为三组向量：
$$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$
这三组向量的物理含义可以用"信息检索"的类比来理解：查询$Q$代表每个位置"想要什么信息"的需求描述；键$K$代表每个位置"拥有什么信息"的索引标签；值$V$代表每个位置实际存储的信息内容。注意力计算的过程就是每个位置用自己的查询去匹配所有位置的键，找到最相关的位置，然后从这些位置的值中提取信息。
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
这个公式可以分解为三个步骤理解。第一步，计算注意力分数矩阵 $QK^T \in \mathbb{R}^{N \times N}$，其中第 $(i, j)$ 个元素 $q_i \cdot k_j^T$ 表示第 $i$ 个位置的查询与第 $j$ 个位置的键之间的点积相似度——点积值越大，说明两个位置之间的关联性越强。第二步，除以缩放因子 $\sqrt{d_k}$。这一步的必要性在于：当特征维度 $d_k$ 较大时，点积的数值会随 $d_k$ 的增大而增大，导致Softmax函数的输入值过大，输出的概率分布趋于"尖锐"（即只关注少数几个位置），梯度趋于零，不利于训练。除以 $\sqrt{d_k}$ 可以将点积值的方差稳定在适当范围内。第三步，对缩放后的注意力分数应用Softmax函数进行归一化，使每个查询位置的注意力权重之和为1，形成概率分布。最后，用注意力权重对值向量 $V$ 进行加权求和，得到每个位置的输出——该输出是全局所有位置信息的加权聚合，权重由位置间的语义相关性决定。
在实际应用中，通常采用多头注意力（Multi-Head Attention）机制。其动机是：单一的注意力计算只能学习一种类型的位置关联模式，而图像中的位置关联可能是多样的（如纹理相似性、几何邻近性、语义共属性等）。多头机制将输入特征沿通道维度划分为 $h$ 个子空间（头），在每个子空间中独立进行注意力计算：
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W_O$$
$$\text{head}_i = \text{Attention}(QW_Q^i, KW_K^i, VW_V^i)$$
每个头可以学习不同类型的关联模式，最后通过拼接和线性投影将多种模式的信息整合。
交叉注意力机制（Cross-Attention） 是自注意力的自然扩展，区别在于查询与键/值来自不同的输入源。在自注意力中，$Q$、$K$、$V$ 均由同一输入 $X$ 生成，建模的是单一序列内部各位置之间的关联。而在交叉注意力中，查询来自输入 $X_1$，键和值来自另一个输入 $X_2$：
$$Q = X_1 W_Q, \quad K = X_2 W_K, \quad V = X_2 W_V$$
$$\text{CrossAttn}(X_1, X_2) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
交叉注意力的物理含义可以理解为：以 $X_1$ 的特征作为"需求方"，从 $X_2$ 的特征中"按需检索"最相关的信息。对于多模态融合任务而言，这种机制尤为适用——例如，以光学图像的特征作为查询，从SAR图像的特征中检索与光学特征语义最匹配的互补信息。与简单的特征拼接或逐元素相加相比，交叉注意力能够根据内容的语义相关性进行精细化的信息选取，避免无关或冲突信息的引入。本文在多模态融合网络的CrossModalMSK模块中正是采用交叉注意力机制来实现光学与SAR特征之间的跨模态信息交互。
多模态遥感数据特性
本文的第二项工作涉及光学图像与SAR图像的多模态融合分割。两种模态的遥感数据在成像原理上存在本质差异，充分理解各自的数据特性和互补关系，是设计有效融合策略的前提。
光学遥感图像
光学遥感图像通过被动接收地表反射或辐射的可见光及近红外波段电磁波进行成像。其工作原理与人眼类似：太阳光照射地表后，不同地物类型的表面材质会对不同波长的光产生差异化的反射，传感器接收这些反射光并转化为数字图像。正是这种波长选择性反射特性，赋予了光学图像丰富的光谱信息——例如，健康植被在近红外波段反射率极高而在红色波段反射率较低，水体在近红外波段几乎完全吸收而在蓝色波段反射率较高，建筑物的反射率则在各波段上都较为均匀。通过多波段的光谱组合和比值分析，可以有效区分不同的地物类型。
除光谱信息外，高分辨率光学图像还能提供清晰的空间纹理和视觉边缘信息，使得地物的形状、结构和空间关系一目了然。这些特性使光学图像成为遥感应用中使用最广泛的数据源。
然而，光学遥感成像的被动工作方式也带来了固有的局限性。第一，光学成像依赖太阳光照射，因此只能在白天进行数据采集，夜间无法成像。第二，可见光和近红外波段的电磁波无法穿透云层、雾霾和浓烟，当目标区域被云层覆盖时，光学传感器获取的图像中被遮挡区域完全无法获取有效的地表信息。在多云多雨的热带和亚热带地区，获取无云光学图像的窗口期十分有限，严重制约了光学遥感数据的时效性。第三，光学图像对光照角度和强度变化敏感，建筑物和树木的阴影区域、高光反射区域等都会干扰地物的正常光谱表现，导致同一地物在不同光照条件下呈现截然不同的影像特征，给分割模型的稳定性带来挑战。
SAR遥感图像
合成孔径雷达（SAR）是一种主动微波遥感传感器，其工作原理与光学传感器截然不同：SAR自身发射微波脉冲信号照射地表，然后接收地表反射回来的回波信号，通过对回波信号进行处理和分析生成地表图像。SAR的工作波段为微波波段（波长通常在厘米至分米量级），远长于光学波段（波长在纳米量级）。
SAR最突出的优势在于其全天候、全天时的成像能力。由于微波波长远大于云层水滴和雾霾颗粒的尺寸，微波可以穿透云层、雨雾和轻度烟尘等大气遮挡物而几乎不发生衰减。同时，SAR自身发射微波脉冲，不依赖太阳光照，因此可以在白天和夜间任何时间进行成像。这一特性使SAR在光学传感器无法工作的恶劣天气条件下仍然能够提供有效的地表观测数据，在洪水灾害监测、地震灾情评估等需要快速响应的应急场景中具有不可替代的价值。
SAR图像的灰度值反映的是地表对微波的后向散射强度，与地物的物理和几何特性密切相关。影响后向散射强度的主要因素包括：地物表面的介电常数（含水量越高，散射越强）、表面粗糙度（粗糙表面产生更强的散射）、地物的几何形状（垂直墙面和金属结构会产生强烈的角反射）以及微波的入射角度。因此，SAR图像能够提供光学图像无法获取的地表物理结构信息。
然而，SAR图像也面临若干固有的挑战。首先，相干斑噪声是SAR图像最显著的问题。相干斑噪声由微波的相干干涉效应产生——当微波照射粗糙表面时，来自不同散射体的回波信号之间发生相干叠加，产生随机的增强和抵消效应，在图像中表现为颗粒状的明暗斑点。相干斑噪声的存在严重降低了SAR图像的视觉质量，使得地物的边界和纹理信息被噪声掩盖，大大增加了自动化解译的难度。其次，SAR图像缺乏光学图像中的光谱信息。标准的单极化SAR图像仅包含一个通道的散射强度信息，无法像多波段光学图像那样通过光谱差异来区分地物类型，导致对某些光谱特性差异显著但散射特性相似的地物（如不同作物类型）区分能力有限。
[此处插入图：光学图像与SAR图像对比示意图。选取WHU-OPT-SAR数据集中的同一区域，左侧展示光学图像（RGB合成），右侧展示SAR图像（灰度），下方展示语义标注图。标注两种图像在视觉表现上的差异：光学图像色彩丰富、纹理清晰，SAR图像噪声明显但建筑结构突出。]
多模态互补性分析
从上述分析可以看出，光学图像和SAR图像在信息维度上具有天然的互补性，如表2-1所示。
[此处插入表格：表2-1 光学图像与SAR图像特性对比]
| 特性 | 光学图像 | SAR图像 |
|------|---------|---------|
| 成像方式 | 被动接收反射光 | 主动发射微波脉冲 |
| 工作波段 | 可见光、近红外 | 微波（L/C/X波段） |
| 成像条件 | 依赖日照，受云雨影响 | 全天候、全天时 |
| 信息类型 | 光谱反射特性 | 后向散射特性 |
| 优势 | 光谱丰富、纹理清晰 | 穿透云层、结构信息 |
| 局限 | 云遮挡、光照敏感 | 相干斑噪声、缺乏光谱 |
通过融合两种模态的互补信息，可以在多个方面实现优势互补。在数据可用性方面，当光学图像因云层覆盖而无法提供有效信息时，SAR数据可以填补信息缺口，确保地表观测的连续性。在地物区分能力方面，光学图像丰富的光谱信息可以增强SAR数据有限的类别判别能力，而SAR图像的几何结构信息可以辅助光学图像在阴影区域和光照不均匀区域的地物识别。在特征表达方面，融合两种模态能够从光谱、纹理、结构等多个维度全面表征地物特征，提供比任何单一模态更完整的信息。
然而，两种模态在数据特性上的巨大差异也给融合带来了显著挑战。首先，两种模态的通道数不同——光学图像通常包含3或4个通道（RGB或RGBNIR），SAR图像通常为单通道。其次，两种模态的值域分布差异显著，光学图像的像素值通常为0-255的正整数，SAR图像的散射强度可能以分贝为单位表示，包含负值且动态范围不同。此外，两种模态的噪声特性完全不同——光学图像主要受高斯噪声影响，SAR图像则受乘性的相干斑噪声影响。这些异质性要求融合算法能够有效地弥合模态鸿沟，在保持各模态特异性特征的同时实现深层次的语义对齐和信息互补。
数据集与评价指标
ISPRS Potsdam数据集
ISPRS Potsdam数据集由国际摄影测量与遥感学会（ISPRS）提供，是遥感图像语义分割领域广泛使用的标准基准数据集之一。该数据集覆盖德国波茨坦市的城市区域，以其高分辨率和精细标注著称。数据集包含38幅6000×6000像素的航空正射影像，地面采样距离（Ground Sample Distance, GSD）为5厘米。每幅图像包含近红外（NIR）、红色（R）和绿色（G）三个通道（NIRRG），同时提供对应的数字表面模型（DSM）数据。数据集标注了6个语义类别：不透水面（Impervious Surface）、建筑物（Building）、低矮植被（Low Vegetation）、树木（Tree）、车辆（Car）和背景/杂波（Clutter/Background），在评价时通常只计算前5个类别的指标。
Potsdam数据集的主要挑战在于：第一，城市场景中不同类别的地物紧密交织，建筑物与不透水面的光谱特征高度相似，容易产生混淆；第二，车辆目标尺度较小，仅占图像中极少的像素比例，对模型的小目标检测能力提出了较高要求；第三，树木和低矮植被虽然同属植被类别，但需要被区分为两个独立的类别，要求模型能够捕获植被类内的细微差异。
在本文实验中，按照常用的实验设置，选取23幅图像用于训练（排除标注存在错误的第710号图像），14幅图像用于测试。训练和测试时，将原始图像裁剪为512×512像素的小块进行处理。
[此处插入图：ISPRS Potsdam数据集样例。展示2-3组样例，每组包含NIRRG正射影像和对应的语义标注图（不同颜色代表不同类别）。附图例说明各颜色对应的地物类别。]
ISPRS Vaihingen数据集
ISPRS Vaihingen数据集同样由ISPRS提供，覆盖德国Vaihingen城市区域。该数据集包含33幅高分辨率航空正射影像，图像大小不一（平均约2494×2064像素），GSD为9厘米。每幅图像同样包含NIRRG三个通道和对应的DSM数据。语义类别与Potsdam数据集完全一致，包含不透水面、建筑物、低矮植被、树木、车辆和背景/杂波共6个类别。
与Potsdam数据集相比，Vaihingen数据集的GSD稍大（9厘米 vs 5厘米），意味着每个像素覆盖的地面面积更大，图像中地物的细节信息相对较少。此外，Vaihingen的城市建筑密度较高，建筑物之间的间距较小，阴影遮挡现象更为显著，增加了建筑物边界提取和阴影区域地物识别的难度。同时，Vaihingen的总数据量较Potsdam更少，对模型在有限训练数据条件下的泛化能力提出了更高要求。这些差异使得Vaihingen数据集成为评估模型跨场景泛化能力的有价值补充。
在本文实验中，按照常用的划分方式，选取部分图像用于训练，其余用于测试。
[此处插入图：ISPRS Vaihingen数据集样例。格式同Potsdam。]
LoveDA数据集
LoveDA数据集是一个面向领域自适应研究的大规模遥感图像语义分割数据集，覆盖中国南京、常州和武汉三个城市的城市和乡村区域。该数据集包含5987幅1024×1024像素的高分辨率遥感图像，GSD为0.3米，每幅图像包含红色（R）、绿色（G）和蓝色（B）三个通道。数据集标注了7个语义类别：背景（Background）、建筑物（Building）、道路（Road）、水体（Water）、裸地（Barren）、森林（Forest）和农田（Agriculture）。
LoveDA数据集相比Potsdam和Vaihingen数据集具有两个显著特点。第一，场景多样性更强。LoveDA同时包含城市和乡村两种差异显著的场景类型：城市场景中建筑物密集、道路网络复杂、地物尺度较小但种类丰富；乡村场景中农田和森林面积广阔、地物边界模糊、类间特征差异较小（如农田和裸地在某些季节光谱特征高度相似）。第二，类别不平衡更严重。不同类别的像素比例差异悬殊，背景类可能占据大量像素，而裸地等少数类的像素比例极低，这对损失函数的设计和模型对少数类的学习能力提出了更高要求。
这些特点使LoveDA成为评估模型在复杂多变场景下综合分割能力的重要基准。在本文实验中，按照官方划分进行训练和测试。
[此处插入图：LoveDA数据集样例。展示城市和乡村各1-2组样例，包含RGB影像和对应标注图。]
WHU-OPT-SAR数据集
WHU-OPT-SAR数据集是一个专为光学与SAR图像融合语义分割任务设计的大规模多模态遥感数据集，由武汉大学构建并公开发布。该数据集覆盖中国湖北省约50000平方公里的区域（30°N-33°N，108°E-117°E），地形涵盖了山地、林地、丘陵、平原等多种复杂类型，植被包括针叶林、阔叶林、灌木和水生植被等多种种类，为多模态融合分割方法的评估提供了地理和生态条件丰富的实验基础。
数据集中的光学图像由高分一号（GF-1）卫星采集，包含红色（R）、绿色（G）、蓝色（B）和近红外（NIR）四个通道，原始地面分辨率为2米。SAR图像由高分三号（GF-3）卫星采集，为单通道极化图像，地面分辨率为5米。为实现两种模态图像之间的逐像素空间对应，光学图像通过双线性插值重采样至5米分辨率，并在WGS-84坐标系下与SAR图像进行精确的几何配准。数据集共包含100幅5556×3704像素的配准光学-SAR图像对，提供像素级语义标注，包含7个类别：背景（Background）、农田（Farmland）、城市（City）、村庄（Village）、水体（Water）、森林（Forest）和道路（Road）。
WHU-OPT-SAR数据集是目前公开可用的最大规模光学-SAR融合语义分割数据集。相比前三个单模态数据集，WHU-OPT-SAR的核心挑战在于：第一，光学图像和SAR图像在成像机理上的本质差异导致同一地物在两种模态中呈现截然不同的外观，模型需要学会跨越模态鸿沟提取互补信息；第二，SAR图像中固有的相干斑噪声可能在融合过程中干扰光学特征的判别性，要求融合策略具有一定的噪声鲁棒性；第三，数据集覆盖的地理范围广、地形和植被种类多样，对模型的泛化能力提出了较高要求。
在本文实验中，将原始图像裁剪为固定大小的图像块，按照训练集、验证集和测试集进行划分。
[此处插入图：WHU-OPT-SAR数据集样例。展示2-3组样例，每组包含光学图像（RGB或RGBNIR合成）、对应的SAR图像（灰度）和语义标注图。直观展示同一区域在光学和SAR两种模态下的显著差异。]
评价指标
为全面评估语义分割模型的性能，本文采用以下评价指标。各指标均基于混淆矩阵进行计算，混淆矩阵是评估分类和分割模型的基础工具。对于包含 $K$ 个类别的语义分割任务，混淆矩阵 $\mathbf{M} \in \mathbb{R}^{K \times K}$ 中的元素 $M_{ij}$ 表示真实类别为 $i$ 且被预测为类别 $j$ 的像素数量。对角线元素 $M_{ii}$ 代表正确分割的像素数量，非对角线元素 $M_{ij}$（$i \neq j$）反映了类别 $i$ 被误判为类别 $j$ 的像素数量。
总体精度（Overall Accuracy, OA）
总体精度衡量的是所有像素中被正确分类的比例，是最直观的整体性能指标：
$$OA = \frac{\sum_{i=0}^{K-1} M_{ii}}{\sum_{i=0}^{K-1}\sum_{j=0}^{K-1} M_{ij}}$$
其中，分子为混淆矩阵对角线元素之和（正确分类的总像素数），分母为所有元素之和（总像素数）。OA直观易懂，但在类别像素分布不均衡时可能具有误导性——例如，当背景类占据90%的像素时，即使模型将所有像素都预测为背景，OA也可达到90%，但实际分割效果极差。因此，OA通常需要与其他指标结合使用。
交并比与平均交并比（IoU / mIoU）
交并比（Intersection over Union, IoU）是语义分割中最核心的类别级评价指标。对于某一类别 $i$，IoU衡量的是模型预测为该类别的区域与真实属于该类别的区域之间的重叠程度：
$$IoU_i = \frac{M_{ii}}{\sum_{j=0}^{K-1} M_{ij} + \sum_{j=0}^{K-1} M_{ji} - M_{ii}}$$
分子 $M_{ii}$ 为预测正确的像素数，即预测区域与真实区域的交集。分母包含三项：$\sum_{j} M_{ij}$ 为真实属于类别 $i$ 的总像素数（真实区域面积），$\sum_{j} M_{ji}$ 为被预测为类别 $i$ 的总像素数（预测区域面积），减去 $M_{ii}$ 避免交集被重复计算，最终得到预测区域与真实区域的并集。IoU的取值范围为 $[0, 1]$，值越接近1表示预测区域与真实区域的重叠度越高，分割效果越好。
平均交并比（mIoU）是所有类别IoU的算术平均值：
$$mIoU = \frac{1}{K}\sum_{i=0}^{K-1} IoU_i$$
mIoU对每个类别赋予相同的权重，不受类别像素数量不平衡的影响。即使某一少数类仅占图像的极小比例，其IoU对mIoU的贡献权重与多数类完全相同。这一特性使mIoU成为语义分割任务中最被广泛认可和使用的综合评价指标。
F1分数与平均F1分数（F1 / mF1）
F1分数是精确率（Precision）和召回率（Recall）的调和平均值，从分类的角度综合评估模型的准确性和完整性。对于类别 $i$：
$$Precision_i = \frac{M_{ii}}{\sum_{j=0}^{K-1} M_{ji}}, \quad Recall_i = \frac{M_{ii}}{\sum_{j=0}^{K-1} M_{ij}}$$
精确率 $Precision_i$ 反映了模型预测为类别 $i$ 的像素中真正属于该类别的比例，即"预测的准不准"——高精确率意味着低误检率。召回率 $Recall_i$ 反映了真实属于类别 $i$ 的像素中被模型成功检出的比例，即"找得全不全"——高召回率意味着低漏检率。F1分数平衡了两者的影响：
$$F1_i = \frac{2 \times Precision_i \times Recall_i}{Precision_i + Recall_i}$$
使用调和平均而非算术平均的原因在于：调和平均对极端值更为敏感——当精确率或召回率中任一项极低时，F1分数都会显著降低，从而避免了一项指标极高而另一项极低时给出误导性的高评分。平均F1分数（mF1）为所有类别F1的算术平均值。
计算效率指标
除分割精度指标外，本文还采用以下指标全面评估模型的计算效率：
参数量（Parameters, Params）：模型中所有可学习参数的总数，以百万（M）为单位。参数量反映了模型的存储需求和过拟合风险——参数量越大，模型的表达能力通常越强，但对训练数据量的需求也越大，且存储和传输成本更高。
浮点运算量（Floating Point Operations, FLOPs）：模型进行一次前向推理所需的浮点乘加运算次数，以十亿次（G）为单位。FLOPs反映了模型的理论计算复杂度，是评估模型计算效率的重要指标。需要注意的是，FLOPs不完全等同于实际运行时间，后者还受到硬件架构、内存访问模式和算子并行度等因素的影响。
推理速度（Frames Per Second, FPS）：模型在特定硬件上每秒能够处理的图像数量，以帧/秒（fps）为单位。FPS是最直接反映模型实际部署效率的指标，综合了计算复杂度、内存访问效率和硬件利用率等因素。
本章小结
本章系统介绍了本文研究工作所涉及的相关理论与技术基础。首先，阐述了语义分割任务的基本概念，分析了编码器-解码器架构如何通过层次化特征提取、渐进式空间恢复和跳跃连接来协调语义理解与空间定位之间的矛盾，并介绍了交叉熵损失和Dice损失各自的优势与不足及联合使用的合理性。其次，详细介绍了状态空间模型的理论基础，从连续SSM的状态方程和观测方程出发，阐述了离散化过程中递推模式与卷积模式的双重计算视角，分析了从S4的固定参数设计到Mamba的输入依赖选择性机制的关键技术跨越，并介绍了SS2D如何通过四方向交叉扫描将一维SSM的全局建模能力扩展到二维图像处理中。然后，分别介绍了SE和ECA通道注意力、空间注意力以及自注意力和交叉注意力等本文涉及的关键注意力机制，分析了各自的设计动机、数学原理和适用场景。接着，从成像原理层面分析了光学遥感图像和SAR遥感图像各自的数据特性、优势与固有局限，并系统讨论了两种模态之间的互补性和融合挑战。最后，详细介绍了ISPRS Potsdam、ISPRS Vaihingen、LoveDA和WHU-OPT-SAR四个公开数据集的基本信息、数据特点和实验划分，以及mIoU、OA、mF1等分割精度指标和参数量、FLOPs、FPS等计算效率指标的数学定义与物理含义。本章的内容为后续第三章和第四章的方法设计与实验分析提供了完整的理论支撑和实验基础。
基于双路径解耦的遥感图像语义分割方法
遥感图像语义分割是遥感图像智能解译中的核心任务，其目标是将图像中的每个像素分配至对应的地物类别，从而实现对地表覆盖信息的精确提取与定量分析。这一技术在城市规划、环境监测、灾害评估等领域具有广泛的应用价值。高分辨率遥感图像具有地物类型丰富、目标尺度差异显著、背景环境复杂等特点，这对分割模型同时提出了全局上下文建模能力与局部细节捕获能力的双重要求。然而，现有基于状态空间模型的分割方法普遍采用单路径的序列化处理架构，将上述两种本质不同的子任务耦合于同一计算流程中，导致优化过程中出现目标冲突。此外，在多阶段特征处理中，通道-空间注意力的同质化重复施加也引入了不必要的计算冗余。
针对上述问题，本章提出了一种基于双路径解耦思想的遥感图像语义分割网络DP-UNet，核心设计遵循"解耦"与"精简"两大原则。首先，在解码器中设计了双路径解耦状态空间模块（Dual-path VSS, DVSS），将全局上下文建模与局部细节增强分离为两条独立的专用路径，使各子任务获得充分的优化空间；其次，在多尺度特征融合阶段，提出了轻量级的多尺度空间核模块（Multi-Scale Kernel, MSK），以纯空间注意力取代冗余的通道-空间双重注意力，在保持融合效果的同时显著降低了计算开销。
现有方法的问题分析
在深入阐述本章所提方法之前，有必要对遥感图像分割中现有方法所面临的共性问题进行系统分析。本节从全局-局部建模的耦合瓶颈和注意力机制的冗余施加两个角度展开讨论，揭示制约当前方法性能进一步提升的关键因素。
全局-局部建模的耦合瓶颈
遥感图像语义分割本质上需要模型同时完成两个层面的推理任务：一是全局上下文建模，即在较大空间范围内整合语义信息，识别地物类别的宏观分布模式，确保同类地物在不同空间位置的分类一致性；二是局部细节增强，即在精细空间尺度上保留并强化边缘轮廓、纹理特征等高频信息，确保地物边界的准确描绘和小目标的有效识别。这两个任务虽然作用于同一输入特征，但其优化目标存在内在差异——前者追求特征的平滑性和语义一致性，后者追求特征的锐利性和空间精确性。
以基于状态空间模型的分割网络为例，当前主流方法普遍采用单路径的序列化设计。以CM-UNet中的CSMamba模块为代表，其核心SS2D组件通过四方向二维状态空间扫描实现全局上下文建模，同时在扫描路径中嵌入通道注意力与空间注意力门控以增强特征选择能力。然而，这种设计将全局语义聚合与局部细节增强共享同一组网络参数和计算路径。在训练过程中，梯度信号同时承载着两种子任务的优化需求，使得网络参数被迫在两种目标之间进行折衷，难以在任一方面达到最优。
为了更直观地理解这一耦合效应的影响，图3-1展示了在遥感分割场景中典型的失败案例。在包含多尺度地物目标的图像区域中，单路径架构的模型往往表现出两类典型的误分割模式：一类是在大尺度地物内部出现不一致的分类结果，表明全局上下文建模不充分；另一类是在地物边界处出现模糊或锯齿状的分割结果，表明局部细节信息在处理过程中受到了全局平滑化操作的损害。
**图3-1 单路径架构在遥感分割中的典型失败案例（a）原始遥感图像；（b）真实标签；（c）单路径方法的分割结果；（d）本章方法的分割结果**
上述分析表明，全局上下文建模与局部细节增强之间的耦合是限制当前方法性能提升的一个关键瓶颈。将这两个具有不同优化目标的子任务进行显式解耦，使之在独立的路径中接受专门的优化，是突破这一性能瓶颈的可行策略。
注意力机制的冗余施加
注意力机制已成为现代深度网络中增强特征表达能力的标准手段。在遥感分割任务中，通道注意力和空间注意力分别从特征通道间依赖建模和空间位置间关联建模两个维度提升特征的判别力，被广泛应用于网络的各个阶段。然而，当注意力模块在网络中被不加区分地重复施加时，其边际收益将迅速递减，而带来的参数量和计算量增长却是线性累积的。
具体而言，现有方法中注意力冗余主要体现在两个层面。第一个层面是模块内部的注意力冗余。以CM-UNet的CSMamba模块为例，其内部SS2D组件已经嵌入了通道注意力门控和空间注意力门控，用于在状态空间扫描过程中对特征进行自适应选择。这些注意力门控与SS2D的扫描操作紧密耦合，形成了一套完整的特征选择机制。第二个层面是跨模块的注意力冗余。在特征融合阶段，MSAA模块再次引入了通道注意力和空间注意力的完整组合，对来自编码器的跳跃连接特征进行重新加权。由于SS2D内部的注意力门控已经完成了一次通道-空间维度的特征筛选，MSAA中相似的注意力操作在很大程度上重复了SS2D的工作，造成了同质化的注意力堆叠。
这种双重冗余的直接后果是计算效率的不必要降低。更深层的问题在于，多级同质注意力的叠加可能导致注意力权重的过度聚焦现象：每一级注意力都倾向于增强已经被前一级强化的特征响应，抑制已经被弱化的特征信号，使得最终的特征表示过度集中于少数显著区域，反而降低了网络对弱响应但语义重要的特征的捕获能力。
图3-2直观展示了注意力冗余对网络特征表示的影响。通过可视化不同阶段的注意力权重分布，可以观察到冗余注意力施加后特征响应的过度集中现象，以及去除冗余后特征分布更加均衡合理的效果。
**图3-2 注意力冗余现象的可视化分析（a）SS2D内部注意力权重分布；（b）MSAA注意力权重分布；（c）冗余注意力叠加后的特征响应；（d）精简注意力后的特征响应**
基于上述分析，本章主张在网络设计中应遵循"专注分工、各司其职"的注意力部署策略：在解码器的核心处理模块中，将注意力集中用于通道维度的特征重标定，辅助局部细节增强路径的精准聚焦；在多尺度特征融合阶段，则专注于空间维度的注意力建模，以弥合不同尺度特征图之间的空间对齐差异。通过这种有针对性的注意力配置，避免同质注意力的重复施加，在保持甚至提升模型性能的同时，显著改善计算效率。
	DP-UNet整体架构
基于上一节的分析，本节提出了一种面向遥感图像语义分割的编码器-解码器网络DP-UNet，其整体架构如图3-3所示。该网络由三个核心组件构成：基于ResNet-18的多层级特征提取编码器、基于MSK模块的多尺度特征融合机制，以及基于DVSS模块的渐进式上采样解码器。
**图3-3 DP-UNet整体网络架构（a）网络全局结构，红色箭头表示上采样，蓝色箭头表示下采样；（b）DVSS模块的内部结构**
编码器
编码器采用在ImageNet上预训练的ResNet-18作为骨干网络，负责从输入图像中提取多层级的视觉特征。给定输入图像$I \in \mathbb{R}^{H \times W \times 3}$，编码器输出四个不同分辨率的特征图$\{F_1, F_2, F_3, F_4\}$，其空间分辨率分别为原始图像的$1/4$、$1/8$、$1/16$和$1/32$，对应的通道数分别为64、128、256和512。
选择ResNet-18而非更深层的网络作为编码器骨干，基于以下考量：首先，ResNet-18的残差连接结构能够有效缓解梯度消失问题，保证多层级特征的有效提取；其次，其较为紧凑的参数规模与本章追求的高效计算目标一致；最后，浅层网络提取的特征保留了更丰富的空间细节信息，有利于后续解码器中局部细节增强路径的有效工作。编码过程可形式化表示为：
$$\{F_1, F_2, F_3, F_4\} = \text{Enc}(I) \tag{3-1}$$
其中$\text{Enc}(\cdot)$表示编码器的特征提取操作。
多尺度特征融合
编码器输出的前三层特征$\{F_1, F_2, F_3\}$通过三个MSK模块进行跨尺度融合，生成增强后的跳跃连接特征$\{F_1', F_2', F_3'\}$；最深层特征$F_4$则直接传入解码器。每个MSK模块以三层编码器特征作为输入，将其中两层的特征图通过双线性插值调整至目标分辨率后，与目标层特征进行拼接融合。该融合过程可表示为：
$$F_i' = \text{MSK}_i(F_1, F_2, F_3), \quad i = 1, 2, 3 \tag{3-2}$$
其中$\text{MSK}_i$表示第$i$个MSK模块，其将来自其他两个尺度的特征调整至目标尺度后进行融合处理。MSK模块的详细设计将在3.4节中进行阐述。
解码器
解码器采用渐进式上采样策略，由四个DVSS模块组成。解码阶段0直接处理最深层特征$F_4$，随后的解码阶段1至3依次将上采样后的深层特征与对应的增强跳跃连接特征进行融合，并通过DVSS模块进行进一步处理。具体而言，每个解码阶段首先对前一阶段的输出进行双线性上采样，与对应层级的增强跳跃特征进行通道拼接，再通过1×1逐点卷积进行通道降维，最后输入DVSS模块进行全局-局部双路径处理。该过程可表示为：
$$D_4 = \text{DVSS}_4(F_4) \tag{3-3}$$
$$D_i = \text{DVSS}_i(\text{Proj}(\text{Up}(D_{i+1})), F_i'), \quad i = 3, 2, 1 \tag{3-4}$$
其中$\text{Proj}(\cdot)$表示逐点卷积投影操作，用于通道数的调整；$\text{Up}(\cdot)$表示双线性上采样操作。DVSS模块的详细设计将在3.3节中进行阐述。
预测头与损失函数
解码器最终阶段的输出$D_1$通过一个$1 \times 1$卷积头生成预测结果，再经双线性上采样恢复至原始分辨率，获得最终的分割预测图$P$。此外，为了强化深层解码器的学习信号，在解码阶段2至4设置辅助预测头，提供深度监督。最终的训练损失函数定义为：

$$P = \text{Head}(D_1) \tag{3-5}$$
$$\mathcal{L} = \mathcal{J}(P, Y) + \lambda \sum_{i=2}^{4} \mathcal{J}(P_i, Y) \tag{3-6}$$
其中$Y$为真实标签，$P_i$为第$i$个解码阶段的辅助预测，$\mathcal{J}(\cdot)$为交叉熵损失与Dice损失的联合损失函数，$\lambda = 0.4$为辅助损失的权重系数。
联合损失函数的设计动机在于：交叉熵损失擅长于像素级分类的全局优化，为模型提供稳定且均衡的梯度信号；Dice损失则直接优化分割区域的重叠度，对类别不平衡具有天然的鲁棒性，尤其有利于遥感图像中面积较小的地物类别（如车辆）的学习。两者的结合使模型在全局分类精度和区域分割质量之间取得平衡。深度监督策略通过在中间解码层施加额外的监督信号，促使网络在不同抽象层级上都学习到有意义的语义表示，有效缓解了深层网络中梯度衰减的问题，加速了模型收敛。
双路径解耦状态空间模块（DVSS）
本节详细介绍作为DP-UNet解码器核心的双路径解耦状态空间模块（DVSS）。该模块的设计理念是将全局上下文建模与局部细节增强从共享参数的单一路径中解放出来，为每个子任务建立独立的专用处理路径，并通过自适应门控机制实现两条路径输出的有效融合。
共享基底与路径分离
DVSS模块的整体工作流程如图3-4所示。给定输入特征$X \in \mathbb{R}^{B \times H \times W \times C}$，首先经过层归一化处理，然后输入SS2D模块。SS2D模块采用四方向二维状态空间扫描机制，将二维特征图沿四个方向（从左到右、从右到左、从上到下、从下到上）展开为一维序列，分别进行状态空间建模后合并，从而在保持线性计算复杂度的同时捕获全局范围的特征依赖关系。
与现有方法不同的是，本章对SS2D模块进行了精简化处理，移除了其内部原有的通道注意力门控和空间注意力门控。这一设计决策基于3.1.2节的分析：SS2D的状态空间扫描本身已经具备全局上下文建模能力，内部注意力门控的移除不仅消除了与后续模块之间的注意力冗余，还为DVSS中专门设计的局部路径注意力机制腾出了"注意力预算"。精简后的SS2D输出作为共享基底特征$F_s$，同时送入全局路径和局部路径：

$$F_s = \text{SS2D}(\text{LN}(X)) \tag{3-7}$$
其中$\text{LN}(\cdot)$表示层归一化操作。这种"共享基底、分支增强"的设计确保了两条路径基于相同的特征基础进行各自的专门化处理，既避免了独立编码带来的信息冗余，又保证了路径间的特征一致性。
**图3-4 DVSS模块的内部结构示意图。输入特征经SS2D处理后形成共享基底特征，随后分别进入全局路径和局部路径进行专门化处理，最终通过APFG模块进行自适应融合**
全局上下文路径
全局路径的设计遵循简洁有效的原则，直接保留共享基底特征$F_s$作为全局路径的输出$F_g$：
$$F_g = F_s \tag{3-8}$$
这一设计看似简单，实则蕴含着重要的方法论考量。SS2D模块通过四方向状态空间扫描已经完成了全局范围的上下文建模，其输出特征$F_s$天然地编码了全局语义信息。对这些特征施加额外的变换操作不仅增加计算负担，还可能引入不必要的信息损耗。因此，全局路径的核心思想是信任并保护SS2D所建立的全局上下文表示，将计算资源集中分配给更需要专门化处理的局部细节增强任务。这种非对称的计算资源分配策略体现了本章"精简"设计原则的核心思想。
局部细节增强路径
局部路径是DVSS模块的核心创新所在，由高效通道注意力（ECA）模块和参数域调制卷积（PMC）模块两个关键组件串联构成，旨在从共享基底特征中挖掘并强化局部空间细节信息。
	通道重标定：高效通道注意力模块
局部路径的第一步是对共享基底特征$F_s$进行通道维度的自适应重标定。这一步骤的动机在于：$F_s$中的不同通道编码了不同类型和不同层级的视觉信息，对于局部细节提取而言，并非所有通道同等重要。通过通道注意力机制，可以自适应地增强与边缘、纹理等高频信息相关的通道响应，抑制与全局语义信息相关但对局部细节增强贡献较小的通道响应，从而为后续的PMC模块提供经过信息筛选的输入。
本章采用高效通道注意力（Efficient Channel Attention, ECA）模块实现通道重标定。ECA模块如图3-5所示，其通过全局平均池化获取各通道的全局统计信息，然后利用一维卷积在相邻通道间建模局部跨通道依赖关系，避免了传统SE模块中全连接层带来的通道信息压缩损失和较大的参数开销。ECA模块的计算过程可以表示为：
$$y_c = \text{GAP}(F_s^c) = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} F_s^c(i, j) \tag{3-9}$$
$$\omega = \sigma(\text{Conv1D}_k(\mathbf{y})) \tag{3-10}$$
$$F_{eca} = \omega \odot F_s \tag{3-11}$$
其中$\text{GAP}(\cdot)$表示全局平均池化操作，$\mathbf{y} \in \mathbb{R}^C$为各通道的全局描述向量，$\text{Conv1D}_k(\cdot)$为核大小为$k$的一维卷积操作，$\sigma(\cdot)$为Sigmoid激活函数，$\odot$表示逐元素乘法（即通道级别的特征缩放）。一维卷积核大小$k$通过通道数$C$自适应确定：
$$k = \left| \frac{\log_2 C}{\gamma} + \frac{b}{\gamma} \right|_{\text{odd}} \tag{3-12}$$
其中$\gamma = 2$，$b = 1$，$|\cdot|_{\text{odd}}$表示取最近的奇数。
经通道重标定后的特征$F_{eca}$通过与原始基底特征$F_s$的残差连接进行增强，确保原始信息的完整传递：
$$F_{eca}' = F_{eca} + F_s \tag{3-13}$$
**图3-5 ECA模块的结构示意图**
参数域调制卷积（PMC）
经过通道重标定后的特征$F_{eca}'$被送入参数域调制卷积（Parameterized Modulation Convolution, PMC）模块进行局部空间细节的精细化增强。PMC模块的提出源于对标准卷积行为的深入分析。
在标准卷积运算中，卷积核的中心位置始终与输入特征图的目标采样位置精确对齐，无论输入特征的空间内容如何变化。这种固定的空间对应关系使得中心权重在训练过程中始终处于最稳定的梯度更新条件下：中心位置的采样在所有空间位置上具有最高的对齐一致性，且在滑动窗口的重叠区域中被最频繁地激活，因此累积接收到的梯度信号最为稳定且幅度最大。经过长期训练，这一过程导致了显著的中心权重主导现象——卷积核中心位置的权重幅值远大于周围位置。
中心主导效应意味着标准卷积的输出在很大程度上由中心位置的输入值决定，而来自周围邻域的信息贡献相对较弱。对于全局语义理解任务而言，这种中心聚焦的行为或许是可接受的；但对于局部细节增强任务，特别是边缘检测、纹理提取等需要充分利用邻域差异信息的场景，中心主导效应恰恰削弱了卷积核对边缘方向性和纹理周期性等局部空间模式的敏感度。

PMC模块的核心思想是在参数域（而非空间域）引入一种可学习的抑制性先验，自适应地调制卷积核权重，抑制中心权重的绝对主导地位，促使网络更均衡地利用邻域信息。与传统的可变形卷积在空间域通过学习采样偏移量来改变感受野形状不同，PMC将调制操作转移至参数域，避免了坐标偏移计算和双线性插值等复杂操作，在保持轻量级计算开销的同时实现了对卷积行为的有效调制。
PMC模块的具体计算过程如图3-6所示。设标准卷积核的权重张量为$W \in \mathbb{R}^{C_{out} \times C_{in} \times k_h \times k_w}$，PMC通过以下步骤生成调制后的有效卷积权重$W_{eff}$：
步骤一：通道强度矩阵的计算。将权重张量$W$沿空间维度求和，生成通道强度矩阵$S \in \mathbb{R}^{C_{out} \times C_{in}}$，表征每对输入-输出通道之间的连接强度：
$$S = \sum_{i=1}^{k_h} \sum_{j=1}^{k_w} W(:, :, i, j) \tag{3-14}$$
步骤二：动态抑制掩码的生成。动态掩码$M_d$由三个分量的逐元素乘积构成：可学习掩码矩阵$W_m \in \mathbb{R}^{C_{out} \times C_{in}}$、固定中心掩码$M_c$（仅卷积核中心位置为1，其余位置为0）和通道强度矩阵$S$：
$$M_d = W_m \odot M_c \odot S \tag{3-15}$$
其中$M_c$的引入确保了抑制操作仅作用于卷积核的中心位置，而$W_m$和$S$的参与则使得抑制强度能够根据每对通道的实际连接强度和任务需求进行自适应调整。
步骤三：权重调制。引入可学习缩放因子$\theta$，对原始卷积权重进行调制：
$$W_{eff} = W \odot (\mathbf{1} - \theta \times M_d) \tag{3-16}$$
其中$\mathbf{1}$为与$W$同形状的全1张量。当$\theta \times M_d$在中心位置取正值时，该位置的权重将被适度抑制；$\theta$的可学习性使得网络能够自主控制抑制的全局强度，在中心抑制带来的邻域信息增益与中心信息保留之间寻找最优平衡。
最终，使用调制后的有效权重$W_{eff}$对输入特征$F_{eca}'$进行卷积运算，得到局部路径的输出$F_l$：
$$F_l = \text{Conv}_{W_{eff}}(F_{eca}') \tag{3-17}$$
**图3-6 PMC模块的工作流程示意图。标准卷积权重通过通道强度矩阵和动态掩码在参数域进行自适应调制，抑制中心主导效应**
与传统可变形卷积（如LDC）的对比分析进一步揭示了PMC的设计优势。传统可变形卷积通过预测每个采样位置的空间偏移量$\Delta p$来改变感受野形状，其计算过程涉及偏移量预测网络、双线性插值采样等操作，计算开销较大且难以并行优化。PMC将"变形"的概念从空间域转移至参数域：不改变采样位置，而是调制采样位置对应的权重值。这种转换在保留了调制卷积行为的灵活性的同时，避免了坐标偏移和插值计算，使得PMC可以无缝嵌入标准卷积流程中，无需修改底层的卷积运算实现，具有更好的计算效率和工程实用性。
自适应路径融合门控（APFG）
全局路径输出$F_g$和局部路径输出$F_l$编码了互补的特征信息，需要通过有效的融合策略进行整合。简单的逐元素加法虽然计算高效，但忽略了两条路径在不同空间位置和不同通道维度上的相对重要性差异。例如，在地物内部区域，全局路径提供的语义一致性信息可能更为重要；而在地物边界附近，局部路径提供的细节信息则应获得更高的融合权重。
为此，本章设计了自适应路径融合门控（Adaptive Path Fusion Gate, APFG）模块，通过独立的通道级注意力分别评估两条路径特征的重要性，实现自适应的加权融合。APFG模块的结构如图3-7所示，其具体计算过程如下：
对于全局路径特征$F_g$和局部路径特征$F_l$，分别执行独立的通道注意力操作：
$$\alpha_g = \sigma(W_2^g \cdot \delta(W_1^g \cdot \text{GAP}(F_g))) \tag{3-18}$$
$$\alpha_l = \sigma(W_2^l \cdot \delta(W_1^l \cdot \text{GAP}(F_l))) \tag{3-19}$$
其中$W_1^{(\cdot)} \in \mathbb{R}^{C/r \times C}$和$W_2^{(\cdot)} \in \mathbb{R}^{C \times C/r}$分别为瓶颈结构中的降维和升维线性变换矩阵，$r$为降维比率，$\delta(\cdot)$为GeLU激活函数，$\sigma(\cdot)$为Sigmoid激活函数。两条路径使用独立的参数（上标$g$和$l$），使得各自的注意力权重能够学习到不同的通道重要性模式。
最终的融合输出通过残差连接与DropPath正则化生成：
$$Y = X + \text{DropPath}(\alpha_g \odot F_g + \alpha_l \odot F_l) \tag{3-20}$$
其中$X$为DVSS模块的原始输入特征，残差连接确保了梯度的有效回传和训练的稳定性；DropPath在训练过程中以一定概率随机跳过融合路径，起到正则化作用，增强模型的泛化能力。
值得说明的是，APFG模块中采用GeLU激活函数而非SS2D中使用的SiLU激活函数。这一选择基于对不同模块功能特性的考量：SS2D作为状态空间扫描模块，遵循原始VMamba的设计保持SiLU激活以确保架构一致性；而APFG作为融合来自异质路径（全局与局部）特征的门控模块，GeLU更平滑的梯度曲线有助于在融合过程中实现更精细的权重调节，减少异质特征混合时可能产生的梯度突变。
**图3-7 APFG模块的结构示意图。两条路径的特征分别通过独立的通道注意力进行重标定后相加融合**
多尺度空间核融合模块（MSK）
在编码器-解码器架构中，多尺度特征融合是弥合不同层级特征之间语义鸿沟的关键环节。浅层特征空间分辨率高、包含丰富的空间细节信息但语义抽象程度低，深层特征空间分辨率低但包含高级语义信息。有效的多尺度融合能够使各层级特征相互补充，为解码器提供兼具空间精度和语义深度的输入。
融合策略的设计考量
本章多尺度融合模块MSK的设计基于两个核心考量。第一，注意力功能的分工协作。如3.1.2节所述，DVSS模块中局部路径的ECA已经承担了通道维度的特征重标定任务。在特征融合阶段再次引入通道注意力，不仅造成计算冗余，还可能因注意力的过度施加而损害特征的多样性。因此，MSK模块专注于空间维度的注意力建模，与DVSS中的通道注意力形成互补配置。第二，多尺度融合的核心挑战。不同层级特征图之间存在的主要差异是空间分辨率和语义层级的不匹配，即空间维度的对齐与整合是融合阶段最需要关注的问题。空间注意力能够自适应地识别不同空间位置的融合重要性，在语义丰富区域和语义边界区域分配不同的关注度，有效弥合跨尺度特征之间的空间语义差距。
MSK模块结构
MSK模块的结构如图3-8所示。三个MSK模块分别在编码器的三个尺度上运作，生成增强后的跳跃连接特征$\{F_1', F_2', F_3'\}$。对于每个MSK模块，来自其他两个尺度的特征图首先通过双线性插值调整至目标分辨率，然后与目标尺度的特征沿通道维度拼接，形成包含跨尺度信息的联合特征表示。
**图3-8 MSK模块的结构示意图。三个尺度的特征经拼接和通道压缩后，通过多尺度并行卷积提取空间模式，再经空间注意力调制后恢复至目标通道数**
MSK模块的具体处理流程包含以下步骤：
步骤一：通道压缩。拼接后的特征通过$1 \times 1$卷积压缩至$C/4$通道。这一压缩操作具有双重目的：降低后续多尺度卷积的计算开销，以及通过跨通道的信息混合促进不同尺度来源特征之间的初步整合：

$$F_{comp} = \text{Conv}_{1 \times 1}(\text{Cat}(F_{s_1}, F_{s_2}, F_{s_3})) \tag{3-21}$$
其中$F_{s_1}, F_{s_2}, F_{s_3}$为经尺度调整后的三层特征，$\text{Cat}(\cdot)$表示通道维度拼接。
步骤二：多尺度空间模式提取。在压缩后的特征空间中，三个标准卷积以$3 \times 3$、$5 \times 5$和$7 \times 7$的核大小并行作用于$F_{comp}$，分别提取不同感受野范围内的空间模式信息，其输出进行逐元素求和：
$$F_{ms} = \text{Conv}_{3 \times 3}(F_{comp}) + \text{Conv}_{5 \times 5}(F_{comp}) + \text{Conv}_{7 \times 7}(F_{comp}) \tag{3-22}$$
采用三种不同大小的卷积核使模块能够同时感知精细的局部纹理（$3 \times 3$）、中等范围的结构模式（$5 \times 5$）和较大范围的上下文关系（$7 \times 7$）。由于所有卷积操作都在压缩后的$C/4$通道空间中进行，即便使用较大的卷积核，总体计算成本仍保持在可控范围内。
步骤三：空间注意力调制。对多尺度融合特征$F_{ms}$施加空间注意力机制，自适应地突出空间维度上的关键区域。具体而言，沿通道轴分别计算平均池化和最大池化结果，将两者拼接后通过$7 \times 7$卷积和Sigmoid激活生成空间注意力图：

$$M_{spatial} = \sigma(\text{Conv}_{7 \times 7}(\text{Cat}(\text{AvgPool}(F_{ms}), \text{MaxPool}(F_{ms})))) \tag{3-23}$$
$$F_{att} = F_{ms} \odot M_{spatial} \tag{3-24}$$
平均池化捕获全局统计信息，反映各空间位置的平均响应水平；最大池化则捕获显著性信息，标识各空间位置的最强响应。两者的结合使空间注意力图兼顾了全局分布和局部显著性，能够更精确地识别需要重点关注的空间区域。
步骤四：通道恢复。经空间注意力调制后的特征通过$1 \times 1$卷积恢复至目标通道数：
$$F_{out} = \text{Conv}_{1 \times 1}(F_{att}) \tag{3-25}$$
效率分析
MSK模块相较于基线方法中的MSAA模块在计算效率上实现了显著提升。以融合目标通道数$C' = 32$为例，MSAA模块同时包含通道注意力和空间注意力分支，涉及全连接层、多路径注意力计算等操作，总注意力参数量约为512。而MSK模块通过"先压缩、再处理、后恢复"的策略，将所有核心操作限定在$C/4$通道的压缩空间中进行，且仅保留空间注意力分支，注意力相关参数量降至约98，参数减少超过80%。这一效率优势在不损失分割性能的前提下，为模型在资源受限的部署场景中的实际应用提供了更大的灵活性。
实验结果与分析
数据集介绍
为全面评估本章所提DP-UNet方法的有效性和泛化能力，选取了三个广泛使用的公开遥感图像语义分割数据集进行实验验证。
	ISPRS Potsdam数据集
ISPRS Potsdam数据集由国际摄影测量与遥感学会发布，包含波茨坦市的高分辨率航空正射影像，地面采样距离为5厘米。数据集提供近红外、红色和绿色（NIRRG）三通道影像及对应的数字表面模型。地物标注涵盖六个类别：不透水面（Impervious surfaces）、建筑物（Building）、低矮植被（Low vegetation）、树木（Tree）、车辆（Car）和杂项（Clutter）。本章实验中，选取23幅影像用于训练（排除存在标注错误的7_10号影像），14幅影像用于测试。
ISPRS Vaihingen数据集
ISPRS Vaihingen数据集同样由国际摄影测量与遥感学会发布，包含法伊欣根市的航空正射影像，地面采样距离为9厘米。影像为NIRRG三通道格式，地物类别标注与Potsdam数据集一致，包含六个类别。本章实验中选取12个图像块用于训练，4个图像块用于测试。Vaihingen数据集相较于Potsdam数据集，影像面积较小但地物分布更为复杂，包含更多的阴影区域和不规则建筑形态，对模型的细节捕获能力提出了更高的要求。
LoveDA数据集
LoveDA数据集包含来自中国南京、常州和武汉三个城市的高分辨率遥感影像，地面采样距离为0.3米。该数据集涵盖城市和农村两种典型场景，标注类别为七类：背景（Background）、建筑物（Building）、道路（Road）、水体（Water）、裸地（Barren）、森林（Forest）和农田（Agriculture）。训练集包含1156幅影像，验证集包含677幅影像。LoveDA数据集的特点在于城乡场景差异显著、类别分布严重不均衡，且包含大面积的混合过渡区域，是评估模型在复杂真实场景下泛化能力的有力基准。
实验设置
本章所有实验均基于PyTorch深度学习框架实现，在单块NVIDIA RTX 3090 GPU（24GB显存）上进行训练和测试。训练阶段采用AdamW优化器，初始学习率设置为$6 \times 10^{-4}$，并采用余弦退火学习率调度策略。训练数据经过随机裁剪为$512 \times 512$像素的图像块，并施加随机缩放、随机水平翻转、随机垂直翻转、随机旋转和颜色抖动等数据增强策略以提升模型的泛化能力。测试阶段使用$1024 \times 1024$分辨率的输入，并采用测试时翻转增强以提升预测的稳定性。实验环境的详细配置信息如表3-1所示。
**表3-1 实验环境与参数配置**

| 配置项 | 详细信息 |
|---|---|
| CPU | Intel Core系列处理器 |
| GPU | NVIDIA GeForce RTX 3090（24GB） |
| 操作系统 | Ubuntu 20.04 |
| 深度学习框架 | PyTorch |
| 优化器 | AdamW |
| 初始学习率 | 6×10⁻⁴ |
| 学习率调度 | 余弦退火 |
| 训练裁剪尺寸 | 512×512 |
| 测试输入尺寸 | 1024×1024 |
| 辅助损失权重λ | 0.4 |
评价指标方面，本章采用语义分割领域广泛使用的三个标准指标：各类别F1分数的均值（mF1）、各类别交并比的均值（mIoU）和总体精度（OA）。其中，mF1综合考量了各类别的精确率和召回率，反映模型在各类别上的均衡表现；mIoU直接度量预测区域与真实区域的重叠程度，是语义分割中最核心的评价指标；OA衡量所有像素中被正确分类的比例，反映模型的整体分类准确性。
ISPRS Potsdam数据集实验结果
表3-2展示了DP-UNet与多种代表性方法在ISPRS Potsdam数据集上的定量比较结果。对比方法涵盖了基于CNN的ABCNet、基于Transformer的Segmenter和BANet、CNN-Transformer融合的UNetFormer，以及基于Mamba的CM-UNet，全面覆盖了当前遥感分割领域的主流技术路线。

**表3-2 ISPRS Potsdam数据集上的实验结果**

| 方法 | 骨干网络 | Imp.surf. | Building | Lowveg. | Tree | Car | mF1 | OA | mIoU |
|---|---|---|---|---|---|---|---|---|---|
| ABCNet | R18 | 93.28 | 95.91 | 86.78 | 87.81 | 95.49 | 91.85 | 90.51 | 85.17 |
| Segmenter | ViT-T | 91.56 | 95.38 | 85.43 | 85.01 | 88.51 | 89.27 | 88.78 | 80.71 |
| BANet | ResT-L | 93.36 | 96.70 | 87.48 | 89.11 | 96.05 | 92.55 | 91.01 | 86.36 |
| UNetFormer | R18 | 93.69 | 96.10 | 87.27 | 88.72 | 96.58 | 92.47 | 91.07 | 86.23 |
| CM-UNet | R18 | 93.62 | 96.49 | 87.42 | 88.51 | 96.09 | 92.43 | 91.04 | 86.15 |
| **DP-UNet** | **R18** | **94.10** | **96.70** | **87.78** | **89.13** | 96.09 | **92.76** | **91.57** | **86.71** |
从表3-2可以看出，DP-UNet在mF1、OA和mIoU三个综合指标上均达到了最优水平，分别为92.76%、91.57%和86.71%。在各类别的F1分数上，DP-UNet在不透水面（94.10%）和低矮植被（87.78%）类别上取得了显著的领先优势。不透水面和低矮植被是遥感图像中典型的大面积连续地物，但其内部纹理复杂且与相邻类别的边界过渡区域较为模糊，DP-UNet在这些类别上的优异表现验证了双路径解耦设计在同时保持全局一致性和边界精度方面的有效性。在建筑物类别上，DP-UNet达到96.70%，与BANet并列最优；在树木类别上达到89.13%，同样名列前茅。
值得关注的是，与采用相同ResNet-18骨干网络的对比方法（ABCNet、UNetFormer、CM-UNet）相比，DP-UNet的性能优势更为显著。在相同的特征提取条件下，DP-UNet的mIoU分别超出上述三种方法1.54%、0.48%和0.56%，这一差距直接反映了本章所提解码器设计和融合策略的有效性。
图3-9展示了DP-UNet与UNetFormer、CM-UNet在Potsdam数据集上的定性分割结果对比。从可视化结果可以观察到，在包含建筑物密集区域和不透水面-低矮植被交界区域的场景中，DP-UNet能够生成更加清晰连贯的地物边界，有效减少了边界处的错分割和漏分割现象。特别是在图3-9第一行所示的复杂城区场景中，建筑物屋顶的轮廓更加完整，低矮植被区域的分割结果更加均匀一致，体现了全局路径保障语义一致性与局部路径增强边界清晰度的协同效果。
**图3-9 ISPRS Potsdam数据集上的定性分割结果对比。（a）NIRRG原始影像；（b）UNetFormer分割结果；（c）CM-UNet分割结果；（d）DP-UNet分割结果**

ISPRS Vaihingen数据集实验结果
表3-3展示了各方法在ISPRS Vaihingen数据集上的定量比较结果。在该数据集上，DP-UNet同样取得了最优的综合性能，mF1为91.10%，OA为91.43%，mIoU为83.84%。
**表3-3 ISPRS Vaihingen数据集上的实验结果**
| 方法 | 骨干网络 | Imp.surf. | Building | Lowveg. | Tree | Car | mF1 | OA | mIoU |
|---|---|---|---|---|---|---|---|---|---|
| ABCNet | R18 | 92.72 | 95.20 | 84.55 | 89.71 | 85.35 | 89.52 | 90.78 | 81.31 |
| RS3Mamba | R18-VM-T | 93.07 | 95.70 | 85.06 | 90.94 | 88.76 | 90.67 | 91.25 | 83.13 |
| Segmenter | ViT-T | 89.84 | 93.05 | 81.23 | 88.90 | 67.66 | 84.10 | 88.17 | 73.60 |
| BANet | ResT-L | 92.22 | 95.23 | 83.81 | 89.92 | 86.82 | 89.63 | 90.52 | 81.45 |
| UNetFormer | R18 | 93.07 | 95.42 | 85.03 | 90.74 | 88.99 | 90.65 | 91.21 | 83.09 |
| CM-UNet | R18 | 93.09 | 95.75 | 85.02 | 90.70 | 89.86 | 90.89 | 91.26 | 83.49 |
| **DP-UNet** | **R18** | **93.18** | **95.88** | **85.37** | **90.84** | **90.22** | **91.10** | **91.43** | **83.84** |
在Vaihingen数据集上尤为值得关注的是车辆（Car）类别的分割性能。车辆是遥感图像中典型的小目标，其在整幅图像中仅占极小的面积比例，且形态多样、易被树冠和建筑阴影遮挡，对模型的细节捕获能力构成了严峻挑战。DP-UNet在车辆类别上达到90.22%的F1分数，超越所有对比方法。这一结果充分验证了DVSS模块中局部细节增强路径的有效性：PMC通过抑制卷积核中心主导效应，迫使网络更充分地利用邻域信息，增强了对小目标边缘和轮廓的敏感度；ECA通道注意力则帮助网络自适应地聚焦于与小目标响应相关的特征通道，减少背景干扰。两者的协同作用使得DP-UNet在小目标分割上展现出显著的优势。
图3-10展示了Vaihingen数据集上的定性分割结果对比。在包含小尺寸车辆和复杂建筑结构的区域中，DP-UNet的分割结果更加准确，车辆目标的形态更为完整，建筑物边界的分割更加规整，进一步印证了定量分析的结论。
**图3-10 ISPRS Vaihingen数据集上的定性分割结果对比。（a）NIRRG原始影像；（b）UNetFormer分割结果；（c）CM-UNet分割结果；（d）DP-UNet分割结果**
LoveDA数据集实验结果
表3-4展示了各方法在LoveDA数据集上的定量比较结果。LoveDA数据集覆盖城市与农村两种场景，类别数量多、分布严重不均衡、场景差异大，是评估模型综合能力和泛化性能的高难度基准。
**表3-4 LoveDA数据集上的实验结果**
| 方法 | 骨干网络 | Background | Building | Road | Water | Barren | Forest | Agriculture | mIoU |
|---|---|---|---|---|---|---|---|---|---|
| Segmenter | ViT-T | 38.03 | 50.74 | 48.76 | 77.40 | 13.36 | 43.48 | 58.20 | 47.18 |
| ABCNet | ViT-R50 | 53.00 | 62.18 | 52.42 | 62.02 | 29.80 | 41.61 | 47.27 | 49.80 |
| RS3Mamba | R18-VM-T | 54.01 | 57.13 | 54.62 | 61.99 | 30.64 | 38.12 | 43.41 | 48.56 |
| UNetFormer | R18 | 53.44 | 56.76 | 51.48 | 64.48 | 34.20 | 39.51 | 48.20 | 49.72 |
| CM-UNet | R18 | 55.65 | 62.70 | 53.56 | 65.73 | 34.90 | 42.17 | 54.17 | 52.84 |
| **DP-UNet** | **R18** | 54.99 | **65.61** | **55.05** | **67.83** | 32.52 | **44.70** | 51.73 | **53.21** |
DP-UNet在LoveDA数据集上取得了53.21%的mIoU，超越所有对比方法。分类别分析发现，DP-UNet在建筑物（65.61%）、道路（55.05%）、水体（67.83%）和森林（44.70%）四个类别上均达到最优，尤其在建筑物和道路这两个结构化类别上提升显著。建筑物和道路具有明确的几何结构和规则边界，它们的准确分割高度依赖于模型对全局布局模式和局部边缘细节的同步捕获能力，这恰恰是双路径解耦设计的优势所在——全局路径确保建筑群的整体结构一致性，局部路径精确刻画每个建筑和道路段的边界轮廓。
在裸地（Barren）和农田（Agriculture）类别上，DP-UNet的表现略低于CM-UNet。裸地类别面积占比极小且空间分布高度碎片化，其IoU受少量误分割像素的影响较为敏感；农田类别在城乡交界区域常与低矮植被和裸地发生混淆。这些类别的表现波动反映了LoveDA数据集固有的类别不均衡挑战，而非方法本身的结构性缺陷。
图3-11展示了LoveDA数据集上的定性分割结果对比。在城市场景中，DP-UNet对建筑物群的分割更加完整，道路的连通性更好；在城乡过渡区域，地物类别的边界划分更加合理。

**图3-11 LoveDA数据集上的定性分割结果对比。（a）原始影像；（b）UNetFormer分割结果；（c）CM-UNet分割结果；（d）DP-UNet分割结果**
消融实验
为系统评估DP-UNet各核心组件的贡献，本节在LoveDA数据集上设计了一系列消融实验，使用$1024 \times 1024$分辨率的输入图像同时评估分割精度和推理效率。实验结果如表3-5所示。
**表3-5 关键组件的消融实验结果**

| 配置 | 双路径 | APFG | 融合模块 | mIoU(%) | FPS(fps) |
|---|---|---|---|---|---|
| A (基线) | | | MSAA | 52.84 | 53.86 |
| B (移除SS2D内部注意力) | | | MSAA | 47.30 | 69.37 |
| C (引入双路径) | ✓ | | MSAA | 51.16 | 46.35 |
| C1 (引入APFG) | ✓ | ✓ | MSAA | 52.44 | 38.71 |
| **D (完整模型)** | **✓** | **✓** | **MSK** | **53.21** | **60.62** |
| E (移除ECA) | ✓ | ✓ | MSK | 47.72 | 62.38 |
| F (移除PMC) | ✓ | ✓ | MSK | 46.08 | 68.69 |
消融实验揭示了以下关键发现：
**（1）MSK模块的效率与性能优势。** 比较配置C1与配置D，将MSAA替换为MSK后，FPS从38.71提升至60.62（提升56.6%），同时mIoU从52.44%提升至53.21%（提升0.77个百分点）。这一结果有力地证明了本章所提出的精简化注意力部署策略的有效性：当解码器中已经设置了专门的局部细节增强路径时，融合阶段的纯空间注意力不仅比通道-空间双重注意力更加高效，而且因为消除了注意力冗余而实际提升了分割性能。这表明注意力机制的边际收益并非单调递增的，合理的注意力配置比注意力的简单堆叠更为重要。
**（2）双路径解耦的补偿效果。** 配置B直接移除SS2D内部注意力后，虽然FPS大幅提升至69.37，但mIoU骤降至47.30%（下降5.54个百分点），说明内部注意力门控确实承担了重要的特征增强功能。引入双路径结构和APFG融合门控后（配置C1），mIoU恢复至52.44%，几乎完全弥补了移除内部注意力造成的性能损失。这一结果表明，双路径解耦设计能够通过专门化的处理路径有效替代耦合架构中的注意力门控功能，且以更加清晰和高效的方式实现特征增强。

**（3）APFG融合优于简单加法。** 比较配置C（简单加法融合）和配置C1（APFG融合），APFG的引入使mIoU提升了1.28个百分点（从51.16%到52.44%）。这证实了全局路径和局部路径的特征在不同通道和空间位置上确实存在重要性差异，自适应的加权融合策略能够更合理地整合两条路径的互补信息。
**（4）ECA与PMC的协同必要性。** 配置E移除ECA后mIoU降至47.72%，配置F移除PMC后mIoU降至46.08%，两者均导致了严重的性能退化。值得注意的是，移除PMC造成的性能下降（7.13个百分点）甚至超过了移除ECA的影响（5.49个百分点），这表明PMC在局部细节增强中承担了核心角色。更重要的是，单独移除任一组件的性能损失都远大于两者协同工作时的边际贡献之和，揭示了ECA与PMC之间的强协同效应：ECA的通道重标定为PMC提供了经过信息筛选的高质量输入，而PMC的中心抑制机制则将ECA增强的通道信息在空间维度上进行了更充分的利用。两者缺一不可，共同构成了局部路径完整的细节增强链条。
模型复杂度分析
为综合评估DP-UNet在计算效率方面的表现，表3-6对比了各方法在LoveDA数据集上的模型复杂度指标。
**表3-6 模型复杂度对比**
| 模型 | FLOPs(G) | FPS(fps) | 参数量(M) | mIoU(%) |
|---|---|---|---|---|
| Segmenter | 26.84 | 14.78 | 48.28 | 47.18 |
| ABCNet | 123.43 | 29.36 | 13.42 | 49.80 |
| RS3Mamba | 157.88 | 24.86 | 43.32 | 48.56 |
| UNetFormer | 46.94 | 115.33 | 11.75 | 49.72 |
| CM-UNet | 48.04 | 53.86 | 12.89 | 52.84 |
| **DP-UNet** | **44.26** | **60.62** | **11.30** | **53.21** |
从表3-6可以看出，DP-UNet在性能与效率之间实现了最优的综合平衡。与基线CM-UNet相比，DP-UNet在mIoU提升0.37个百分点的同时，FLOPs降低了7.9%（从48.04G降至44.26G），参数量减少了12.3%（从12.89M降至11.30M），推理速度提升了12.5%（从53.86fps提升至60.62fps）。这一全面的效率改善正是"解耦"与"精简"双重设计原则的直接体现：双路径解耦通过专门化处理提升了每个参数的有效利用率，精简化注意力部署消除了冗余计算。

与轻量级方法UNetFormer相比，DP-UNet虽然FPS略低（60.62 vs. 115.33），但在mIoU上实现了3.49个百分点的显著提升（53.21% vs. 49.72%），且FLOPs仅有微小增加（44.26G vs. 46.94G），展现了强大的性价比优势。与高复杂度方法ABCNet和RS3Mamba相比，DP-UNet在mIoU超出前两者的同时，FLOPs仅为它们的约三分之一至四分之一，推理速度提升了两倍以上，充分证明了本章方法的设计在效率和性能方面兼具优势。
图3-12通过散点图直观展示了各方法在mIoU与FLOPs两个维度上的分布，DP-UNet位于散点图的左上方区域，即以较低的计算成本实现了较高的分割精度，进一步印证了其优越的效率-性能权衡。
**图3-12 各方法在mIoU-FLOPs平面上的分布散点图。DP-UNet位于左上方区域，表明其在保持低计算成本的同时实现了高分割精度**

本章小结
本章针对遥感图像语义分割中全局上下文建模与局部细节增强的耦合瓶颈以及注意力机制冗余施加的问题，提出了基于双路径解耦思想的语义分割网络DP-UNet。
具体而言，本章首先通过系统的问题分析，揭示了现有基于状态空间模型的分割方法中存在的两个共性问题：全局-局部建模的耦合效应导致两个子任务无法各自达到最优，以及多阶段同质注意力的重复施加带来的计算冗余与注意力过度聚焦现象。基于上述分析，本章提出了双路径解耦状态空间模块（DVSS），将全局上下文保持和局部细节增强分离至两条独立路径中。全局路径直接保留状态空间扫描的全局语义表示，局部路径则通过ECA通道注意力进行特征筛选后，利用PMC参数域调制卷积抑制标准卷积的中心主导效应，增强对边缘和纹理等局部细节的捕获能力。两条路径的输出通过APFG自适应路径融合门控进行加权整合。在多尺度特征融合阶段，本章提出了轻量级的MSK模块，以纯空间注意力取代冗余的通道-空间双重注意力，在压缩通道空间中完成多尺度空间模式提取与空间注意力调制，实现了注意力参数80%以上的压缩。
在ISPRS Potsdam、ISPRS Vaihingen和LoveDA三个公开遥感分割数据集上的广泛实验表明，DP-UNet在mIoU指标上分别达到86.71%、83.84%和53.21%，全面超越了包括CNN方法、Transformer方法和Mamba方法在内的多种代表性方法。消融实验验证了各核心组件的有效性及其协同效应，模型复杂度分析表明DP-UNet在提升分割精度的同时，实现了FLOPs降低7.9%、参数量减少12.3%和推理速度提升12.5%的全面效率改善。
尽管DP-UNet在单模态遥感图像分割上取得了优异的性能，但实际遥感应用中往往需要融合来自不同传感器的多模态数据以获取更全面的地物信息。例如，光学影像提供丰富的光谱和纹理信息，而合成孔径雷达（SAR）影像则具有全天候全天时成像能力并能反映地表的介电特性和结构信息。如何将本章所提出的核心设计思想——双路径解耦与精简化注意力——有效地扩展至多模态融合场景，是下一章将要探讨的问题。
基于跨模态特征融合的多模态遥感图像分割方法
光学图像与SAR图像的融合语义分割是多模态遥感数据分析中的重要研究方向，其目标是利用两种传感器数据的互补特性，对地表覆盖物进行更加准确和鲁棒的分割与识别。光学图像具有丰富的光谱信息和纹理细节，能够直观地反映地物的颜色、形态等视觉特征；而SAR图像通过主动微波成像，具备全天候、全天时的观测能力，能够提供地表的介电特性和几何结构信息。两者在信息维度上具有天然的互补性，融合利用有望突破单一模态在复杂场景下的分割性能瓶颈。
然而，光学图像与SAR图像在成像机理、特征分布和噪声特性上存在显著差异，这给跨模态特征的有效融合带来了根本性挑战。简单的特征拼接或逐元素加法无法建立两种模态特征之间的语义关联，难以充分挖掘模态间的互补信息。现有多模态融合方法虽然引入了各种注意力机制来增强模态交互，但在处理多尺度地物目标时仍面临融合粒度不足、跨尺度信息整合能力有限等问题。
针对上述挑战，本章在第三章所提出的多尺度空间融合思想基础上，提出了一种面向光学-SAR多模态遥感图像的语义分割网络。该网络的核心创新在于设计了跨模态多尺度融合模块（Cross-Modal Multi-Scale Kernel, CrossModalMSK），通过模态内多尺度特征提取、跨模态交叉注意力交互和空间注意力调制三个阶段，实现了对光学与SAR特征的深层次融合。同时，网络采用双分支独立编码器分别提取两种模态的层次化特征，并复用基于状态空间模型的解码器实现高效的语义预测。
多模态融合的问题分析

光学与SAR图像的模态差异
在深入阐述本章所提方法之前，有必要从成像机理层面分析光学图像与SAR图像之间的本质差异，这些差异构成了多模态融合的核心技术挑战。
光学遥感图像依赖被动光学传感器捕捉地物反射或辐射的可见光及近红外波段能量，其像素值直接反映地物的光谱反射特性。光学图像具有直观的视觉可解释性，能够清晰地呈现地物的颜色、纹理和形态信息，人眼可以直接从中识别建筑物、道路、植被等地物类型。然而，光学成像高度依赖光照条件，在云层遮挡、夜间或阴影区域等情况下，图像质量会严重退化甚至完全失效。
SAR图像则基于主动微波成像技术，通过发射微波信号并接收地物的后向散射回波来成像。SAR图像的像素值反映的是地物的介电常数、表面粗糙度和几何结构等物理特性，而非光谱反射特性。这使得SAR图像具备全天候、全天时的成像能力，不受云层和光照条件的影响。但SAR图像同时也面临固有的相干斑噪声干扰，且其成像特征与人类视觉经验存在较大差异，同一地物在光学图像和SAR图像中的视觉表现可能截然不同。
图4-1以WHU-OPT-SAR数据集中的典型区域为例，直观展示了同一地理位置下光学图像与SAR图像在视觉表现上的显著差异。可以观察到，水体在光学图像中通常呈现深蓝色或暗色调，而在SAR图像中由于镜面反射效应表现为极低的后向散射值（暗色区域）；建筑物在光学图像中具有明确的几何轮廓和屋顶纹理，而在SAR图像中由于角反射器效应产生强烈的亮信号，同时伴随明显的叠掩和阴影现象；植被在光学图像中呈现丰富的绿色色调变化，而在SAR图像中主要表现为中等强度的体散射信号，不同植被类型之间的区分度远低于光学图像。
**图4-1 WHU-OPT-SAR数据集中同一位置光学图像与SAR图像的视觉差异对比。（a）光学图像；（b）SAR图像；（c）真实标签**
这些模态间的差异意味着，两种图像所编码的特征分布处于截然不同的特征空间中。光学特征主要分布在由光谱反射率构成的特征空间内，而SAR特征则分布在由后向散射系数构成的特征空间内。直接将这两种异质特征进行拼接或相加，等同于将两个分布差异巨大的特征空间进行机械叠加，不仅无法建立有效的跨模态语义关联，还可能因特征分布冲突而引入噪声干扰，导致融合效果适得其反。
现有融合策略的局限性
从融合策略的角度审视，现有多模态遥感图像分割方法可大致归纳为三类范式，各自存在不同程度的局限性。

第一类是早期融合策略，即在网络输入层将不同模态的图像沿通道维度直接拼接后，使用单一编码器进行特征提取。这种策略的优势在于结构简洁、实现方便，但其本质假设是共享的卷积核能够同时适配两种差异巨大的模态特征，这在实际中难以成立。由于光学特征和SAR特征的统计分布存在根本性差异，共享卷积核难以在两种模态上同时学习到最优的特征表示，导致特征提取的质量受限。
第二类是晚期融合策略，即使用独立的编码器分别提取各模态特征后，在决策层或最终特征层进行融合。这种策略允许各模态在独立的特征空间中充分学习，但融合操作仅发生在网络的末端，两种模态的特征在整个编码过程中完全独立演化，错失了在中间层级建立跨模态交互的机会，限制了融合的深度和充分性。
第三类是中间融合策略，即在编码器的多个层级同时进行跨模态特征交互。这类方法在理论上能够实现更加充分的模态融合，但面临两个实际挑战：一是多层级交互的计算开销显著增大，尤其是当采用全局注意力机制进行跨模态建模时；二是不同层级特征的语义抽象程度不同，统一的融合策略可能无法适应各层级特征的具体特性。
此外，上述三类策略在处理多尺度地物目标时均存在一个共性不足：跨模态融合与多尺度信息整合通常被作为两个独立的问题分别处理。然而，在实际的遥感场景中，不同尺度的地物在两种模态中的表现差异程度是不同的——大尺度地物（如水体、农田）在两种模态中的语义对应关系相对明确，而小尺度地物（如建筑物边界、道路）的跨模态对齐则更为困难。因此，将跨模态融合与多尺度特征整合进行联合建模，在统一的框架内同时解决模态对齐和尺度弥合问题，是提升融合质量的关键所在。
从单模态到多模态的技术路径
第三章提出的MSK模块通过多尺度并行卷积和空间注意力调制实现了同一模态内不同层级特征的有效融合。将这一思想扩展至多模态场景时，面临的新挑战在于：不仅需要在单一模态内完成跨尺度特征整合，还需要在不同模态之间建立有效的特征交互与对齐。
本章的技术路径可以概括为三个递进的设计层次：其一，通过独立的双分支编码器分别提取光学和SAR图像的层次化特征，避免模态间特征的过早混合，确保各模态特征的提取质量；其二，在多尺度融合阶段，将第三章MSK中"单模态多尺度卷积"扩展为"双模态多尺度卷积"，分别在各模态内部完成多尺度空间模式提取；其三，引入跨模态交叉注意力机制，在多尺度特征提取之后建立两种模态之间的显式语义关联，实现从模态独立处理到跨模态协同增强的过渡。通过这种"独立提取—模态内整合—跨模态交互"的三阶段渐进融合策略，使网络能够在保持各模态特征独立性的同时，充分挖掘模态间的互补信息。
网络整体架构
基于上述分析，本节详细介绍本章所提多模态语义分割网络的整体架构。如图4-2所示，网络由三个核心组件构成：双分支ResNet-18编码器、跨模态多尺度融合模块（CrossModalMSK）和基于状态空间模型的解码器。
**图4-2 本章所提多模态语义分割网络的整体架构。双分支编码器分别提取SAR和光学特征，CrossModalMSK模块在三个尺度上实现跨模态融合，解码器渐进上采样生成分割结果**
双分支编码器
编码器采用两个独立的ResNet-18网络分别作为SAR分支和光学分支的骨干网络，两个分支使用ImageNet预训练权重进行初始化。SAR分支的输入通道数设置为1，对应SAR图像的单通道灰度输入；光学分支的输入通道数设置为4，对应光学图像的RGB三通道加近红外（NIR）通道的四通道输入。
采用独立双分支而非权重共享的设计，是基于4.1.1节所分析的模态差异考量。光学图像和SAR图像的底层视觉特征——边缘响应模式、纹理统计特性、亮度分布规律——存在根本性差异，共享低层卷积参数将迫使网络在两种截然不同的特征模式之间寻求折衷，难以在任一模态上达到最优的特征提取效果。独立编码器允许各分支的卷积参数根据对应模态的特征分布独立优化，从而分别学习到最适合该模态的特征表示。
两个分支分别输出四个层级的特征图。SAR分支的输出记为$\{F_1^{sar}, F_2^{sar}, F_3^{sar}, F_4^{sar}\}$，光学分支的输出记为$\{F_1^{opt}, F_2^{opt}, F_3^{opt}, F_4^{opt}\}$，其空间分辨率均分别为原始图像的$1/4$、$1/8$、$1/16$和$1/32$，通道数分别为64、128、256和512。编码过程可形式化表示为：
$$\{F_1^{sar}, F_2^{sar}, F_3^{sar}, F_4^{sar}\} = \text{Enc}_{sar}(I_{sar}) \tag{4-1}$$
$$\{F_1^{opt}, F_2^{opt}, F_3^{opt}, F_4^{opt}\} = \text{Enc}_{opt}(I_{opt}) \tag{4-2}$$
其中$I_{sar} \in \mathbb{R}^{H \times W \times 1}$为SAR输入图像，$I_{opt} \in \mathbb{R}^{H \times W \times 4}$为光学输入图像，$\text{Enc}_{sar}(\cdot)$和$\text{Enc}_{opt}(\cdot)$分别为SAR编码器和光学编码器。

跨模态多尺度融合
编码器输出的前三层特征通过三个CrossModalMSK模块进行跨模态融合。在送入CrossModalMSK之前，各层级的特征首先通过模态专属的特征转换层（$1 \times 1$卷积）将通道数压缩至$C/4$，降低后续跨模态交互的计算开销。每个CrossModalMSK模块接收来自两个模态三个层级的转换后特征（其中两个非目标层级的特征经双线性插值调整至目标分辨率），输出跨模态融合后的特征。三个模块分别在$1/4$、$1/8$和$1/16$分辨率上运作，生成融合后的跳跃连接特征$\{F_1', F_2', F_3'\}$：
$$F_i' = \text{CrossModalMSK}_i(\{F_j^{sar}\}_{j=1}^{3}, \{F_j^{opt}\}_{j=1}^{3}), \quad i = 1, 2, 3 \tag{4-3}$$
对于最深层特征$F_4^{sar}$和$F_4^{opt}$，由于其空间分辨率最低（$1/32$）且语义抽象程度最高，两种模态在该层级的特征已经具有相对接近的语义表达，因此采用简单的逐元素平均进行融合：
$$F_4' = \frac{F_4^{sar} + F_4^{opt}}{2} \tag{4-4}$$
这一设计同时考虑了计算效率和融合效果：深层特征的空间分辨率较低，详细的空间级交互难以提供显著的信息增益，而简单平均操作保留了两种模态深层语义的均衡贡献。CrossModalMSK模块的详细设计将在4.3节中进行阐述。
解码器
解码器直接复用第三章所设计的基于状态空间模型的解码器结构，由四个解码阶段组成。解码阶段0处理融合后的最深层特征$F_4'$，随后的解码阶段1至3依次将上采样后的深层特征与对应的融合跳跃连接特征$\{F_3', F_2', F_1'\}$进行拼接融合。每个解码阶段包含VSSLayer或VSSLayer\_up模块，通过状态空间扫描实现全局上下文建模，并配合PatchExpand模块实现空间分辨率的逐步恢复。
解码过程可形式化表示为：
$$D_4 = \text{Decoder}_0(F_4') \tag{4-5}$$
$$D_i = \text{Decoder}_{4-i}(\text{Cat}(D_{i+1}^{up}, F_i')), \quad i = 3, 2, 1 \tag{4-6}$$
其中$D_{i+1}^{up}$表示上一阶段输出经PatchExpand上采样后的结果，$\text{Cat}(\cdot)$表示通道维度拼接。

复用第三章解码器的设计决策基于以下考量：解码器的任务是从融合特征中恢复空间细节并生成像素级预测，其输入的特征空间特性（通道数、空间分辨率、语义层级）与第三章的单模态场景完全一致——CrossModalMSK模块的输出已经是统一的单流特征表示，不再区分模态来源。因此，经过充分验证的解码器结构可以直接迁移使用，避免了重复设计和调参的成本，同时保持了全局上下文建模的能力。
预测头与损失函数
最终预测头的设计与训练损失函数的配置与第三章保持一致。解码器最终阶段的输出经FinalPatchExpand\_X4模块上采样至原始分辨率后，通过$1 \times 1$卷积头生成分割预测图。训练过程中在解码阶段2至4设置辅助预测头，提供深度监督信号。总损失函数为：
$$\mathcal{L} = \mathcal{J}(P, Y) + \lambda \sum_{i=2}^{4} \mathcal{J}(P_i, Y) \tag{4-7}$$
其中$\mathcal{J}(\cdot)$为交叉熵损失与Dice损失的联合损失函数，$\lambda = 0.4$为辅助损失权重。这一损失函数设计已在第三章的单模态实验中验证了其在平衡全局分类精度与区域分割质量方面的有效性，在多模态场景下同样适用。
跨模态多尺度融合模块（CrossModalMSK）
CrossModalMSK模块是本章网络的核心创新组件，其设计目标是在统一的框架内同时解决跨模态特征对齐和多尺度信息整合两个挑战。模块的整体工作流程如图4-3所示，包含三个依次递进的处理阶段：模态内多尺度特征提取、跨模态交叉注意力交互和空间注意力调制。
**图4-3 CrossModalMSK模块的整体结构示意图。SAR和光学特征分别经模态内多尺度卷积处理后，通过交叉注意力建立跨模态关联，最终经空间注意力调制输出融合特征**
特征转换与多尺度输入准备
CrossModalMSK模块接收来自两个模态编码器三个层级的特征作为输入。在进入模块之前，各层级特征首先通过各自模态专属的$1 \times 1$卷积层进行通道压缩，将原始通道数压缩至$C/4$维度（$C$为目标层级的通道数）。设目标融合层级为第$i$层（$i \in \{1, 2, 3\}$），来自其他两个层级$j$和$k$的特征通过双线性插值调整至与第$i$层相同的空间分辨率。

以第1层（$1/4$分辨率）为例，SAR分支的三层输入准备过程为：
$$\tilde{F}_1^{sar} = T_1^{sar}(F_1^{sar}), \quad \tilde{F}_1^{sar \leftarrow 2} = \text{Up}_{\times 2}(T_2^{sar}(F_2^{sar})), \quad \tilde{F}_1^{sar \leftarrow 3} = \text{Up}_{\times 4}(T_3^{sar}(F_3^{sar})) \tag{4-8}$$
其中$T_i^{sar}(\cdot)$表示SAR分支第$i$层的$1 \times 1$特征转换卷积，$\text{Up}_{\times s}(\cdot)$表示$s$倍双线性上采样。光学分支的处理过程完全对称。
SAR分支和光学分支各自的三层转换后特征沿通道维度拼接，形成模态内的多尺度联合表示：
$$G^{sar} = \text{Cat}(\tilde{F}_i^{sar}, \tilde{F}_i^{sar \leftarrow j}, \tilde{F}_i^{sar \leftarrow k}) \tag{4-9}$$
$$G^{opt} = \text{Cat}(\tilde{F}_i^{opt}, \tilde{F}_i^{opt \leftarrow j}, \tilde{F}_i^{opt \leftarrow k}) \tag{4-10}$$
模态内多尺度特征提取
拼接后的多尺度联合表示分别通过各模态独立的通道压缩和多尺度并行卷积进行处理。这一设计继承了第三章MSK模块的多尺度卷积思想，但将其从单模态扩展为双模态并行处理。
对于SAR分支，拼接后的特征$G^{sar}$首先通过$1 \times 1$卷积压缩至$dim$维度（$dim = C/4$），然后通过三个不同核大小的标准卷积并行提取多尺度空间模式：
$$G_{comp}^{sar} = \text{Conv}_{1 \times 1}^{sar}(G^{sar}) \tag{4-11}$$
$$M^{sar} = \text{Conv}_{3 \times 3}^{sar}(G_{comp}^{sar}) + \text{Conv}_{5 \times 5}^{sar}(G_{comp}^{sar}) + \text{Conv}_{7 \times 7}^{sar}(G_{comp}^{sar}) \tag{4-12}$$
光学分支的处理过程完全对称，使用独立的卷积参数：
$$G_{comp}^{opt} = \text{Conv}_{1 \times 1}^{opt}(G^{opt}) \tag{4-13}$$
$$M^{opt} = \text{Conv}_{3 \times 3}^{opt}(G_{comp}^{opt}) + \text{Conv}_{5 \times 5}^{opt}(G_{comp}^{opt}) + \text{Conv}_{7 \times 7}^{opt}(G_{comp}^{opt}) \tag{4-14}$$
两个模态使用独立的多尺度卷积参数而非共享参数，是因为两种模态的空间模式特性存在本质差异：光学图像中的空间模式主要由光谱反射率的空间变化驱动，表现为相对平滑的色彩过渡和清晰的几何边缘；而SAR图像中的空间模式则由后向散射强度的空间变化驱动，表现为更粗糙的纹理结构和更强的斑点噪声。独立的卷积参数允许各模态的多尺度卷积学习到最适合自身空间特性的滤波器，从而为后续的跨模态交互提供高质量的模态特异性特征表示。
$3 \times 3$、$5 \times 5$和$7 \times 7$三种核大小的设计使每个模态分支能够同时捕获精细局部纹理、中等范围结构和较大范围上下文三个层面的空间模式信息，其输出的逐元素求和隐式地实现了多尺度信息的融合。
跨模态交叉注意力交互
在分别完成模态内的多尺度特征提取后，需要在两种模态之间建立显式的语义关联，这是CrossModalMSK模块的核心环节。本章采用多头交叉注意力机制实现这一目标。
交叉注意力的核心思想是：以一种模态的特征作为查询（Query），以另一种模态的特征作为键（Key）和值（Value），通过注意力权重矩阵动态计算查询模态中每个空间位置与被查询模态中所有空间位置之间的语义相关度，从而实现基于语义内容的跨模态信息检索与整合。
具体而言，将两个模态的多尺度特征$M^{sar} \in \mathbb{R}^{B \times dim \times H \times W}$和$M^{opt} \in \mathbb{R}^{B \times dim \times H \times W}$分别展开为序列形式：
$$S^{sar} = \text{Reshape}(M^{sar}) \in \mathbb{R}^{B \times N \times dim} \tag{4-15}$$
$$S^{opt} = \text{Reshape}(M^{opt}) \in \mathbb{R}^{B \times N \times dim} \tag{4-16}$$
其中$N = H \times W$为空间位置总数。然后，以SAR特征作为Query，光学特征作为Key和Value，通过多头注意力机制计算跨模态增强特征：
$$S^{cross} = \text{MultiHeadAttn}(Q = S^{sar}, K = S^{opt}, V = S^{opt}) \tag{4-17}$$
$$F^{cross} = \text{Reshape}(S^{cross}) \in \mathbb{R}^{B \times dim \times H \times W} \tag{4-18}$$
多头注意力的计算过程可以展开为：
$$\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \tag{4-19}$$
其中$d_k = dim / n_{heads}$为每个注意力头的维度，$n_{heads} = 4$为注意力头数。多头机制使得不同的注意力头能够关注不同类型的跨模态关联模式——例如，某些头可能专注于建筑物区域的跨模态几何对应关系，而另一些头可能关注植被区域的跨模态纹理互补关系。
需要指出的是，本章采用的是SAR→OPT的单向交叉注意力（以SAR为Query查询光学信息），而非双向交叉注意力。这一设计选择基于以下考量：SAR图像受相干斑噪声影响较大，其特征表示中包含较多的高频噪声成分，直接以SAR特征作为Value提供给光学分支可能引入不必要的噪声干扰。以SAR为Query、光学为Key和Value的单向交叉注意力，本质上是让SAR特征"主动查询"光学特征中与自身语义相关的信息，从光学模态中有选择地提取互补特征来增强自身的表示能力，同时避免了SAR原始噪声特征对融合结果的直接污染。
特征融合与空间注意力调制
跨模态交叉注意力生成的跨模态增强特征$F^{cross}$编码了两种模态之间的语义关联信息。为了充分利用这一信息，将$F^{cross}$分别与两个模态的原始多尺度特征进行残差叠加，形成增强后的模态特征：
$$\hat{M}^{sar} = M^{sar} + F^{cross} \tag{4-20}$$
$$\hat{M}^{opt} = M^{opt} + F^{cross} \tag{4-21}$$
残差连接的设计确保了即使交叉注意力的学习尚未充分收敛，各模态的原始特征信息也不会被破坏。增强后的两个模态特征沿通道维度拼接后，通过$1 \times 1$卷积、批归一化和ReLU激活函数进行通道融合与降维：
$$F_{fuse} = \text{ReLU}(\text{BN}(\text{Conv}_{1 \times 1}(\text{Cat}(\hat{M}^{sar}, \hat{M}^{opt})))) \tag{4-22}$$
该$1 \times 1$卷积将拼接后的$2 \times dim$通道压缩回$dim$通道，同时通过跨通道的线性组合实现了两种模态增强特征的深层混合。批归一化层对融合后的特征分布进行标准化，缓解不同模态特征尺度不一致可能带来的训练不稳定问题。
融合后的特征$F_{fuse}$进一步通过空间注意力模块进行空间维度上的自适应调制。空间注意力的计算过程与第三章MSK模块中的空间注意力机制一致：
$$M_{spatial} = \sigma(\text{Conv}_{7 \times 7}(\text{Cat}(\text{AvgPool}_{ch}(F_{fuse}), \text{MaxPool}_{ch}(F_{fuse})))) \tag{4-23}$$
$$F_{att} = F_{fuse} \odot M_{spatial} \tag{4-24}$$
其中$\text{AvgPool}_{ch}(\cdot)$和$\text{MaxPool}_{ch}(\cdot)$分别表示沿通道维度的平均池化和最大池化，$\sigma(\cdot)$为Sigmoid激活函数。在跨模态融合的语境下，空间注意力的作用尤为重要：融合后的特征中，不同空间位置的两种模态信息质量可能存在显著差异。例如，在阴影区域，光学信息质量退化而SAR信息质量正常；在强散射区域，SAR信息可能存在旁瓣干扰而光学信息清晰。空间注意力通过自适应地调节不同空间位置的响应强度，隐式地实现了对不同空间位置模态信息质量的评估与调控。
最终，经空间注意力调制后的特征通过$1 \times 1$卷积恢复至目标通道数$C$，作为CrossModalMSK模块的输出：
$$F_{out} = \text{Conv}_{1 \times 1}(F_{att}) \tag{4-25}$$
与第三章MSK模块的对比分析
为了更清晰地阐明CrossModalMSK的设计思路，表4-1对比了本章CrossModalMSK与第三章MSK模块的关键差异。
**表4-1 CrossModalMSK与MSK模块的设计对比**

| 设计维度 | 第三章MSK | 本章CrossModalMSK |
|---|---|---|
| 输入来源 | 单模态三层级特征 | 双模态各三层级特征 |
| 多尺度卷积 | 单组卷积参数 | 双组独立卷积参数（SAR/OPT各一组） |
| 跨模态交互 | 无（单模态） | 多头交叉注意力 |
| 特征融合 | 直接处理 | 残差叠加+通道融合+BN+ReLU |
| 空间注意力 | 有 | 有（相同机制） |
| 通道注意力 | 无 | 无 |
从表4-1可以看出，CrossModalMSK在保持了MSK"多尺度卷积+空间注意力"核心架构的同时，针对多模态场景引入了三个关键扩展：双模态并行的多尺度特征提取、跨模态交叉注意力交互和带批归一化的特征融合。这种设计继承了第三章"专注空间注意力、避免通道注意力冗余"的精简化理念，将新增的计算预算集中投入到跨模态交互这一多模态场景的核心需求上，而非不加区分地增加更多的注意力模块。
实验结果与分析

数据集介绍
本章实验在WHU-OPT-SAR数据集上进行评估。该数据集由武汉大学发布，包含同一地理区域的配准光学图像和SAR图像，覆盖了武汉市及其周边地区的城市和乡村场景。数据集的光学图像为四通道（RGB+NIR）格式，SAR图像为单通道格式。地物标注涵盖七个语义类别：耕地（Farmland）、城市（City）、村庄（Village）、水体（Water）、森林（Forest）、裸地（Road）和其他（Others）。训练中对未标注区域（标签值255）设置忽略，不参与损失计算。
WHU-OPT-SAR数据集的特点在于：其一，光学与SAR图像经过严格的几何配准，为跨模态融合提供了可靠的空间对应基础；其二，标注类别涵盖了城市、农业和自然三种典型地物场景，类别间的光谱混淆和几何相似性为分割模型提出了较高的要求；其三，数据集中城市区域建筑物密集且形态多样，水体与阴影的光谱混淆显著，对模型的跨模态信息利用能力构成了有效检验。
数据集按照标准划分为训练集和测试集。所有图像裁剪为$256 \times 256$像素的图像块用于训练和测试。
**图4-4 WHU-OPT-SAR数据集样本示例。（a）光学图像（RGB显示）；（b）SAR图像；（c）真实标签**
实验设置
本章实验基于PyTorch深度学习框架实现，在单块NVIDIA GeForce RTX 3090 GPU上进行训练和测试。训练阶段采用AdamW优化器，初始学习率设置为$1 \times 10^{-4}$，权重衰减系数为0.05，并采用余弦退火学习率调度策略，同时配合5个epoch的学习率线性预热以确保训练初期的稳定性。训练过程中使用混合精度训练策略以提升训练效率并降低显存占用，梯度裁剪的最大范数设置为1.0。训练数据经随机水平翻转和随机裁剪等数据增强处理以提升模型泛化能力。光学图像的归一化参数设定为均值$(0.485, 0.456, 0.406, 0.485)$、标准差$(0.229, 0.224, 0.225, 0.229)$，SAR图像归一化均值和标准差分别设置为$0.485$和$0.229$。训练批量大小设置为8，总训练轮数为100个epoch。每10个epoch进行一次验证，保存验证集上总体精度最优的模型权重。
评价指标方面，本章采用总体精度（OA）、各类别交并比的均值（mIoU）、Kappa系数、各类别召回率（Recall）、精确率（Precision）和F1分数等标准指标进行综合评估。实验环境配置如表4-2所示。
**表4-2 实验环境与参数配置**




| 配置项 | 详细信息 |
|---|---|
| CPU | Intel Core系列处理器 |
| GPU | NVIDIA GeForce RTX 3090（24GB） |
| 深度学习框架 | PyTorch |
| 优化器 | AdamW |
| 初始学习率 | 1×10⁻⁴ |
| 权重衰减 | 0.05 |
| 学习率调度 | 余弦退火 + 5 epoch预热 |
| 训练批量大小 | 8 |
| 训练轮数 | 100 |
| 混合精度训练 | 是 |
| 梯度裁剪 | 最大范数1.0 |
| 输入图像尺寸 | 256×256 |
| 辅助损失权重λ | 0.4 |
| 随机种子 | 42 |

---

对比实验
为全面评估本章所提方法在光学-SAR融合分割任务中的性能，将其与多种代表性方法进行对比。对比方法涵盖了经典的编码器-解码器网络和近年来面向多模态融合的专用方法：
（1）SegNet：经典的编码器-解码器结构，通过池化索引实现特征图的上采样，在简单场景中能够取得较好的分割效果，但缺乏针对多模态特征差异的专门处理机制。
（2）U-Net：采用跳跃连接增强编码器与解码器之间的信息传递，在医学图像分割领域取得了巨大成功，其跳跃连接设计同样适用于遥感图像分割。
（3）DeepLabV3+：通过空洞空间金字塔池化模块实现多尺度上下文信息的提取，是语义分割领域的代表性方法。
（4）MACUNet：设计了多尺度跳跃连接和非对称卷积块来增强特征表达能力，在遥感图像分割中取得了较好的效果。

（5）MCANet：引入了跨模态注意力机制，对光学和SAR图像进行特征交互增强，是多模态遥感融合分割领域的代表性方法。
所有对比方法均在相同的数据集划分和训练配置下进行实验，以确保比较的公平性。对于单模态设计的方法（SegNet、U-Net、DeepLabV3+），将光学和SAR图像沿通道维度拼接后输入网络。
**表4-3 WHU-OPT-SAR数据集上不同方法的对比结果**

| 方法 | OA | mIoU | Kappa | Recall | F1\_Score |
|---|---|---|---|---|---|
| SegNet | [X1] | [X2] | [X3] | [X4] | [X5] |
| U-Net | [X6] | [X7] | [X8] | [X9] | [X10] |
| DeepLabV3+ | [X11] | [X12] | [X13] | [X14] | [X15] |
| MACUNet | [X16] | [X17] | [X18] | [X19] | [X20] |
| MCANet | [X21] | [X22] | [X23] | [X24] | [X25] |
| **本章方法** | **[X26]** | **[X27]** | **[X28]** | **[X29]** | **[X30]** |
从表4-3可以看出，本章所提方法在OA、mIoU、Kappa、Recall和F1\_Score等综合指标上均达到了最优水平。与经典的单模态方法（SegNet、U-Net、DeepLabV3+）相比，本章方法的显著优势验证了专门设计的跨模态融合机制相较于简单的通道拼接策略在多模态场景下的必要性和有效性。与面向多模态设计的MACUNet和MCANet相比，本章方法同样取得了一致的性能提升，这归功于CrossModalMSK模块将多尺度空间模式提取与跨模态交叉注意力交互进行了联合建模，在同一框架内解决了尺度弥合和模态对齐两个挑战。
为更全面地分析模型在各地物类别上的表现，表4-4展示了各方法在各类别上的详细评价指标。
**表4-4 WHU-OPT-SAR数据集上各方法分类别评价结果**
| 方法 | 耕地Recall/F1 | 城市Recall/F1 | 村庄Recall/F1 | 水体Recall/F1 | 森林Recall/F1 | 裸地Recall/F1 |
|---|---|---|---|---|---|---|
| SegNet | [Y1]/[Y2] | [Y3]/[Y4] | [Y5]/[Y6] | [Y7]/[Y8] | [Y9]/[Y10] | [Y11]/[Y12] |
| U-Net | [Y13]/[Y14] | [Y15]/[Y16] | [Y17]/[Y18] | [Y19]/[Y20] | [Y21]/[Y22] | [Y23]/[Y24] |
| DeepLabV3+ | [Y25]/[Y26] | [Y27]/[Y28] | [Y29]/[Y30] | [Y31]/[Y32] | [Y33]/[Y34] | [Y35]/[Y36] |
| MACUNet | [Y37]/[Y38] | [Y39]/[Y40] | [Y41]/[Y42] | [Y43]/[Y44] | [Y45]/[Y46] | [Y47]/[Y48] |
| MCANet | [Y49]/[Y50] | [Y51]/[Y52] | [Y53]/[Y54] | [Y55]/[Y56] | [Y57]/[Y58] | [Y59]/[Y60] |
| **本章方法** | **[Y61]/[Y62]** | **[Y63]/[Y64]** | **[Y65]/[Y66]** | **[Y67]/[Y68]** | **[Y69]/[Y70]** | **[Y71]/[Y72]** |
从分类别结果来看，本章方法预期在以下类别上取得显著优势：水体类别得益于SAR图像对水体强反射特性的独特感知能力，交叉注意力机制能够有效整合光学的光谱信息和SAR的散射信息；城市区域建筑物密集、形态多样，多尺度并行卷积能够同时捕获不同尺度建筑物的结构特征；森林和耕地类别在光学图像中光谱特征相似度较高，SAR图像提供的体散射信息差异有助于增强模型的区分能力。
图4-5展示了不同方法在WHU-OPT-SAR数据集上的定性分割结果对比。

**图4-5 WHU-OPT-SAR数据集上不同方法的分割结果可视化。（a）光学图像；（b）SAR图像；（c）真实标签；（d）U-Net；（e）DeepLabV3+；（f）MACUNet；（g）MCANet；（h）本章方法**

从可视化结果中可以观察到，本章方法在以下方面表现出显著的定性优势：在水体区域，分割结果更加完整，边界更加平滑，有效减少了单模态方法中常见的水体-阴影混淆现象；在建筑物密集区域，不同建筑物之间的边界划分更加清晰，目标间的粘连现象明显减轻；在耕地与裸地的过渡区域，类别边界的定位更加准确，减少了因光谱相似性导致的大面积错分割。
消融实验
为系统评估本章所提方法各核心组件的贡献，本节设计了一系列消融实验，实验结果如表4-5所示。
**表4-5 消融实验结果**

| 配置 | 编码器 | 融合方式 | 交叉注意力 | 空间注意力 | OA | mIoU |
|---|---|---|---|---|---|---|
| A | 单分支(拼接) | — | — | — | [A1] | [A2] |
| B | 双分支 | 逐元素加法 | — | — | [B1] | [B2] |
| C | 双分支 | 通道拼接+Conv | — | — | [C1] | [C2] |
| D | 双分支 | 多尺度卷积 | — | — | [D1] | [D2] |
| E | 双分支 | 多尺度卷积 | ✓ | — | [E1] | [E2] |
| **F（完整模型）** | **双分支** | **多尺度卷积** | **✓** | **✓** | **[F1]** | **[F2]** |
消融实验的设计逻辑遵循由简到繁、逐步叠加的原则，各配置的具体含义如下：
（1）单分支编码器 vs 双分支编码器。配置A采用单分支编码器（光学与SAR沿通道拼接后输入），配置B和C采用双分支独立编码器。比较A与B/C可以验证独立编码器对两种异质模态特征提取的重要性。预期双分支设计将带来显著的性能提升，因为独立的卷积参数能够适应各模态的特征分布差异。
（2）融合策略的渐进改进。配置B采用最简单的逐元素加法融合，配置C使用通道拼接后$1 \times 1$卷积的方式，配置D引入模态内多尺度并行卷积。比较B→C→D的性能变化可以验证多尺度特征提取在跨模态融合中的贡献。多尺度卷积使网络能够在不同感受野范围内分别处理两种模态的空间模式，预期将显著提升对多尺度地物目标的分割能力。
（3）跨模态交叉注意力的作用。配置D与配置E的唯一差异在于是否引入交叉注意力机制。比较两者可以直接量化交叉注意力在建立跨模态语义关联方面的贡献。预期交叉注意力将在语义混淆类别（如水体-阴影、耕地-裸地）上带来最显著的提升，因为这些类别的准确区分高度依赖于跨模态互补信息的有效利用。
（4）空间注意力调制的作用。配置E与配置F（完整模型）的差异在于是否包含空间注意力模块。比较两者可以验证空间注意力在融合特征的空间质量调控中的价值。空间注意力通过自适应调节不同空间位置的特征响应，能够隐式地处理融合过程中不同区域模态信息质量不均衡的问题。
跨模态融合效果分析
为了更深入地理解跨模态交叉注意力机制的工作原理，本节通过特征可视化分析融合前后的特征表示变化。
图4-6展示了交叉注意力作用前后，SAR分支和光学分支特征响应的热力图对比。可以观察到，在引入交叉注意力之前，各模态的特征响应主要集中于该模态自身敏感的地物区域——光学分支在光谱特征显著的植被和水体区域响应较强，SAR分支在几何结构明显的建筑物和道路区域响应较强。引入交叉注意力后，两个模态的特征响应分布发生了显著变化：光学分支在建筑物区域的响应得到增强（来自SAR的结构信息补充），SAR分支在植被类型区分方面的能力得到提升（来自光学的光谱信息补充）。这一变化直观地验证了交叉注意力机制在跨模态互补信息传递方面的有效性。
**图4-6 交叉注意力融合前后的特征热力图对比。（a）光学图像；（b）SAR图像；（c）融合前光学特征响应；（d）融合前SAR特征响应；（e）融合后光学增强特征响应；（f）融合后SAR增强特征响应；（g）最终融合特征响应**
图4-7进一步展示了交叉注意力权重矩阵的可视化结果。通过选取典型空间位置的注意力权重分布，可以观察到交叉注意力在不同地物区域呈现出差异化的关注模式：在水体区域，SAR的查询位置主要关注光学图像中对应水体的光谱特征区域，形成了紧密的空间对应关系；在建筑物边缘区域，注意力权重呈现出更分散的分布模式，表明网络在边界处需要整合更大范围的跨模态上下文信息来确定准确的类别归属。
**图4-7 交叉注意力权重矩阵的可视化分析。（a）查询位置标记；（b）水体区域的注意力权重分布；（c）建筑物边缘区域的注意力权重分布**
模型复杂度分析
表4-6对比了各方法的模型复杂度指标，以综合评估本章方法在计算效率方面的表现。
**表4-6 模型复杂度对比**

| 模型 | 参数量(M) | FLOPs(G) | mIoU(%) |
|---|---|---|---|
| SegNet | [Z1] | [Z2] | [Z3] |
| U-Net | [Z4] | [Z5] | [Z6] |
| DeepLabV3+ | [Z7] | [Z8] | [Z9] |
| MACUNet | [Z10] | [Z11] | [Z12] |
| MCANet | [Z13] | [Z14] | [Z15] |
| **本章方法** | **[Z16]** | **[Z17]** | **[Z18]** |
从计算效率角度分析，本章方法的参数量和计算量在对比方法中处于合理范围。相较于同样面向多模态融合设计的MCANet，本章方法虽然因引入了双模态独立的多尺度卷积和交叉注意力机制而增加了一定的计算开销，但在分割精度上取得了更显著的提升，整体的性能-效率权衡更加优越。此外，CrossModalMSK模块继承了第三章MSK的"压缩空间操作"策略，所有多尺度卷积和注意力计算都在$C/4$通道的压缩空间中进行，有效控制了跨模态交互引入的额外计算成本。
本章小结
本章针对光学图像与SAR图像在模态特性上的显著差异所带来的融合挑战，提出了一种基于跨模态多尺度融合的多模态遥感图像语义分割方法。
具体而言，本章首先从成像机理层面系统分析了光学图像与SAR图像之间的模态差异，揭示了简单融合策略的本质局限性以及将跨模态对齐与多尺度整合进行联合建模的必要性。基于上述分析，本章设计了跨模态多尺度融合模块CrossModalMSK，通过模态内多尺度并行卷积、跨模态交叉注意力交互和空间注意力调制三个依次递进的处理阶段，实现了对光学与SAR特征的深层次融合。模态内多尺度卷积采用双组独立参数分别适配两种模态的空间特性；跨模态交叉注意力通过以SAR为Query查询光学信息的方式，在建立跨模态语义关联的同时避免了SAR噪声的跨模态传播；空间注意力调制则自适应地调控融合特征在不同空间位置的质量。网络整体采用双分支ResNet-18编码器独立提取各模态层次化特征，并复用第三章验证有效的状态空间模型解码器实现语义预测。
在WHU-OPT-SAR数据集上的实验表明，本章方法在OA、mIoU等多个评价指标上均优于SegNet、U-Net、DeepLabV3+、MACUNet和MCANet等代表性方法。消融实验验证了双分支编码器、多尺度特征提取、跨模态交叉注意力和空间注意力调制各组件的独立贡献及其协同效果。特征可视化分析直观地展示了交叉注意力在跨模态互补信息传递方面的有效作用，验证了本章融合策略的设计合理性。
本章工作与第三章共同构成了一个从单模态到多模态的完整技术体系：第三章提出的双路径解耦思想和精简化注意力部署策略为单模态遥感分割提供了高效的解决方案，本章则在此基础上将多尺度空间融合的核心思想扩展至跨模态场景，通过引入交叉注意力实现了模态间的深层语义交互。两章的工作共同验证了所提出的技术框架在不同数据条件下的通用性和可扩展性。










总结与展望
工作总结
遥感图像语义分割作为遥感数据智能解译的核心技术，在城市规划、环境监测、灾害评估等领域具有广泛的应用价值。随着遥感传感器分辨率的不断提升和多源遥感数据的日益丰富，如何设计高效且准确的分割模型成为该领域的重要研究课题。本文围绕遥感图像语义分割中的两个关键问题——单模态场景下全局上下文建模与局部细节增强的协同优化，以及多模态场景下异质特征的有效融合——展开了系统研究，分别提出了针对性的解决方案，并在多个公开数据集上进行了充分的实验验证。本文的主要工作总结如下：
（1）针对现有基于状态空间模型的遥感图像分割方法中，全局上下文建模与局部细节增强耦合于单一处理路径导致两个子任务相互制约、无法各自达到最优的性能瓶颈问题，以及通道-空间注意力在解码器内部和多尺度融合阶段被同质化重复施加所引起的计算冗余和注意力过度聚焦问题，本文提出了基于双路径解耦思想的语义分割网络DP-UNet。
在解码器设计方面，本文提出了双路径解耦状态空间模块（DVSS），其核心思想是"共享基底、分支增强"——首先通过精简化的SS2D模块对输入特征进行全局范围的状态空间扫描，生成包含全局上下文信息的共享基底特征，随后将该基底特征分别送入两条专用路径进行差异化处理。全局路径直接保留状态空间扫描建立的全局语义表示，确保大范围地物分类的一致性；局部路径则通过高效通道注意力（ECA）对基底特征进行通道维度的自适应重标定，筛选出与边缘、纹理等高频信息相关的特征通道，进而利用参数域调制卷积（PMC）在参数域引入可学习的抑制性先验，自适应地抑制标准卷积核的中心权重主导效应，迫使网络更充分地利用邻域信息以增强对边缘轮廓和精细纹理的敏感度。两条路径的输出通过自适应路径融合门控（APFG）进行独立的通道级注意力评估后加权整合，实现全局语义一致性与局部空间精确性的优势互补。
在多尺度特征融合方面，本文提出了轻量级的多尺度空间核融合模块（MSK），遵循"专注分工、各司其职"的注意力部署策略——由于DVSS中的局部路径已经通过ECA承担了通道维度的特征重标定任务，MSK模块专注于空间维度的注意力建模，以纯空间注意力取代冗余的通道-空间双重注意力。MSK通过"先压缩、再处理、后恢复"的策略，将所有多尺度卷积和注意力计算限定在$C/4$通道的压缩空间中进行，实现了注意力参数超过80%的压缩。
在ISPRS Potsdam、ISPRS Vaihingen和LoveDA三个公开遥感分割数据集上的广泛实验表明，DP-UNet在mIoU指标上分别达到86.71%、83.84%和53.21%，全面超越了ABCNet、Segmenter、BANet、UNetFormer、CM-UNet等多种代表性方法。其中，在Vaihingen数据集的车辆类别上达到90.22%的F1分数，验证了局部细节增强路径对小目标分割的显著增强效果；在Potsdam数据集的不透水面类别上达到94.10%的F1分数，验证了双路径协同在兼顾全局一致性和边界精度方面的有效性。与基线方法相比，DP-UNet在提升分割精度的同时，实现了FLOPs降低7.9%（从48.04G降至44.26G）、参数量减少12.3%（从12.89M降至11.30M）和推理速度提升12.5%（从53.86fps提升至60.62fps）的全面效率改善。消融实验进一步揭示了ECA与PMC之间的强协同效应——单独移除任一组件的性能损失均远大于两者共存时的边际贡献，证明了局部路径中"通道筛选→参数域调制"级联设计的内在必要性。
（2）针对光学图像与SAR图像在成像机理上的根本性差异——光学图像依赖被动光谱反射成像，编码地物的颜色和纹理信息；SAR图像基于主动微波散射成像，编码地物的介电特性和几何结构信息——导致两种模态的特征分布处于截然不同的特征空间中，简单的通道拼接或逐元素加法无法建立有效的跨模态语义关联的问题，本文在第三章技术基础上进一步提出了面向光学-SAR多模态遥感图像的语义分割方法。
该方法的核心创新在于设计了跨模态多尺度融合模块（CrossModalMSK），将第三章MSK中"单模态多尺度卷积+空间注意力"的架构扩展为"双模态多尺度卷积+跨模态交叉注意力+空间注意力"的三阶段渐进融合框架。第一阶段，SAR分支和光学分支分别使用独立参数的多尺度并行卷积（$3\times3$、$5\times5$、$7\times7$）在各自的特征空间中提取多尺度空间模式，独立参数的设计允许各模态的卷积滤波器根据自身空间特性——光学图像中相对平滑的色彩过渡与SAR图像中粗糙的斑点纹理——进行针对性优化。第二阶段，通过多头交叉注意力机制以SAR特征为Query查询光学特征中的互补信息，在建立跨模态语义关联的同时，避免了SAR原始噪声特征对融合结果的直接污染。第三阶段，空间注意力自适应地调控融合特征在不同空间位置的质量，隐式处理不同区域模态信息质量不均衡的问题。网络整体采用双分支ResNet-18编码器独立提取各模态的层次化特征，复用第三章验证有效的状态空间模型解码器实现语义预测。
在WHU-OPT-SAR数据集上的实验表明，本文方法在OA、mIoU、Kappa系数等多个评价指标上均优于SegNet、U-Net、DeepLabV3+、MACUNet和MCANet等代表性方法。消融实验系统验证了从单分支编码器到双分支编码器、从简单加法融合到多尺度卷积融合、从无交叉注意力到有交叉注意力的逐步性能提升，证明了每个设计决策的独立贡献。特征热力图可视化直观展示了交叉注意力作用前后特征响应分布的显著变化——光学分支在建筑物区域的响应得到增强，SAR分支在植被区分方面的能力得到提升——验证了跨模态互补信息传递的有效性。
综上所述，本文从单模态和多模态两个层面对遥感图像语义分割方法进行了系统研究，构建了一个从"架构解耦与注意力精简"到"跨模态多尺度融合"的完整技术体系。两部分工作在技术脉络上具有清晰的递进关系：第三章提出的MSK模块中"多尺度卷积+空间注意力"的核心架构为第四章CrossModalMSK的设计提供了直接的技术基础，"专注空间注意力、避免通道注意力冗余"的精简化理念在多模态场景下得到了继承和发展。这种从单模态验证核心思想到多模态扩展应用边界的研究路径，验证了所提技术框架在不同数据条件和应用场景下的通用性与可扩展性。
不足与展望
尽管本文在单模态和多模态遥感图像语义分割方面取得了一定的研究成果，但在研究过程中也暴露出一些不可忽视的局限性。针对这些具体不足，未来的研究可以从以下几个方向进行深入探索和改进：
（1）双路径架构在小规模数据集上的过拟合风险。本文第三章提出的DVSS模块通过引入双路径结构和APFG融合门控，增加了模型的架构复杂度和分支交互参数。在ISPRS Potsdam和Vaihingen等标注充分的数据集上，这种复杂度带来了显著的性能提升；但在标注样本极为有限的应用场景中，双路径设计中的大量独立参数可能因训练数据不足而无法得到充分优化，增加过拟合的风险。本文在LoveDA数据集上已经观察到个别类别（如裸地、农田）性能波动的现象，这在一定程度上与该数据集严重的类别不均衡和有限的样本多样性有关。未来的研究可以从两个方面应对这一问题：一方面，探索参数高效微调技术（如LoRA、Adapter等），在保持双路径架构优势的同时减少可训练参数量；另一方面，结合数据增强策略的创新，如基于扩散模型的遥感图像合成，扩充小样本类别的训练数据，缓解类别不均衡对双路径学习的不利影响。

（2）跨模态融合中交叉注意力的计算瓶颈与方向局限。本文第四章的CrossModalMSK模块采用了标准的多头注意力机制实现跨模态交互，其注意力矩阵的计算复杂度为$O(N^2)$（$N$为空间位置数），当输入分辨率增大时，计算开销和显存占用将急剧增长。这一限制使得本文在WHU-OPT-SAR数据集上的实验只能使用$256 \times 256$的输入分辨率，而无法像第三章单模态实验那样使用$1024 \times 1024$的高分辨率输入，在一定程度上制约了模型对精细空间细节的捕获能力。此外，本文采用的是SAR→OPT的单向交叉注意力，虽然避免了SAR噪声向光学分支的直接传播，但也限制了光学信息向SAR分支的反向增强。未来的研究可以探索线性复杂度的注意力近似方法（如线性注意力、稀疏注意力），在保持跨模态交互能力的同时将复杂度降至$O(N)$，从而支持更高分辨率的输入。同时，可以设计基于噪声感知门控的双向交叉注意力机制，在SAR→OPT和OPT→SAR两个方向上分别设置自适应的信息传递门控，根据各空间位置的噪声水平动态调节信息流向，在充分利用双向互补性的同时抑制噪声的跨模态传播。
（3）PMC参数域调制机制的理论深化与扩展。本文提出的PMC模块基于对标准卷积中心权重主导效应的经验性观察，通过引入可学习的抑制性先验来调制卷积核权重。虽然消融实验验证了PMC在局部细节增强方面的显著贡献，但目前对中心主导效应的形成机理和PMC的调制行为缺乏严格的理论分析。例如，抑制强度参数$\theta$在训练过程中的收敛行为、最优抑制程度与输入特征分布之间的关系等问题尚未得到深入探讨。未来的研究可以从优化理论的角度，分析标准卷积在梯度下降过程中中心权重的累积更新规律，为PMC的抑制策略提供理论支撑。此外，PMC目前仅对卷积核的中心位置施加抑制，未来可以探索更灵活的参数域调制模式，如基于输入内容的动态调制、对非中心位置的差异化增强等，进一步释放参数域调制的潜力。
（4）向更多模态和更广泛应用场景的扩展。本文第四章仅探索了光学与SAR两种模态的融合，但实际遥感应用中还涉及高光谱、LiDAR点云、热红外等更多类型的传感器数据。这些模态在数据维度（二维影像 vs. 三维点云）、时空分辨率和物理量纲上存在更大的异构性，当前CrossModalMSK中基于序列化注意力的融合方式难以直接适配。未来可以探索模态无关的统一特征表示学习框架，通过设计通用的模态编码器将不同类型的传感器数据映射至共享的语义特征空间，在该空间中实现统一的跨模态交互与融合。此外，本文的实验主要集中在城市和农业场景，未来有必要在极地冰盖监测、海洋目标检测、灾后损毁评估等更具挑战性的应用场景中验证方法的泛化能力，推动遥感语义分割技术向更广泛的实际应用落地。

综上所述，本文虽然在遥感图像语义分割的架构设计和多模态融合方面取得了一定进展，但在模型泛化性、计算效率、理论深度和应用广度等方面仍存在提升空间。随着状态空间模型、多模态大模型、自监督预训练等前沿技术的不断成熟，遥感图像智能解译领域有望在分割精度、计算效率和实际部署等方面取得更大的突破，为城市治理、生态保护和灾害应急等关键应用场景提供更加智能和可靠的技术支撑。
