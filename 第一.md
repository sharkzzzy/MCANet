摘要： 遥感图像语义分割是遥感领域的核心技术之一，在城市规划、环境监测、灾害评估等领域具有重要的应用价值。随着遥感传感器分辨率的不断提高，高分辨率遥感图像中包含的地物类型日益多样、尺度差异显著、背景环境愈加复杂，对语义分割算法的特征提取能力和计算效率提出了更高的要求。近年来，状态空间模型（State Space Model, SSM）凭借其线性计算复杂度和全局建模能力，为遥感图像分割提供了新的技术路径。然而，现有基于SSM的分割方法仍然存在全局建模与局部细节增强相互耦合、注意力机制冗余堆叠导致计算开销增大等问题。同时，单一模态数据往往难以全面表征复杂地物的特征信息，光学图像与合成孔径雷达（SAR）图像在成像机理上的本质差异也给跨模态特征融合带来了显著挑战。

为此，本文围绕遥感图像语义分割中的上述问题开展了以下研究：

（1）针对现有基于SSM的分割方法中全局上下文建模与局部细节增强在单一路径中耦合处理、注意力机制冗余堆叠等问题，本文提出了基于双路径解耦的语义分割网络DP-UNet。该网络在解码器中设计了双路径解耦VSS模块（DVSS），采用"共享基座、分别增强"的策略：以二维选择性扫描模块（SS2D）作为共享基座生成基础特征，全局路径直接保留该基础特征以维持完整的上下文语义信息，局部路径则在此基础上通过高效通道注意力（ECA）进行通道重标定，并利用参数域可调卷积（PMC）在参数空间引入可学习的中心抑制先验以增强对边缘与纹理细节的感知能力，最终通过自适应路径融合门控（APFG）实现两条路径特征的自适应整合。在多尺度特征融合阶段，设计了轻量化多尺度空间核模块（MSK），在压缩通道空间内通过多尺度卷积提取空间模式并以纯空间注意力进行特征增强，有效避免了通道注意力在网络中的冗余施加。实验结果表明，DP-UNet在ISPRS Potsdam数据集上的平均交并比达到86.71%、总体精度达到91.57%，在ISPRS Vaihingen数据集上的平均交并比达到83.84%、总体精度达到91.43%，在LoveDA数据集上的平均交并比达到53.21%，模型参数量为11.30M，计算量为44.26G FLOPs，能够有效解决全局与局部特征耦合及注意力冗余问题，在分割精度与计算效率之间取得了良好的平衡。

（2）针对单一模态遥感数据特征表征能力有限、光学图像与SAR图像模态差异显著导致特征融合不充分等问题，本文在DP-UNet的基础上提出了基于跨模态融合的多模态语义分割网络。该网络设计了双分支编码器分别提取光学图像和SAR图像的模态专属特征，并提出跨模态多尺度融合模块（CrossModalMSK），通过模态内多尺度空间特征提取捕获各模态不同尺度的空间信息，通过模态间交叉注意力机制建模两种模态特征之间的关联性并提取互补信息，实现不同模态、不同尺度特征的有效对齐与协同融合。解码器端复用DVSS模块对融合后的多模态特征进行渐进式上采样与精细化重建。实验结果表明，该网络在WHU-OPT-SAR数据集上的[占位：平均交并比达到XX%、总体精度达到XX%]，能够有效融合光学与SAR图像的互补信息，显著提升了复杂场景下的地物分割性能。

关键词： 遥感图像语义分割；状态空间模型；双路径解耦；参数域可调卷积；多模态融合；跨模态注意力

第一章 绪论
1.1 研究背景与意义
随着遥感技术的飞速发展，尤其是卫星遥感和无人机遥感平台的日益普及，遥感图像的空间分辨率不断提高，所获取的图像数据量也在急剧增长[1]。遥感图像能够为地理环境研究提供广阔的观测视角，已被广泛应用于土地覆盖分类[2-3]、环境监测[4]、灾害评估[5]、城市规划[6]和精准农业[7]等领域，为国民经济建设和社会发展提供了重要的数据支撑。因此，如何从海量遥感图像中高效、准确地提取地物信息，已成为遥感领域亟待解决的关键问题。

语义分割是计算机视觉领域中的一项核心技术，旨在对图像中的每一个像素进行类别标注，在遥感图像分析中扮演着至关重要的角色[8]。该技术能够实现对建筑物、道路、植被、水体等不同地物类型的自动识别与精细划分，为上述应用场景提供高效的决策支持。近年来，基于深度学习的语义分割方法取得了显著进展，从早期的全卷积网络（Fully Convolutional Network, FCN）[9]到编码器-解码器架构的U-Net[10]，再到融合多尺度上下文信息的DeepLabV3+[11]，分割精度和效率均得到了大幅提升。

然而，高分辨率遥感图像的语义分割仍然面临诸多挑战。首先，遥感图像中地物类型多样、尺度差异显著，同一场景中既包含大面积的农田、水体等宏观地物，也存在车辆、小型建筑等细小目标，要求模型同时具备全局上下文建模能力和局部细节捕获能力。其次，遥感图像背景复杂，地物边界模糊，不同类别之间存在"同物异谱"和"异物同谱"现象，进一步加大了精确分割的难度。此外，高分辨率图像的数据量庞大，对算法的计算效率提出了更高的要求，如何在精度与效率之间取得平衡是一个重要的研究课题。

为应对上述挑战，研究者们不断探索更强大的模型架构。基于Transformer[12]的方法通过自注意力机制有效捕获了长距离依赖关系，在语义分割任务中展现出优越的性能。然而，自注意力机制的计算复杂度与输入序列长度呈二次方关系，当应用于高分辨率遥感图像时，巨大的计算开销严重制约了其实际部署。近年来，状态空间模型（State Space Model, SSM）[13]凭借其线性计算复杂度和全局序列建模能力，为解决这一矛盾提供了新的技术路径。以Mamba[14]为代表的选择性状态空间模型通过输入依赖的选择机制，在保持线性复杂度的同时实现了对长序列的高效建模，已在自然语言处理和计算机视觉等领域展现出巨大潜力。在遥感图像分割中，基于Mamba的方法[15]已初步验证了其在处理大规模场景时兼顾精度与效率的可行性。然而，现有方法仍存在全局上下文建模与局部细节增强在单一处理路径中相互耦合、注意力机制在网络不同阶段冗余堆叠等问题，限制了模型性能的进一步提升。

此外，在实际遥感应用中，单一模态的数据往往难以全面表征复杂地物的特征信息。光学图像虽然具有丰富的光谱和纹理信息，能够直观反映地表覆盖物的属性和类型，但易受云层遮挡、光照变化等环境因素的影响，在恶劣天气条件下成像质量显著下降。合成孔径雷达（Synthetic Aperture Radar, SAR）作为一种主动微波遥感技术，具备全天候、全天时的成像能力，能够穿透云层和部分地表覆盖物，提供稳定可靠的地表观测信息[16]。然而，SAR图像受相干斑噪声干扰严重，且缺乏光谱信息，对地物类型的区分能力有限。通过融合光学图像与SAR图像，可以充分发挥两种模态的互补优势，弥补单一数据源的不足，从而全面、准确地获取地表特征信息。然而，光学图像与SAR图像在成像机理、数据特性和信息表达形式上存在本质差异，如何有效弥合这种模态鸿沟、实现跨模态特征的充分融合，仍然是一个具有挑战性的研究问题。

综上所述，针对遥感图像语义分割中存在的全局与局部特征建模耦合、计算效率受限以及多模态特征融合困难等问题，开展基于状态空间模型的高效分割方法研究，并探索光学与SAR图像的跨模态融合策略，对于推动遥感图像智能解译技术的发展具有重要的理论意义和应用价值。

1.2 国内外研究现状
遥感图像语义分割是遥感与计算机视觉交叉领域的重要研究方向。近年来，随着深度学习技术的快速发展，该领域的研究方法经历了从传统方法到深度学习方法、从卷积神经网络到Transformer和状态空间模型、从单模态到多模态的演进过程。这一演进的核心驱动力在于对两个基本矛盾的持续求解：一是全局上下文建模能力与计算效率之间的矛盾，二是单一模态信息不足与多模态融合困难之间的矛盾。本节将从基于卷积神经网络的方法、基于Transformer的方法、基于状态空间模型的方法以及多模态融合方法四个方面，对国内外研究现状进行综述。

1.2.1 基于卷积神经网络的语义分割方法
卷积神经网络（Convolutional Neural Network, CNN）是深度学习在语义分割领域最早也是最广泛应用的基础架构。语义分割任务本质上要求模型同时完成两个目标：对图像内容进行高层语义理解以确定"是什么"，以及保留精确的空间位置信息以确定"在哪里"。这两个目标之间的内在张力构成了语义分割方法演进的核心线索。

2015年，Shelhamer等人[9]提出的全卷积网络（FCN）首次将分类网络中的全连接层替换为卷积层，实现了端到端的像素级预测，奠定了深度学习语义分割的基础范式。FCN的核心贡献在于证明了分类网络中学习到的高层语义特征可以通过全卷积化直接转化为密集预测能力。然而，FCN的根本局限也随之暴露：分类网络中连续的池化操作虽然有效地扩大了感受野并提取了抽象语义特征，但同时导致特征图分辨率大幅降低，分割结果的边界较为粗糙，空间细节信息损失严重。简单的双线性插值上采样难以恢复已丢失的空间精度，这本质上反映了语义抽象与空间精度之间的矛盾。

为系统性地解决这一矛盾，Ronneberger等人[10]提出了U-Net架构。U-Net的核心创新在于其对称的编码器-解码器结构以及跳跃连接机制。编码器通过逐层下采样提取从低级纹理到高级语义的层次化特征，解码器则通过逐层上采样逐步恢复空间分辨率。关键的跳跃连接将编码器各层的高分辨率特征直接传递至对应的解码器层，使得解码器在恢复空间细节时能够利用编码器保留的精确位置信息，从而有效融合了深层语义信息与浅层空间细节。这种设计巧妙地将"语义理解"和"空间定位"分别交由编码器和解码器处理，再通过跳跃连接实现两者的协同，在医学图像分割中取得了突破性成果，并迅速被广泛应用于遥感图像分割领域，成为后续众多分割网络的基础架构。在此基础上，Zhou等人[17]提出了UNet++，通过在编码器与解码器之间引入密集的嵌套跳跃连接，构建了多条不同深度的特征传递路径。这种设计的深层动机在于：U-Net中单一层级的跳跃连接存在语义鸿沟问题——编码器浅层特征富含空间细节但语义信息薄弱，直接与解码器深层的高语义特征拼接可能导致融合效果不佳。UNet++通过密集连接使得不同语义层级的特征能够逐步过渡和融合，缓解了这一问题。

在特征提取骨干网络方面，He等人[18]提出了深度残差网络（ResNet），通过引入残差学习框架有效解决了深层网络训练中的梯度消失问题。ResNet的核心思想是在卷积层之间建立恒等映射的捷径连接，使网络仅需学习输入与输出之间的残差信息，极大地简化了优化过程，使得网络深度可以大幅增加而不会出现性能退化。由于其优异的特征提取能力和训练稳定性，ResNet系列（尤其是ResNet-18和ResNet-50）被广泛用作语义分割模型的编码器骨干，为后续的特征解码和像素级预测提供了高质量的多层级特征表示。

尽管U-Net及其变体有效缓解了空间信息丢失的问题，但编码器中卷积操作的感受野仍然受到卷积核大小的固有限制。即使通过多层堆叠，每个卷积层仅能感知其局部邻域内的信息，对于遥感图像中大面积均质区域（如水体、农田）的一致性分割以及远距离空间关系（如道路的连通性）的捕获，仍然力有不逮。为在不增加参数量的前提下有效扩大感受野，Chen等人[19]提出了DeepLab系列方法。其中，DeepLabV2引入了空洞空间金字塔池化（Atrous Spatial Pyramid Pooling, ASPP）模块，其核心思想是在标准卷积核的采样位置之间插入空洞（即固定间隔），使得卷积核在不增加参数数量的情况下覆盖更大的空间范围。通过并行应用多个不同膨胀率的空洞卷积，ASPP能够同时捕获不同尺度的上下文信息。DeepLabV3+[11]在此基础上进一步结合了编码器-解码器结构，利用解码器逐步恢复空间细节，提升了分割边界的精细度。然而，空洞卷积虽然扩大了理论感受野，但其采样模式仍然是规则的网格状分布，对于不规则形状的地物目标和复杂的空间关系，建模能力仍然有限。

与ASPP的并行多尺度策略不同，Zhao等人[20]提出的金字塔场景解析网络（PSPNet）采用了全局池化的思路来获取多尺度上下文信息。PSPNet通过金字塔池化模块将特征图划分为不同大小的区域并分别进行池化操作，从1×1到6×6的多种尺度，从而聚合从全局到局部的上下文信息。与ASPP相比，PSPNet的全局池化操作能够直接获取整幅图像的全局信息，有效缓解了对大尺度物体和复杂背景的误判问题。然而，池化操作本身会丢弃空间位置信息，且多尺度特征的简单拼接方式可能引入冗余信息。

在模型轻量化方面，随着遥感图像实时处理和边缘部署需求的增长，如何在保持分割精度的同时降低模型的计算开销成为另一重要研究方向。Badrinarayanan等人[21]提出了SegNet，通过利用编码阶段的最大池化索引进行非参数化上采样，避免了转置卷积中大量可学习参数的引入，显著减少了模型参数量。Yu等人[22]提出了双边分割网络（BiSeNet），其设计思想尤为值得关注：它明确地将语义理解和空间细节保持这两个目标分配给两个独立的分支——空间路径通过少量下采样保留高分辨率的空间细节，上下文路径通过快速下采样获取全局语义信息——最后通过特征融合模块将两者结合。这种"将不同任务分配给专用路径"的设计理念，为后续的多路径分割架构提供了重要启发。Howard等人[23]提出的MobileNetV3通过引入深度可分离卷积和反转残差结构，大幅降低了骨干网络的计算开销，为轻量化语义分割模型提供了高效的特征提取基础。

在遥感图像分割领域，基于CNN的方法同样取得了丰富成果，并针对遥感图像的特殊性进行了针对性优化。Li等人[24]提出的ABCNet通过注意力增强的双边网络结构，在空间细节分支和上下文信息分支中分别引入注意力机制，增强了对遥感图像中复杂场景的特征选择能力，实现了精细分割。Wang等人[25]提出了UNetFormer，以ResNet作为编码器骨干，并在解码器中融合了Transformer模块的全局-局部注意力机制。UNetFormer的设计反映了一个重要趋势：研究者开始认识到CNN在局部特征提取方面的高效性和Transformer在全局建模方面的优越性可以互补，而非互相替代。这一思想为后续CNN与其他全局建模技术（如状态空间模型）的融合提供了重要参考。

综上所述，基于CNN的语义分割方法经历了从FCN的全卷积化开创、U-Net的编码器-解码器结构确立、DeepLab系列的多尺度感受野扩展到轻量化模型的效率优化等发展阶段。然而，无论是空洞卷积还是池化操作，本质上都是通过各种策略来弥补卷积操作局部感受野的固有限制，难以真正实现全局范围内任意位置之间的直接信息交互。这一根本性限制促使研究者将目光转向具备全局建模能力的Transformer架构。

1.2.2 基于Transformer的语义分割方法
卷积神经网络通过局部感受野的层层堆叠来间接捕获全局信息的方式，在效率和效果上均存在瓶颈。Transformer[12]的出现提供了一种全新的思路：通过自注意力机制，序列中任意两个位置之间可以建立直接的依赖关系，从根本上克服了CNN局部感受野的限制。

Dosovitskiy等人[26]首次将Transformer应用于计算机视觉领域，提出了Vision Transformer（ViT）。ViT将图像划分为固定大小的图像块（patch），将每个图像块展平为一维向量后作为序列输入Transformer编码器，通过多头自注意力机制建模图像块之间的全局关系。ViT在大规模图像分类任务中取得了优异的性能，证明了Transformer架构在视觉任务中的可行性。然而，ViT的设计也暴露了Transformer应用于视觉任务的核心挑战：自注意力的计算复杂度与序列长度呈二次方关系（
O
(
N
2
)
O(N 
2
 )，其中
N
N为图像块数量）。对于遥感图像分割而言，高分辨率图像需要密集的像素级预测，图像块数量远大于分类任务，导致计算开销急剧增大。此外，ViT的固定单尺度图像块划分方式无法生成类似CNN的多尺度层次化特征，而多尺度特征对于处理遥感图像中尺度差异显著的地物目标至关重要。

针对上述问题，研究者从两个方向进行了改进。第一个方向是构建层次化的Transformer结构以生成多尺度特征。Wang等人[27]提出了PVT（Pyramid Vision Transformer），设计了具有渐进缩减空间分辨率的金字塔结构，在每个阶段逐步降低特征图分辨率的同时增加通道维度，生成了类似CNN的多尺度特征金字塔。PVT还引入了空间缩减注意力机制，通过在计算注意力前对键（Key）和值（Value）进行空间降采样来降低计算复杂度，为后续密集预测任务中的Transformer骨干设计提供了重要参考。第二个方向是通过局部化策略降低自注意力的计算复杂度。Liu等人[28]提出了Swin Transformer，通过滑动窗口机制将自注意力的计算限制在固定大小的局部窗口内，将计算复杂度从
O
(
N
2
)
O(N 
2
 )降低至
O
(
N
)
O(N)。同时，通过相邻层之间窗口的规律性移位实现跨窗口信息交互，在不显著增加计算量的前提下维持了全局建模能力。Swin Transformer的层次化设计与窗口注意力机制使其成为密集预测任务中最具影响力的Transformer骨干之一。

在语义分割领域，Zheng等人[29]提出了SETR（Segmentation Transformer），将ViT作为编码器，首次验证了纯Transformer架构在语义分割任务中的可行性。然而，SETR直接使用ViT的单尺度特征进行解码，缺乏多尺度信息的融合，在处理多尺度地物时效果受限。Xie等人[30]提出了SegFormer，设计了层次化的Mix-Transformer编码器和轻量级的全多层感知机（All-MLP）解码器。SegFormer的编码器通过重叠的图像块嵌入保留了相邻块之间的局部连续性，同时利用高效的自注意力机制进行全局建模。其轻量级的MLP解码器摒弃了复杂的注意力模块，仅通过多层感知机融合不同层级的特征，在多个分割基准上取得了精度与效率的良好平衡，证明了编码器中充分的特征提取可以简化解码器的设计。Cao等人[31]提出了Swin-Unet，首次构建了基于纯Swin Transformer的U形编码器-解码器架构，将U-Net中的CNN模块完全替换为Swin Transformer块，通过块合并和块扩展操作实现特征的下采样和上采样，在医学图像分割任务中验证了Transformer架构可以端到端地替代CNN完成密集预测任务。

在遥感图像分割中，基于Transformer的方法也得到了广泛应用。Wang等人[32]提出了BANet，采用Transformer作为骨干网络提取多层级特征，并设计了双边感知模块将高层语义信息与低层空间细节进行融合，其核心思想是通过变换注意力机制动态地建模不同层级特征之间的关联关系，从而实现更精细的语义-空间信息交互，有效提升了对遥感图像中复杂场景的理解能力。Strudel等人[33]提出了Segmenter，采用纯Transformer架构进行语义分割，在编码器和解码器中均使用Transformer块，通过全局注意力机制增强了对大范围地物的识别能力。然而，纯Transformer架构的计算开销较大，且缺乏CNN在局部特征提取方面的归纳偏置优势，在处理遥感图像中精细的地物边界和小目标时，其性能相比CNN-Transformer混合架构并不总是占优。

然而，尽管Transformer在建模长距离依赖方面具有显著优势，其固有的计算效率问题始终是制约实际应用的核心瓶颈。即使采用Swin Transformer的局部窗口策略，当应用于高分辨率遥感图像（如6000×6000像素）时，窗口数量仍然巨大，计算和内存开销依然可观。此外，局部窗口策略虽然降低了计算复杂度，但也在一定程度上牺牲了Transformer最核心的全局直接交互优势——窗口内的注意力计算实质上退化为一种特殊的局部操作，跨窗口信息仅通过窗口移位间接传递。这一效率与全局建模能力之间的根本矛盾，促使研究者开始探索Transformer之外的全局建模范式，其中状态空间模型凭借其线性复杂度的序列建模能力成为最具前景的替代方案。

1.2.3 基于状态空间模型的语义分割方法
状态空间模型（State Space Model, SSM）源于控制理论和信号处理领域，其基本思想是通过一组隐状态的线性递推来描述输入序列到输出序列的映射关系[13]。具体而言，SSM将输入信号通过状态方程映射为隐状态序列，再通过观测方程将隐状态转化为输出。与Transformer的自注意力机制通过全局配对计算来建模序列依赖关系不同，SSM通过隐状态的逐步递推积累全局信息，其计算复杂度与序列长度呈线性关系（
O
(
N
)
O(N)），在处理长序列时具有显著的效率优势。

然而，早期的SSM存在一个关键限制：其状态转移矩阵是固定的，无法根据输入内容自适应地调整信息的保留和遗忘策略，导致模型对不同输入的区分能力不足。Gu等人[34]提出的结构化状态空间序列模型（S4）通过对状态转移矩阵进行HiPPO初始化和对角化参数设计，首次在长距离序列建模任务中取得了突破性成果，证明了SSM在捕获长距离依赖关系方面的潜力。但S4的参数仍然是输入无关的，即对所有输入使用相同的状态转移规则，这限制了其对输入内容的选择性处理能力。

在此基础上，Gu和Dao[14]提出了Mamba模型，实现了SSM从线性时不变系统到输入依赖系统的关键跨越。Mamba的核心创新在于将状态转移矩阵和输入矩阵参数化为输入的函数，使得模型能够根据当前输入内容自适应地决定哪些信息应当被保留到隐状态中、哪些信息应当被遗忘。这种选择性状态空间机制（Selective State Space）赋予了模型类似于注意力机制的内容感知能力，但其计算方式仍然是线性递推，保持了
O
(
N
)
O(N)的计算复杂度。同时，Mamba设计了硬件感知的并行扫描算法，通过将递推计算重构为并行前缀和运算，充分利用GPU的并行计算能力，在保持理论线性复杂度的同时实现了实际运行速度的大幅提升。

Mamba在自然语言处理任务中展现出了与Transformer相当甚至超越的性能后，迅速被引入计算机视觉领域。然而，将SSM从一维序列扩展到二维图像面临一个根本性挑战：图像数据具有天然的二维空间结构，而SSM本质上是一维序列模型，如何将二维空间信息有效地编码为一维序列并保持空间关系是关键问题。针对这一挑战，Liu等人[35]提出了VMamba，设计了二维选择性扫描模块（SS2D）。SS2D的核心思想是沿四个方向（从左上到右下、从右下到左上、从左下到右上、从右上到左下）分别将二维特征图展开为一维序列，对每个方向独立进行选择性SSM扫描，最后将四个方向的输出进行融合。这种交叉扫描策略确保了特征图上任意两个位置之间至少存在一条扫描路径使其信息可达，从而在保持线性复杂度的同时实现了对二维空间全局依赖关系的有效建模。Zhu等人[36]提出了Vision Mamba（Vim），采用双向状态空间模型处理图像序列，通过正向和反向两条扫描路径捕获全局视觉上下文，在图像分类任务中实现了与ViT相当的性能但具有更低的计算开销。

在语义分割领域，基于Mamba的方法针对密集预测任务的特殊需求进行了针对性设计。Xing等人[37]提出了SegMamba，将Mamba引入三维医学图像分割任务。三维体数据的序列长度远大于二维图像，传统Transformer方法在此场景下面临更为严峻的计算瓶颈，而Mamba的线性复杂度使其能够高效处理长达数百万体素的三维序列，有效建模了体数据中的长距离空间依赖关系。Ruan和Xiang[38]提出了VM-UNet，将Vision Mamba模块嵌入U-Net的编码器-解码器架构中，构建了纯SSM的语义分割网络。VM-UNet的实验结果表明，SSM不仅可以作为Transformer的替代方案用于特征编码，还能够在解码器中有效地进行特征恢复和精细化，验证了SSM作为编码器-解码器骨干的全面可行性。Ma等人[39]提出了U-Mamba，将Mamba模块与CNN模块在编码器中进行混合设计，利用CNN提取局部特征、Mamba捕获全局依赖，两者的协同使模型在局部细节和全局语义两方面均具有良好表现，为CNN与SSM的融合提供了有效范式。

在遥感图像分割领域，基于Mamba的方法同样受到了广泛关注，并针对遥感图像的特殊需求进行了适配。Chen等人[40]提出了RS3Mamba，将Mamba与CNN相结合用于遥感图像语义分割。RS3Mamba设计了辅助的选择性状态空间模块，通过与CNN分支的协同工作增强了多尺度特征的提取能力，在多个遥感分割基准上展现了SSM在遥感领域的应用潜力。Shi等人[15]提出了CM-UNet，构建了一个将基于CNN的ResNet编码器与基于Mamba的解码器相结合的混合架构。CM-UNet在解码器中设计了CSMamba模块，该模块在SS2D模块的基础上引入了通道注意力和空间注意力门控机制，旨在增强SS2D输出特征的判别性。同时，CM-UNet提出了多尺度注意力聚合（MSAA）模块用于跳跃连接处的多尺度特征融合，该模块同样采用了通道-空间双重注意力来增强融合特征的表征能力。CM-UNet在ISPRS Potsdam、Vaihingen和LoveDA等多个遥感分割数据集上取得了优异的性能，验证了CNN编码器+Mamba解码器这一混合架构的有效性。

尽管上述方法取得了显著进展，但深入分析现有基于Mamba的遥感分割方法，仍可发现两方面值得关注的问题。

第一，全局与局部的耦合处理问题。现有方法的解码器通常采用单一路径的序列化设计，在同一处理流程中先后完成全局上下文建模和局部细节增强两种任务。然而，这两种任务具有本质不同的优化目标：全局上下文建模需要在较大的空间范围内整合语义信息以确保类别预测的一致性，而局部细节增强则需要在精细的空间尺度上保留边缘、纹理等高频信息。将它们耦合在同一路径中意味着两种任务共享相同的参数空间和计算流程，在优化过程中可能相互制约——加强全局建模的参数更新可能损害局部细节的保持，反之亦然。这种耦合限制了模型在两个方面同时达到最优的潜力。

第二，注意力机制的冗余施加问题。部分方法在网络的多个阶段重复使用功能相似的注意力操作。例如，在特征变换阶段已经应用了通道和空间注意力来增强特征选择能力，而在跳跃连接的特征融合阶段又再次施加了类似的通道-空间双重注意力。这种同质注意力的重复堆叠虽然在一定程度上增强了特征聚焦能力，但也带来了显著的参数和计算冗余，且重复的注意力操作在梯度传播过程中可能产生优化干扰，并不能保证性能的等比例提升。

如何将全局建模与局部增强解耦为独立的专用路径使各自获得充分的优化空间，以及如何在网络的不同阶段合理分配注意力机制的功能以避免冗余，是当前基于SSM的遥感分割方法需要进一步解决的关键问题。

1.2.4 多模态遥感图像融合分割方法
上述单模态语义分割方法的进展主要集中在模型架构的改进上，但无论采用何种架构，其输入数据均为单一模态的遥感图像。在实际遥感应用中，单一模态数据的信息表征能力往往受限于其自身的成像机理。光学图像能够提供丰富的光谱和纹理信息，但在云层遮挡、光照不足等条件下成像质量严重退化；SAR图像具备全天候、全天时成像能力，但受相干斑噪声干扰且缺乏光谱信息。多模态数据融合通过整合不同传感器的互补信息，可以从根本上弥补单一模态的不足，已成为提升语义分割性能的重要手段[41]。

根据融合阶段的不同，多模态融合方法通常可分为早期融合、中期融合和晚期融合三种策略[42]。早期融合在输入层直接拼接不同模态的数据，方法最为简单直接，但由于不同模态数据在数值范围、统计分布和语义表达上存在显著差异，直接拼接可能导致模态间的特征相互干扰，反而降低分割性能。晚期融合在决策层对各模态独立预测的结果进行集成（如投票或加权平均），能够避免模态间特征的直接干扰，但由于各模态的特征提取过程完全独立，缺乏特征层面的交互，无法充分利用模态间的互补信息。中期融合在特征提取的中间阶段对不同模态的特征进行交互与整合，能够在保持模态特异性特征提取的同时实现深层次的特征互补，是目前研究最为广泛且效果最为显著的融合策略。

在中期融合的具体实现方式上，早期工作主要采用简单的特征拼接或逐元素操作。Hazirbas等人[43]提出了FuseNet，采用双分支编码器分别处理RGB图像和深度图像，并将深度分支各层的特征通过逐元素相加的方式融合至RGB分支的编码器中。FuseNet的设计思想在于让深度信息作为辅助信息逐级增强RGB特征的表达能力。然而，这种单向融合方式存在两个局限：一是深度分支向RGB分支的单向信息传递未能实现双向的模态交互，RGB分支的信息无法反过来指导深度特征的提取；二是简单的逐元素相加难以建模两种模态特征之间的复杂非线性关系，融合深度有限。Audebert等人[44]同样采用双分支CNN网络分别处理光学图像和数字表面模型（DSM）图像，并通过逐元素相加进行特征融合，面临类似的局限。

为解决简单融合策略的不足，研究者们引入了注意力机制来实现更精细的模态特征交互。Ma等人[45]提出了AMM-FuseNet，采用通道注意力机制动态评估不同模态各通道特征的重要性，并结合密集连接的空间金字塔池化增强多尺度特征的表征能力，在一定程度上缓解了简单融合策略中模态信息利用不充分的问题。然而，通道注意力仅在通道维度上对特征进行重标定，无法在空间维度上精细地对齐不同模态的特征响应，对于存在空间错位的多模态数据融合效果有限。

在光学与SAR图像的融合分割方面，由于两种模态在成像机理上的本质差异，跨模态特征融合面临更大的挑战。光学图像反映地物的光谱反射特性，其特征表达主要体现在颜色、纹理和光谱梯度等方面；而SAR图像通过后向散射系数表征地表结构特征，其特征表达主要体现在散射强度、极化特性和空间纹理等方面。这两种截然不同的特征表达方式使得简单的特征级融合难以有效地建模模态间的语义对应关系。此外，SAR图像固有的相干斑噪声会在融合过程中传播至光学特征空间，可能干扰光学特征的判别性。

针对光学与SAR图像融合的特殊挑战，Li等人[46]提出了MCANet并构建了WHU-OPT-SAR数据集。MCANet设计了多模态交叉注意力模块，其核心思想是以一种模态的特征作为查询（Query），以另一种模态的特征作为键（Key）和值（Value），通过交叉注意力计算来提取两种模态之间的互补信息。这种交叉注意力机制相比简单的特征拼接或相加，能够更精细地建模模态间的语义关联，动态地从对方模态中选取最相关的信息进行融合。同时，MCANet还设计了低高层特征融合模块，将浅层的空间细节与深层的语义信息相结合以优化分割结果。实验结果验证了交叉注意力机制在光学-SAR融合任务中的显著优势。Feng等人[47]提出了CMGFNet，进一步引入了门控融合机制来解决模态间信息质量不均衡的问题。门控机制通过学习一组空间自适应的融合权重，使模型能够根据不同空间位置的特征质量动态调节光学和SAR特征的贡献比例：在光学特征质量较高的区域（如无云覆盖区域）赋予光学特征更大的权重，在SAR特征更可靠的区域（如云层覆盖区域）则加大SAR特征的权重，有效应对了模态间语义不一致和信息质量不均衡的问题。Zhang等人[48]提出了CMX，设计了交叉模态特征校正模块，通过双向特征交互实现不同模态之间的有效融合。CMX的设计思想具有较好的通用性，其跨模态融合范式不局限于特定的模态组合，对光学与SAR图像的融合也具有重要的借鉴意义。

综上所述，多模态遥感图像融合分割方法从早期的简单特征拼接发展到基于注意力机制的精细交互融合，在融合深度和效果上取得了显著进步。然而，现有方法仍存在以下需要进一步解决的问题：首先，多数方法在融合过程中仅关注单一尺度的特征交互，而光学与SAR图像在不同尺度上的特征互补性存在差异——在低层级，两种模态的纹理和边缘信息差异显著，需要精细的空间对齐；在高层级，两种模态的语义信息趋于一致，可以进行更直接的语义融合——缺乏对多尺度跨模态交互的系统性考虑。其次，现有方法的解码器设计大多沿用标准的CNN或Transformer架构，未能充分利用SSM等新型全局建模技术在多模态融合特征重建中的潜力。如何在多模态融合框架中有效整合状态空间模型的高效全局建模能力，并针对多模态场景设计专用的多尺度跨模态融合策略，是一个值得深入探索的研究方向。

1.3 主要研究内容
针对上述研究现状中存在的问题，本文围绕基于状态空间模型的遥感图像语义分割方法展开研究，致力于解决现有方法中全局与局部特征建模耦合、注意力机制冗余以及多模态特征融合不充分等问题。具体研究内容如下：

（1）针对现有基于SSM的遥感分割方法中全局上下文建模与局部细节增强在单一路径中耦合处理、注意力机制在不同网络阶段冗余堆叠等问题，本文提出了基于双路径解耦的语义分割网络DP-UNet。全局上下文建模与局部细节增强是语义分割中两个本质不同的子任务：前者需要在较大空间范围内整合语义信息以保证类别预测的区域一致性，后者则需要在精细空间尺度上保留高频细节以确保分割边界的准确性。现有方法将两者置于同一处理路径中顺序执行，导致两种任务的参数优化相互制约。同时，功能相似的注意力机制在特征变换和特征融合等多个阶段被重复施加，带来了不必要的计算冗余。为此，本文在解码器中设计了双路径解耦VSS模块（DVSS），采用"共享基座、分别增强"的策略。该设计的核心思想是：全局建模和局部增强虽然是不同的任务，但它们共享相同的底层特征基础。因此，DVSS首先以SS2D模块作为共享基座生成统一的基础特征表示，随后将其分发至两条专用路径——全局路径直接保留该基础特征以维持完整的上下文语义信息，局部路径则在此基础上通过高效通道注意力（ECA）进行通道重标定，聚焦于信息量丰富的特征通道，并利用所设计的参数域可调卷积（PMC）在卷积权重的参数空间中引入可学习的中心抑制先验，打破标准卷积的中心主导倾向，增强对边缘与纹理等局部细节的感知能力。最终通过自适应路径融合门控（APFG）根据各路径特征的信息量动态分配融合权重，实现两条路径特征的自适应整合。在多尺度特征融合阶段，考虑到DVSS的局部路径已承担了通道维度的特征重标定任务，本文设计了轻量化多尺度空间核模块（MSK），在压缩通道空间内以纯空间注意力完成多尺度特征融合，避免通道注意力在网络中的冗余施加，从而在提升分割精度的同时有效降低计算开销。

（2）针对单一模态遥感数据特征表征能力有限、光学与SAR图像模态差异显著导致特征融合不充分等问题，本文在DP-UNet的基础上进一步扩展至多模态融合场景，提出了基于跨模态融合的语义分割网络。单一模态遥感数据在面对复杂地表环境时存在信息不足的固有局限：光学图像易受云层遮挡和光照变化影响，在恶劣天气条件下地物信息严重缺失；SAR图像虽能全天候成像，但受相干斑噪声干扰且缺乏光谱信息，对地物类型的区分能力有限。两种模态在信息维度上具有天然的互补性——光学图像擅长表征地物的光谱属性和视觉纹理，SAR图像擅长反映地表的几何结构和散射特性。然而，两种模态在成像机理、特征分布和噪声特性上的巨大差异，使得简单的特征拼接或相加难以有效挖掘模态间的互补信息。为此，本文设计了双分支编码器分别提取光学图像和SAR图像的模态专属特征，避免异质模态特征在早期阶段的相互干扰。在此基础上，提出了跨模态多尺度融合模块（CrossModalMSK），该模块通过模态内多尺度空间特征提取分别捕获各模态在不同尺度上的空间模式，并通过模态间交叉注意力机制建模两种模态特征之间的语义关联性，从对方模态中动态提取最具互补价值的信息，实现不同模态、不同尺度特征之间的有效对齐与协同融合。解码器端复用第一个工作中已验证有效的DVSS模块，对融合后的多模态特征进行全局-局部解耦增强与渐进式精细化重建，将单模态工作中的核心设计思想自然地延伸至多模态场景。

（3）为验证所提方法的有效性，本文在ISPRS Potsdam、ISPRS Vaihingen、LoveDA和WHU-OPT-SAR四个公开遥感数据集上开展了系统性的实验评估。实验设计涵盖三个层面：首先，与多种具有代表性的现有方法进行全面的对比实验，从平均交并比、总体精度、各类别F1分数等多个指标维度验证所提方法的分割性能优势；其次，通过逐步添加和移除关键模块的消融实验，系统性地验证DVSS、PMC、MSK、CrossModalMSK等核心模块各自的贡献及其协同效应；最后，从浮点运算量、参数量和推理速度等方面进行模型复杂度分析，全面评估所提方法在精度与效率之间的平衡能力。

1.4 本文组织结构
本文聚焦于基于状态空间模型的遥感图像语义分割方法研究，全文共分为五章，各章主要内容安排如下：

第一章为绪论。首先介绍了遥感图像语义分割的研究背景与意义；其次分别从基于卷积神经网络的方法、基于Transformer的方法、基于状态空间模型的方法以及多模态融合方法四个方面综述了国内外研究现状，分析了各类方法的技术演进脉络、核心优势及存在的局限性；接着根据当前研究中存在的问题，确定了本文的主要研究内容；最后介绍了论文的组织结构。

第二章为相关理论与技术基础。首先介绍了语义分割任务的基本概念和编码器-解码器框架；其次阐述了状态空间模型的基本原理，包括从S4到Mamba的演进过程以及二维选择性扫描机制；然后介绍了本文涉及的注意力机制和多模态遥感数据的基本特性；接着介绍了本文实验所用的四个公开数据集和语义分割任务的评价指标；最后对本章内容进行小结。

第三章为基于双路径解耦的单模态遥感图像语义分割方法。首先分析了现有基于SSM的分割方法中存在的全局-局部耦合和注意力冗余问题；然后详细介绍了DP-UNet的整体架构以及DVSS模块、PMC模块和MSK模块的设计原理与实现细节；接着在ISPRS Potsdam、ISPRS Vaihingen和LoveDA三个数据集上进行对比实验、消融实验和模型复杂度分析；最后对本章内容进行小结。

第四章为基于跨模态融合的多模态遥感图像语义分割方法。首先分析了单模态分割方法的局限性以及光学与SAR图像跨模态融合的挑战；然后详细介绍了多模态融合网络的整体架构以及跨模态多尺度融合模块的设计原理；接着在WHU-OPT-SAR数据集上进行实验分析；最后对本章内容进行小结。

第五章为总结与展望。总结了本文的主要研究工作和创新点，分析了当前工作的局限性，并对未来的研究方向进行了展望。

参考文献
[1] Zhu X X, Tuia D, Mou L, et al. Deep learning in remote sensing: A comprehensive review and list of resources[J]. IEEE Geoscience and Remote Sensing Magazine, 2017, 5(4): 8-36.

[2] Kussul N, Lavreniuk M, Skakun S, et al. Deep learning classification of land cover and crop types using remote sensing data[J]. IEEE Geoscience and Remote Sensing Letters, 2017, 14(5): 778-782.

[3] Zhang C, Sargent I, Pan X, et al. An object-based convolutional neural network (OCNN) for urban land use classification[J]. Remote Sensing of Environment, 2018, 216: 57-70.

[4] Sublime J, Kalinicheva E. Automatic post-disaster damage mapping using deep-learning techniques for change detection: Case study of the Tohoku tsunami[J]. Remote Sensing, 2019, 11(9): 1123.

[5] Gupta R, Hosfelt R, Saber S, et al. xBD: A dataset for assessing building damage from satellite imagery[J]. arXiv preprint arXiv:1911.09296, 2019.

[6] Wurm M, Stark T, Zhu X X, et al. Semantic segmentation of slums in satellite images using transfer learning on fully convolutional neural networks[J]. ISPRS Journal of Photogrammetry and Remote Sensing, 2019, 150: 59-69.

[7] Kamilaris A, Prenafeta-Boldú F X. Deep learning in agriculture: A survey[J]. Computers and Electronics in Agriculture, 2018, 147: 70-90.

[8] Yuan X, Shi J, Gu L. A review of deep learning methods for semantic segmentation of remote sensing imagery[J]. Expert Systems with Applications, 2021, 169: 114417.

[9] Shelhamer E, Long J, Darrell T. Fully convolutional networks for semantic segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39(4): 640-651.

[10] Ronneberger O, Fischer P, Brox T. U-Net: Convolutional networks for biomedical image segmentation[C]//Proceedings of the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer, 2015: 234-241.

[11] Chen L C, Zhu Y, Papandreou G, et al. Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2018: 801-818.

[12] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in Neural Information Processing Systems (NeurIPS), 2017: 5998-6008.

[13] Gu A, Dao T, Ermon S, et al. HiPPO: Recurrent memory with optimal polynomial projections[C]//Advances in Neural Information Processing Systems (NeurIPS), 2020: 1474-1487.

[14] Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces[J]. arXiv preprint arXiv:2312.00752, 2023.

[15] Shi C, Chen Y, Wang G. CM-UNet: Hybrid CNN-Mamba UNet for remote sensing image semantic segmentation[J]. arXiv preprint arXiv:2405.10530, 2024.

[16] Moreira A, Prats-Iraola P, Younis M, et al. A tutorial on synthetic aperture radar[J]. IEEE Geoscience and Remote Sensing Magazine, 2013, 1(1): 6-43.

[17] Zhou Z, Siddiquee M M R, Tajbakhsh N, et al. UNet++: A nested U-Net architecture for medical image segmentation[C]//Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Springer, 2018: 3-11.

[18] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016: 770-778.

[19] Chen L C, Papandreou G, Kokkinos I, et al. DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018, 40(4): 834-848.

[20] Zhao H, Shi J, Qi X, et al. Pyramid scene parsing network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017: 2881-2890.

[21] Badrinarayanan V, Kendall A, Cipolla R. SegNet: A deep convolutional encoder-decoder architecture for image segmentation[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017, 39(12): 2481-2495.

[22] Yu C, Wang J, Peng C, et al. BiSeNet: Bilateral segmentation network for real-time semantic segmentation[C]//Proceedings of the European Conference on Computer Vision (ECCV). Springer, 2018: 325-341.

[23] Howard A, Sandler M, Chu G, et al. Searching for MobileNetV3[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019: 1314-1324.

[24] Li R, Zheng S, Zhang C, et al. ABCNet: Attentive bilateral contextual network for efficient semantic segmentation of fine-resolution remotely sensed imagery[J]. ISPRS Journal of Photogrammetry and Remote Sensing, 2021, 181: 84-98.

[25] Wang L, Li R, Zhang C, et al. UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery[J]. ISPRS Journal of Photogrammetry and Remote Sensing, 2022, 190: 196-214.

[26] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[C]//Proceedings of the International Conference on Learning Representations (ICLR), 2021.

[27] Wang W, Xie E, Li X, et al. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021: 568-578.

[28] Liu Z, Lin Y, Cao Y, et al. Swin Transformer: Hierarchical vision transformer using shifted windows[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021: 10012-10022.

[29] Zheng S, Lu J, Zhao H, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021: 6881-6890.

[30] Xie E, Wang W, Yu Z, et al. SegFormer: Simple and efficient design for semantic segmentation with transformers[C]//Advances in Neural Information Processing Systems (NeurIPS), 2021: 12077-12090.

[31] Cao H, Wang Y, Chen J, et al. Swin-Unet: Unet-like pure transformer for medical image segmentation[C]//Proceedings of the European Conference on Computer Vision (ECCV) Workshops. Springer, 2022: 205-218.

[32] Wang L, Li R, Duan C, et al. A novel transformer based semantic segmentation scheme for fine-resolution remote sensing images[J]. IEEE Geoscience and Remote Sensing Letters, 2022, 19: 1-5.

[33] Strudel R, Garcia R, Laptev I, et al. Segmenter: Transformer for semantic segmentation[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021: 7262-7272.

[34] Gu A, Goel K, Ré C. Efficiently modeling long sequences with structured state spaces[C]//Proceedings of the International Conference on Learning Representations (ICLR), 2022.

[35] Liu Y, Tian Y, Zhao Y, et al. VMamba: Visual state space model[J]. arXiv preprint arXiv:2401.10166, 2024.

[36] Zhu L, Liao B, Zhang Q, et al. Vision Mamba: Efficient visual representation learning with bidirectional state space model[J]. arXiv preprint arXiv:2401.09417, 2024.

[37] Xing Z, Ye T, Yang Y, et al. SegMamba: Long-range sequential modeling Mamba for 3D medical image segmentation[J]. arXiv preprint arXiv:2401.13560, 2024.

[38] Ruan J, Xiang S. VM-UNet: Vision Mamba UNet for medical image segmentation[J]. arXiv preprint arXiv:2402.02491, 2024.

[39] Ma J, Li F, Wang B. U-Mamba: Enhancing long-range dependency for biomedical image segmentation[J]. arXiv preprint arXiv:2401.04722, 2024.

[40] Chen T, Zhu L, Niu B, et al. RS3Mamba: Visual state space model for remote sensing image semantic segmentation[J]. IEEE Geoscience and Remote Sensing Letters, 2024.

[41] Gao L, Hong D, Yao J, et al. Spectral superresolution of multispectral imagery with joint sparse and low-rank learning[J]. IEEE Transactions on Geoscience and Remote Sensing, 2021, 59(3): 2269-2280.

[42] Zhang J. Multi-source remote sensing data fusion: Status and trends[J]. International Journal of Image and Data Fusion, 2010, 1(1): 5-24.

[43] Hazirbas C, Ma L, Domokos C, et al. FuseNet: Incorporating depth into semantic segmentation via fusion-based CNN architecture[C]//Proceedings of the Asian Conference on Computer Vision (ACCV). Springer, 2016: 213-228.

[44] Audebert N, Le Saux B, Lefèvre S. Beyond RGB: Very high resolution urban remote sensing with multimodal deep networks[J]. ISPRS Journal of Photogrammetry and Remote Sensing, 2018, 140: 20-32.

[45] Ma X, Mao Z, Li X, et al. AMM-FuseNet: Attention-based multi-modal image fusion network for land cover mapping[J]. Remote Sensing, 2022, 14(18): 4458.

[46] Li X, Zhang G, Cui H, et al. MCANet: A joint semantic segmentation framework of optical and SAR images for land use classification[J]. International Journal of Applied Earth Observation and Geoinformation, 2022, 106: 102638.





[47] Feng J, Wang L, Yu H, et al. CMGFNet: A cross-modal gated fusion network for building extraction from very high-resolution remote sensing images and GIS data[J]. ISPRS Journal of Photogrammetry and Remote Sensing, 2022, 188: 61-76.

[48] Zhang J, Liu H, Yang K, et al. CMX: Cross-modal fusion for RGB-X semantic segmentation with transformers[J]. IEEE Transactions on Intelligent Transportation Systems, 2023, 24(12): 14679-14694.
